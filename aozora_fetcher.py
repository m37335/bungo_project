#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é’ç©ºæ–‡åº«ãƒ†ã‚­ã‚¹ãƒˆå–å¾—API
ä»•æ§˜æ›¸ bungo_update_spec_draft01.md S2ç«  é’ç©ºæ–‡åº«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«åŸºã¥ãå®Ÿè£…
"""

import requests
import re
import time
from typing import Dict, List, Optional, Tuple
import logging
from urllib.parse import urljoin, quote
import json
from bs4 import BeautifulSoup
import zipfile
import io
import os

class AozoraFetcher:
    """é’ç©ºæ–‡åº«ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self, cache_dir: str = "aozora_cache"):
        self.base_url = "https://www.aozora.gr.jp"
        self.api_url = "https://pubserver1.herokuapp.com/api/v0.1/books"
        self.cache_dir = cache_dir
        self.logger = logging.getLogger(__name__)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'BungoMapSystem/1.0 (Educational Research Purpose)'
        })
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(cache_dir, exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
    
    def search_author_works(self, author_name: str) -> List[Dict]:
        """
        ä½œè€…åã§ä½œå“æ¤œç´¢
        
        Args:
            author_name: ä½œè€…å
            
        Returns:
            ä½œå“ãƒªã‚¹ãƒˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€IDã€URLç­‰ï¼‰
        """
        self.logger.info(f"ğŸ” é’ç©ºæ–‡åº«æ¤œç´¢: {author_name}")
        
        try:
            # é’ç©ºæ–‡åº«APIæ¤œç´¢
            params = {'author': author_name}
            response = self.session.get(self.api_url, params=params, timeout=10)
            response.raise_for_status()
            
            books = response.json()
            
            if not books:
                self.logger.warning(f"âŒ æ¤œç´¢çµæœãªã—: {author_name}")
                return []
            
            works = []
            for book in books[:10]:  # æœ€å¤§10ä½œå“
                work_info = {
                    'book_id': book.get('book_id'),
                    'title': book.get('title'),
                    'author': book.get('authors', [{}])[0].get('last_name', '') + book.get('authors', [{}])[0].get('first_name', ''),
                    'text_url': book.get('text_url'),
                    'html_url': book.get('html_url'),
                    'card_url': book.get('card_url'),
                    'release_date': book.get('release_date'),
                    'input_encoding': book.get('input_encoding', 'Shift_JIS')
                }
                works.append(work_info)
            
            self.logger.info(f"âœ… æ¤œç´¢å®Œäº†: {author_name} - {len(works)}ä½œå“ç™ºè¦‹")
            return works
            
        except requests.RequestException as e:
            self.logger.error(f"âŒ APIæ¤œç´¢ã‚¨ãƒ©ãƒ¼ {author_name}: {e}")
            return []
        except Exception as e:
            self.logger.error(f"âŒ æ¤œç´¢å‡¦ç†ã‚¨ãƒ©ãƒ¼ {author_name}: {e}")
            return []
    
    def get_text_content(self, work_info: Dict) -> Optional[str]:
        """
        ä½œå“ã®æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
        
        Args:
            work_info: search_author_works()ã®çµæœã®1ã¤
            
        Returns:
            æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆé’ç©ºæ–‡åº«è¨˜æ³•é™¤å»æ¸ˆã¿ï¼‰
        """
        book_id = work_info.get('book_id')
        title = work_info.get('title')
        text_url = work_info.get('text_url')
        
        if not text_url:
            self.logger.warning(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆURLæœªå–å¾—: {title}")
            return None
        
        self.logger.info(f"ğŸ“¥ ãƒ†ã‚­ã‚¹ãƒˆå–å¾—é–‹å§‹: {title}")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        cache_file = os.path.join(self.cache_dir, f"{book_id}_{title}.txt")
        if os.path.exists(cache_file):
            self.logger.info(f"ğŸ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿: {title}")
            with open(cache_file, 'r', encoding='utf-8') as f:
                return f.read()
        
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
            response = self.session.get(text_url, timeout=30)
            response.raise_for_status()
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºãƒ»å¤‰æ›
            encoding = work_info.get('input_encoding', 'shift_jis')
            try:
                text_content = response.content.decode(encoding)
            except UnicodeDecodeError:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                text_content = response.content.decode('shift_jis', errors='ignore')
            
            # é’ç©ºæ–‡åº«è¨˜æ³•é™¤å»
            cleaned_text = self._clean_aozora_text(text_content)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            with open(cache_file, 'w', encoding='utf-8') as f:
                f.write(cleaned_text)
            
            self.logger.info(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆå–å¾—å®Œäº†: {title} ({len(cleaned_text)}æ–‡å­—)")
            return cleaned_text
            
        except requests.RequestException as e:
            self.logger.error(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼ {title}: {e}")
            return None
        except Exception as e:
            self.logger.error(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼ {title}: {e}")
            return None
    
    def _clean_aozora_text(self, raw_text: str) -> str:
        """
        é’ç©ºæ–‡åº«è¨˜æ³•é™¤å»
        
        Args:
            raw_text: ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            è¨˜æ³•é™¤å»æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
        """
        # åŸºæœ¬çš„ãªé’ç©ºæ–‡åº«è¨˜æ³•ã‚’é™¤å»
        patterns = [
            r'-------------------------------------------------------.*?-------------------------------------------------------',  # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ãƒ•ãƒƒã‚¿ãƒ¼
            r'åº•æœ¬ï¼š.*?\n',  # åº•æœ¬æƒ…å ±
            r'å…¥åŠ›ï¼š.*?\n',  # å…¥åŠ›è€…æƒ…å ±
            r'æ ¡æ­£ï¼š.*?\n',  # æ ¡æ­£è€…æƒ…å ±
            r'â€».*?\n',      # æ³¨é‡ˆè¡Œ
            r'ï¼»ï¼ƒ.*?ï¼½',   # è¨˜æ³•ã‚¿ã‚°
            r'ï½œ',          # ãƒ«ãƒ“é–‹å§‹è¨˜å·
            r'ã€Š.*?ã€‹',     # ãƒ«ãƒ“
            r'ã€”.*?ã€•',     # ç·¨è€…æ³¨
        ]
        
        cleaned = raw_text
        for pattern in patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.DOTALL)
        
        # ä½™åˆ†ãªç©ºç™½ãƒ»æ”¹è¡Œæ•´ç†
        cleaned = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned)
        cleaned = re.sub(r'ã€€+', 'ã€€', cleaned)
        cleaned = cleaned.strip()
        
        return cleaned
    
    def get_multiple_works_text(self, author_name: str, max_works: int = 5) -> List[Dict]:
        """
        æŒ‡å®šä½œè€…ã®è¤‡æ•°ä½œå“ãƒ†ã‚­ã‚¹ãƒˆä¸€æ‹¬å–å¾—
        
        Args:
            author_name: ä½œè€…å
            max_works: æœ€å¤§å–å¾—ä½œå“æ•°
            
        Returns:
            ä½œå“ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
        """
        self.logger.info(f"ğŸ“š è¤‡æ•°ä½œå“å–å¾—é–‹å§‹: {author_name} (æœ€å¤§{max_works}ä½œå“)")
        
        # ä½œå“æ¤œç´¢
        works = self.search_author_works(author_name)
        if not works:
            return []
        
        results = []
        for i, work in enumerate(works[:max_works]):
            self.logger.info(f"ğŸ“– å–å¾—ä¸­ {i+1}/{min(len(works), max_works)}: {work['title']}")
            
            text_content = self.get_text_content(work)
            if text_content:
                results.append({
                    'author': author_name,
                    'title': work['title'],
                    'book_id': work['book_id'],
                    'text_content': text_content,
                    'char_count': len(text_content),
                    'aozora_url': work.get('card_url', ''),
                    'release_date': work.get('release_date')
                })
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆç¤¼å„€ï¼‰
            time.sleep(1)
        
        self.logger.info(f"âœ… è¤‡æ•°ä½œå“å–å¾—å®Œäº†: {author_name} - {len(results)}ä½œå“")
        return results
    
    def get_author_profile(self, author_name: str) -> Optional[Dict]:
        """
        ä½œè€…ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±å–å¾—
        
        Args:
            author_name: ä½œè€…å
            
        Returns:
            ä½œè€…æƒ…å ±ï¼ˆç”Ÿå¹´æœˆæ—¥ç­‰ï¼‰
        """
        try:
            # é’ç©ºæ–‡åº«ã®ä½œè€…æ¤œç´¢
            search_url = f"{self.base_url}/index_pages/person_search.html"
            
            # ç°¡æ˜“å®Ÿè£…ï¼ˆå®Ÿéš›ã¯ã‚ˆã‚Šè¤‡é›‘ãªæ¤œç´¢ãŒå¿…è¦ï¼‰
            return {
                'name': author_name,
                'birth_year': None,
                'death_year': None,
                'profile_url': f"{self.base_url}/index_pages/person_search.html"
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼ {author_name}: {e}")
            return None

def test_aozora_fetcher():
    """é’ç©ºæ–‡åº«å–å¾—ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª é’ç©ºæ–‡åº«å–å¾—ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    fetcher = AozoraFetcher()
    
    # å¤ç›®æ¼±çŸ³ä½œå“æ¤œç´¢
    print("\n1. ä½œå“æ¤œç´¢ãƒ†ã‚¹ãƒˆ")
    works = fetcher.search_author_works("å¤ç›®æ¼±çŸ³")
    print(f"æ¤œç´¢çµæœ: {len(works)}ä½œå“")
    for work in works[:3]:
        print(f"  - {work['title']} (ID: {work['book_id']})")
    
    # ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ãƒ†ã‚¹ãƒˆ
    if works:
        print("\n2. ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ãƒ†ã‚¹ãƒˆ")
        first_work = works[0]
        text = fetcher.get_text_content(first_work)
        if text:
            print(f"ãƒ†ã‚­ã‚¹ãƒˆå–å¾—æˆåŠŸ: {first_work['title']}")
            print(f"æ–‡å­—æ•°: {len(text)}")
            print(f"å†’é ­: {text[:200]}...")
        else:
            print("ãƒ†ã‚­ã‚¹ãƒˆå–å¾—å¤±æ•—")
    
    # è¤‡æ•°ä½œå“å–å¾—ãƒ†ã‚¹ãƒˆ
    print("\n3. è¤‡æ•°ä½œå“å–å¾—ãƒ†ã‚¹ãƒˆ")
    results = fetcher.get_multiple_works_text("å¤ç›®æ¼±çŸ³", max_works=2)
    print(f"å–å¾—å®Œäº†: {len(results)}ä½œå“")
    for result in results:
        print(f"  - {result['title']}: {result['char_count']}æ–‡å­—")
    
    print("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    test_aozora_fetcher() 