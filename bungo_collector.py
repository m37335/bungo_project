import wikipedia
import pandas as pd
import os
import re
import time
from typing import List, Dict, Optional
from dotenv import load_dotenv

# OpenAI APIã¨Google Sheets APIã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’æ¡ä»¶ä»˜ãã«ã™ã‚‹
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("è­¦å‘Š: OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚AIè£œå®Œæ©Ÿèƒ½ã¯ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")

# Google Sheetsé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆæ¡ä»¶ä»˜ãï¼‰
GSPREAD_AVAILABLE = False
try:
    import gspread
    from oauth2client.service_account import ServiceAccountCredentials
    GSPREAD_AVAILABLE = True
    print("ğŸ“Š Google Sheetsæ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError:
    print("âš ï¸  Google Sheetsæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™ã€‚gspreadã¨oauth2clientã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")

class BungoCollector:
    """æ—¥æœ¬ã®æ–‡è±ªæƒ…å ±åé›†ãƒ»æ•´ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸè¨­å®š"""
        load_dotenv()
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.google_credentials_path = os.getenv('GOOGLE_CREDENTIALS_PATH', 'credentials.json')
        self.max_authors = int(os.getenv('MAX_AUTHORS', '50'))
        
        # Wikipediaè¨€èªè¨­å®š
        wikipedia.set_lang("ja")
        
        # OpenAIè¨­å®šã¯ enhance_with_ai ãƒ¡ã‚½ãƒƒãƒ‰å†…ã§è¡Œã†
        
        self.authors_data = []
        
    def get_authors_list(self) -> List[str]:
        """
        Wikipediaã‹ã‚‰æ—¥æœ¬ã®æ–‡è±ªä¸€è¦§ã‚’å–å¾—
        
        Returns:
            List[str]: æ–‡è±ªåã®ãƒªã‚¹ãƒˆ
        """
        print("æ–‡è±ªä¸€è¦§ã‚’å–å¾—ä¸­...")
        authors = []
        
        # æ‰‹å‹•ã§æœ‰åãªæ–‡è±ªã‚’è¿½åŠ ï¼ˆä¸»è¦ãƒªã‚¹ãƒˆï¼‰
        famous_authors = [
            "å¤ç›®æ¼±çŸ³", "èŠ¥å·é¾ä¹‹ä»‹", "å¤ªå®°æ²»", "å·ç«¯åº·æˆ", "ä¸‰å³¶ç”±ç´€å¤«",
            "æ¨‹å£ä¸€è‘‰", "æ£®é´å¤–", "å®®æ²¢è³¢æ²»", "è°·å´æ½¤ä¸€éƒ", "äº•ä¼é±’äºŒ",
            "å‚å£å®‰å¾", "çŸ³å·å•„æœ¨", "ä¸è¬é‡æ™¶å­", "æ­£å²¡å­è¦", "ä¸­åŸä¸­ä¹Ÿ",
            "å¿—è³€ç›´å“‰", "æ­¦è€…å°è·¯å®Ÿç¯¤", "æœ‰å³¶æ­¦éƒ", "å³¶å´è—¤æ‘", "å›½æœ¨ç”°ç‹¬æ­©",
            "å°¾å´ç´…è‘‰", "äºŒè‘‰äº­å››è¿·", "åªå†…é€é¥", "å¾³ç”°ç§‹å£°", "ç”°å±±èŠ±è¢‹",
            "æ–°ç¾å—å‰", "å°å·æœªæ˜", "ç«‹åŸé“é€ ", "æ¢¶äº•åŸºæ¬¡éƒ", "æ¨ªå…‰åˆ©ä¸€"
        ]
        
        authors.extend(famous_authors)
        print(f"æ‰‹å‹•ãƒªã‚¹ãƒˆã‹ã‚‰{len(famous_authors)}åã®æ–‡è±ªã‚’è¿½åŠ ")
        
        try:
            # ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ä½œå®¶ã‚’å–å¾—ï¼ˆãƒœãƒ¼ãƒŠã‚¹ï¼‰
            category_pages = [
                "æ—¥æœ¬ã®å°èª¬å®¶",
                "æ—¥æœ¬ã®ä½œå®¶"
            ]
            
            for category in category_pages:
                try:
                    print(f"ã‚«ãƒ†ã‚´ãƒªã€Œ{category}ã€ã‹ã‚‰è¿½åŠ å–å¾—ä¸­...")
                    cat_page = wikipedia.page(f"Category:{category}")
                    if hasattr(cat_page, 'links'):
                        cat_members = cat_page.links
                        additional_authors = [name for name in cat_members[:10] 
                                           if self._is_valid_author_name(name) and name not in authors]
                        authors.extend(additional_authors)
                        print(f"ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰{len(additional_authors)}åã‚’è¿½åŠ ")
                    time.sleep(1)  # APIè² è·è»½æ¸›
                except Exception as e:
                    print(f"ã‚«ãƒ†ã‚´ãƒªã€Œ{category}ã€ã®å–å¾—ã«å¤±æ•—: {e}")
                    continue
                    
        except Exception as e:
            print(f"ã‚«ãƒ†ã‚´ãƒªå–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        # é‡è¤‡é™¤å»ã¨åˆ¶é™
        authors = list(set(authors))
        authors = [name for name in authors if self._is_valid_author_name(name)]
        authors = authors[:self.max_authors]
        
        print(f"å–å¾—å®Œäº†: {len(authors)}åã®æ–‡è±ª")
        return authors
    
    def get_wikipedia_content(self, author_name: str) -> Optional[str]:
        """
        æŒ‡å®šã—ãŸä½œå®¶ã®Wikipediaãƒšãƒ¼ã‚¸ã®æœ¬æ–‡ã‚’å–å¾—
        
        Args:
            author_name (str): ä½œå®¶å
            
        Returns:
            Optional[str]: Wikipediaæœ¬æ–‡ã€å–å¾—å¤±æ•—æ™‚ã¯None
        """
        try:
            print(f"ã€Œ{author_name}ã€ã®Wikipediaãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
            page = wikipedia.page(author_name)
            content = page.content
            
            # æœ¬æ–‡ãŒçŸ­ã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if len(content) < 100:
                print(f"ã€Œ{author_name}ã€: æœ¬æ–‡ãŒçŸ­ã™ãã¾ã™")
                return None
                
            return content
            
        except wikipedia.exceptions.DisambiguationError as e:
            # æ›–æ˜§ã•å›é¿ãƒšãƒ¼ã‚¸ã®å ´åˆã€æœ€åˆã®å€™è£œã‚’è©¦ã™
            try:
                print(f"ã€Œ{author_name}ã€: æ›–æ˜§ã•å›é¿ãƒšãƒ¼ã‚¸ã§ã™ã€‚æœ€åˆã®å€™è£œã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                page = wikipedia.page(e.options[0])
                return page.content
            except Exception:
                print(f"ã€Œ{author_name}ã€: æ›–æ˜§ã•å›é¿ã®è§£æ±ºã«å¤±æ•—")
                return None
                
        except wikipedia.exceptions.PageError:
            print(f"ã€Œ{author_name}ã€: ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        except Exception as e:
            print(f"ã€Œ{author_name}ã€: å–å¾—ã‚¨ãƒ©ãƒ¼ - {e}")
            return None
    
    def extract_works_and_places(self, author_name: str, content: str) -> Dict[str, List[str]]:
        """
        æ­£è¦è¡¨ç¾ã‚’ä½¿ã£ã¦ä½œå“åã¨æ‰€ç¸ã®åœ°ã‚’æŠ½å‡º
        
        Args:
            author_name (str): ä½œå®¶å
            content (str): Wikipediaæœ¬æ–‡
            
        Returns:
            Dict[str, List[str]]: ä½œå“ã¨å ´æ‰€ã®ãƒªã‚¹ãƒˆ
        """
        result = {
            "works": [],
            "places": []
        }
        
        # ä½œå“åæŠ½å‡ºï¼ˆã€ã€ã§å›²ã¾ã‚ŒãŸã‚‚ã®ï¼‰
        work_pattern = r'ã€([^ã€]+)ã€'
        works = re.findall(work_pattern, content)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆé•·ã™ãã‚‹ã€çŸ­ã™ãã‚‹ã€æ•°å­—ã®ã¿ãªã©ã‚’é™¤å¤–ï¼‰
        filtered_works = []
        for work in works:
            if 2 <= len(work) <= 30 and not work.isdigit():
                filtered_works.append(work)
        
        result["works"] = list(set(filtered_works))[:10]  # é‡è¤‡é™¤å»ã€æœ€å¤§10ä½œå“
        
        # åœ°åæŠ½å‡ºï¼ˆéƒ½é“åºœçœŒåã€æœ‰åãªå¸‚åãªã©ï¼‰
        place_keywords = [
            # éƒ½é“åºœçœŒ
            "åŒ—æµ·é“", "é’æ£®", "å²©æ‰‹", "å®®åŸ", "ç§‹ç”°", "å±±å½¢", "ç¦å³¶",
            "èŒ¨åŸ", "æ ƒæœ¨", "ç¾¤é¦¬", "åŸ¼ç‰", "åƒè‘‰", "æ±äº¬", "ç¥å¥ˆå·",
            "æ–°æ½Ÿ", "å¯Œå±±", "çŸ³å·", "ç¦äº•", "å±±æ¢¨", "é•·é‡", "å²é˜œ",
            "é™å²¡", "æ„›çŸ¥", "ä¸‰é‡", "æ»‹è³€", "äº¬éƒ½", "å¤§é˜ª", "å…µåº«",
            "å¥ˆè‰¯", "å’Œæ­Œå±±", "é³¥å–", "å³¶æ ¹", "å²¡å±±", "åºƒå³¶", "å±±å£",
            "å¾³å³¶", "é¦™å·", "æ„›åª›", "é«˜çŸ¥", "ç¦å²¡", "ä½è³€", "é•·å´",
            "ç†Šæœ¬", "å¤§åˆ†", "å®®å´", "é¹¿å…å³¶", "æ²–ç¸„",
            # ä¸»è¦éƒ½å¸‚
            "æœ­å¹Œ", "ä»™å°", "æ¨ªæµœ", "åå¤å±‹", "ç¥æˆ¸", "ç¦å²¡"
        ]
        
        found_places = []
        for place in place_keywords:
            if place in content:
                found_places.append(place)
        
        result["places"] = list(set(found_places))[:5]  # é‡è¤‡é™¤å»ã€æœ€å¤§5ç®‡æ‰€
        
        print(f"ã€Œ{author_name}ã€æŠ½å‡ºçµæœ: ä½œå“{len(result['works'])}ä»¶, å ´æ‰€{len(result['places'])}ä»¶")
        return result
    
    def enhance_with_ai(self, author_name: str, extracted_data: Dict[str, List[str]]) -> Dict[str, List[str]]:
        """
        OpenAI APIã‚’ä½¿ç”¨ã—ã¦æƒ…å ±ã‚’è£œå®Œãƒ»è¦ç´„
        
        Args:
            author_name (str): ä½œå®¶å
            extracted_data (Dict): æŠ½å‡ºæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            Dict[str, List[str]]: è£œå®Œã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        if not OPENAI_AVAILABLE:
            print(f"ã€Œ{author_name}ã€: OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä½¿ç”¨ã—ã¾ã™ã€‚")
            return extracted_data
            
        if not self.openai_api_key:
            print(f"ã€Œ{author_name}ã€: OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä½¿ç”¨ã—ã¾ã™ã€‚")
            return extracted_data
            
        try:
            print(f"ã€Œ{author_name}ã€: AIè£œå®Œä¸­...")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = f"""
æ—¥æœ¬ã®æ–‡è±ªã€Œ{author_name}ã€ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’æ•´ç†ãƒ»è£œå®Œã—ã¦ãã ã•ã„ã€‚

ç¾åœ¨æŠ½å‡ºã•ã‚Œã¦ã„ã‚‹æƒ…å ±:
- ä»£è¡¨ä½œ: {', '.join(extracted_data['works']) if extracted_data['works'] else 'ãªã—'}
- æ‰€ç¸ã®åœ°: {', '.join(extracted_data['places']) if extracted_data['places'] else 'ãªã—'}

ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
ä»£è¡¨ä½œ:
- ä½œå“å1
- ä½œå“å2
- ä½œå“å3

æ‰€ç¸ã®åœ°:
- åœ°å1ï¼ˆç†ç”±ï¼‰
- åœ°å2ï¼ˆç†ç”±ï¼‰
- åœ°å3ï¼ˆç†ç”±ï¼‰

æ³¨æ„äº‹é …:
- ä»£è¡¨ä½œã¯æœ€å¤§5ä½œå“
- æ‰€ç¸ã®åœ°ã¯æœ€å¤§3ç®‡æ‰€
- ç†ç”±ã¯ç°¡æ½”ã«ï¼ˆå‡ºç”Ÿåœ°ã€æ´»å‹•åœ°ã€è¨˜å¿µé¤¨ãªã©ï¼‰
"""
            
            # OpenAI API 1.0.0ä»¥é™ã®æ–°ã—ã„å½¢å¼
            client = OpenAI(api_key=self.openai_api_key)
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬æ–‡å­¦ã®å°‚é–€å®¶ã§ã™ã€‚æ­£ç¢ºã§ç°¡æ½”ãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            ai_response = response.choices[0].message.content
            
            # AIå¿œç­”ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            enhanced_data = self._parse_ai_response(ai_response)
            
            print(f"ã€Œ{author_name}ã€: AIè£œå®Œå®Œäº†")
            time.sleep(1)  # APIåˆ¶é™å¯¾ç­–
            
            return enhanced_data
            
        except Exception as e:
            print(f"ã€Œ{author_name}ã€: AIè£œå®Œã‚¨ãƒ©ãƒ¼ - {e}")
            return extracted_data
    
    def create_dataframe(self) -> pd.DataFrame:
        """
        åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
        
        Returns:
            pd.DataFrame: æ•´ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        print("ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢ä¸­...")
        
        rows = []
        for author_data in self.authors_data:
            author_name = author_data['name']
            works = author_data['works']
            places = author_data['places']
            
            # ä½œå“ã”ã¨ã«è¡Œã‚’ä½œæˆï¼ˆä½œå“ãŒãªã„å ´åˆã¯1è¡Œã ã‘ï¼‰
            if not works:
                works = ['æƒ…å ±ãªã—']
                
            for work in works:
                row = {
                    'ä½œå®¶å': author_name,
                    'ä»£è¡¨ä½œ': work,
                    'æ‰€ç¸ã®åœ°': ', '.join(places) if places else 'æƒ…å ±ãªã—'
                }
                rows.append(row)
        
        df = pd.DataFrame(rows)
        print(f"DataFrameä½œæˆå®Œäº†: {len(df)}è¡Œ")
        return df
    
    def save_to_csv(self, df: pd.DataFrame, filename: str = "authors.csv"):
        """
        DataFrameã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        Args:
            df (pd.DataFrame): ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿
            filename (str): ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        try:
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"CSVä¿å­˜å®Œäº†: {filename}")
        except Exception as e:
            print(f"CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def save_to_google_sheets(self, df: pd.DataFrame, sheet_name: str = "æ—¥æœ¬æ–‡è±ªãƒ‡ãƒ¼ã‚¿"):
        """
        DataFrameã‚’Google Sheetsã«ä¿å­˜
        
        Args:
            df (pd.DataFrame): ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿
            sheet_name (str): ã‚·ãƒ¼ãƒˆå
        """
        if not GSPREAD_AVAILABLE:
            print("Google Sheetsæ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚gspreadãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
            return
            
        if not os.path.exists(self.google_credentials_path):
            print(f"Googleèªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.google_credentials_path}")
            return
            
        try:
            print("Google Sheetsã«ä¿å­˜ä¸­...")
            
            # èªè¨¼è¨­å®š
            scope = ['https://spreadsheets.google.com/feeds',
                    'https://www.googleapis.com/auth/drive']
            creds = ServiceAccountCredentials.from_json_keyfile_name(
                self.google_credentials_path, scope)
            client = gspread.authorize(creds)
            
            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä½œæˆ
            sheet = client.create(sheet_name)
            worksheet = sheet.get_worksheet(0)
            
            # ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            worksheet.append_row(df.columns.tolist())
            
            # ãƒ‡ãƒ¼ã‚¿è¡Œ
            for _, row in df.iterrows():
                worksheet.append_row(row.tolist())
            
            # å…±æœ‰è¨­å®šï¼ˆèª°ã§ã‚‚é–²è¦§å¯èƒ½ï¼‰
            sheet.share('', perm_type='anyone', role='reader')
            
            print(f"Google Sheetsä¿å­˜å®Œäº†: {sheet.url}")
            
        except Exception as e:
            print(f"Google Sheetsä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def process_all_authors(self):
        """
        å…¨ä½“ã®å‡¦ç†ã‚’å®Ÿè¡Œ
        """
        print("=== æ—¥æœ¬æ–‡è±ªæƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ ===")
        
        # 1. ä½œå®¶ä¸€è¦§å–å¾—
        authors = self.get_authors_list()
        if not authors:
            print("ä½œå®¶ä¸€è¦§ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return
        
        # 2. å„ä½œå®¶ã®æƒ…å ±åé›†
        for i, author in enumerate(authors, 1):
            print(f"\n[{i}/{len(authors)}] å‡¦ç†ä¸­: {author}")
            
            # Wikipediaæœ¬æ–‡å–å¾—
            content = self.get_wikipedia_content(author)
            if not content:
                continue
            
            # åˆæœŸæŠ½å‡º
            extracted_data = self.extract_works_and_places(author, content)
            
            # AIè£œå®Œ
            enhanced_data = self.enhance_with_ai(author, extracted_data)
            
            # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            self.authors_data.append({
                'name': author,
                'works': enhanced_data['works'],
                'places': enhanced_data['places']
            })
            
            # é€²æ—è¡¨ç¤º
            if i % 5 == 0:
                print(f"é€²æ—: {i}/{len(authors)} å®Œäº†")
        
        # 3. ãƒ‡ãƒ¼ã‚¿æ•´å½¢ã¨å‡ºåŠ›
        if self.authors_data:
            df = self.create_dataframe()
            
            # CSVå‡ºåŠ›
            self.save_to_csv(df)
            
            # Google Sheetså‡ºåŠ›
            if GSPREAD_AVAILABLE:
                self.save_to_google_sheets(df)
            else:
                print("Google Sheetså‡ºåŠ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆgspreadãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ï¼‰")
            
            print(f"\n=== å‡¦ç†å®Œäº† ===")
            print(f"å‡¦ç†ã—ãŸä½œå®¶æ•°: {len(self.authors_data)}")
            print(f"å‡ºåŠ›è¡Œæ•°: {len(df)}")
        else:
            print("åé›†ã§ããŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    def _parse_ai_response(self, response: str) -> Dict[str, List[str]]:
        """
        AIå¿œç­”ã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        
        Args:
            response (str): AIå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            Dict[str, List[str]]: æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        result = {"works": [], "places": []}
        
        lines = response.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if 'ä»£è¡¨ä½œ:' in line:
                current_section = 'works'
            elif 'æ‰€ç¸ã®åœ°:' in line:
                current_section = 'places'
            elif line.startswith('- ') and current_section:
                item = line[2:].strip()
                if current_section == 'works':
                    result['works'].append(item)
                elif current_section == 'places':
                    # æ‹¬å¼§å†…ã®èª¬æ˜ã‚’é™¤å»
                    place = re.sub(r'ï¼ˆ.+?ï¼‰', '', item).strip()
                    result['places'].append(place)
        
        return result
    
    def _is_valid_author_name(self, name: str) -> bool:
        """
        æœ‰åŠ¹ãªæ–‡è±ªåã‹ãƒã‚§ãƒƒã‚¯
        
        Args:
            name (str): ãƒã‚§ãƒƒã‚¯ã™ã‚‹åå‰
            
        Returns:
            bool: æœ‰åŠ¹ãªåå‰ã®å ´åˆTrue
        """
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_patterns = [
            r'^\d',  # æ•°å­—ã§å§‹ã¾ã‚‹
            r'Category:',  # ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸
            r'Template:',  # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒšãƒ¼ã‚¸
            r'List of',  # ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸
            r'ä¸€è¦§$',  # ä¸€è¦§ã§çµ‚ã‚ã‚‹
            r'å¹´$',  # å¹´ã§çµ‚ã‚ã‚‹
        ]
        
        for pattern in exclude_patterns:
            if re.search(pattern, name):
                return False
        
        # æ—¥æœ¬èªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆUnicodeç¯„å›²ã§åˆ¤å®šï¼‰
        japanese_chars = False
        for char in name:
            # ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã®Unicodeç¯„å›²
            if ('\u3040' <= char <= '\u309F') or \
               ('\u30A0' <= char <= '\u30FF') or \
               ('\u4E00' <= char <= '\u9FFF'):
                japanese_chars = True
                break
        
        return japanese_chars and len(name) >= 2 and len(name) <= 10
        
if __name__ == "__main__":
    collector = BungoCollector()
    collector.process_all_authors() 