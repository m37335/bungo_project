import wikipedia
import pandas as pd
import os
import re
import time
from typing import List, Dict, Optional
from dotenv import load_dotenv

# OpenAI APIã¨Google Sheets APIã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’æ¡ä»¶ä»˜ãã«ã™ã‚‹
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("è­¦å‘Š: OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚AIè£œå®Œæ©Ÿèƒ½ã¯ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")

# Google Sheetsé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆæ¡ä»¶ä»˜ãï¼‰
GSPREAD_AVAILABLE = False
try:
    import gspread
    from oauth2client.service_account import ServiceAccountCredentials
    GSPREAD_AVAILABLE = True
    print("ğŸ“Š Google Sheetsæ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError:
    print("âš ï¸  Google Sheetsæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™ã€‚gspreadã¨oauth2clientã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")

class BungoCollector:
    """æ—¥æœ¬ã®æ–‡è±ªæƒ…å ±åé›†ãƒ»æ•´ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸè¨­å®š"""
        load_dotenv()
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.google_credentials_path = os.getenv('GOOGLE_CREDENTIALS_PATH', 'credentials.json')
        self.max_authors = int(os.getenv('MAX_AUTHORS', '50'))
        
        # Wikipediaè¨€èªè¨­å®š
        wikipedia.set_lang("ja")
        
        # OpenAIè¨­å®šã¯ enhance_with_ai ãƒ¡ã‚½ãƒƒãƒ‰å†…ã§è¡Œã†
        
        self.authors_data = []
        
    def get_authors_list(self) -> List[str]:
        """
        Wikipediaã‹ã‚‰æ—¥æœ¬ã®æ–‡è±ªä¸€è¦§ã‚’å–å¾—
        
        Returns:
            List[str]: æ–‡è±ªåã®ãƒªã‚¹ãƒˆ
        """
        print("æ–‡è±ªä¸€è¦§ã‚’å–å¾—ä¸­...")
        authors = []
        
        # æ‰‹å‹•ã§æœ‰åãªæ–‡è±ªã‚’è¿½åŠ ï¼ˆä¸»è¦ãƒªã‚¹ãƒˆï¼‰
        famous_authors = [
            "å¤ç›®æ¼±çŸ³", "èŠ¥å·é¾ä¹‹ä»‹", "å¤ªå®°æ²»", "å·ç«¯åº·æˆ", "ä¸‰å³¶ç”±ç´€å¤«",
            "æ¨‹å£ä¸€è‘‰", "æ£®é´å¤–", "å®®æ²¢è³¢æ²»", "è°·å´æ½¤ä¸€éƒ", "äº•ä¼é±’äºŒ",
            "å‚å£å®‰å¾", "çŸ³å·å•„æœ¨", "ä¸è¬é‡æ™¶å­", "æ­£å²¡å­è¦", "ä¸­åŸä¸­ä¹Ÿ",
            "å¿—è³€ç›´å“‰", "æ­¦è€…å°è·¯å®Ÿç¯¤", "æœ‰å³¶æ­¦éƒ", "å³¶å´è—¤æ‘", "å›½æœ¨ç”°ç‹¬æ­©",
            "å°¾å´ç´…è‘‰", "äºŒè‘‰äº­å››è¿·", "åªå†…é€é¥", "å¾³ç”°ç§‹å£°", "ç”°å±±èŠ±è¢‹",
            "æ–°ç¾å—å‰", "å°å·æœªæ˜", "ç«‹åŸé“é€ ", "æ¢¶äº•åŸºæ¬¡éƒ", "æ¨ªå…‰åˆ©ä¸€"
        ]
        
        authors.extend(famous_authors)
        print(f"æ‰‹å‹•ãƒªã‚¹ãƒˆã‹ã‚‰{len(famous_authors)}åã®æ–‡è±ªã‚’è¿½åŠ ")
        
        try:
            # ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ä½œå®¶ã‚’å–å¾—ï¼ˆãƒœãƒ¼ãƒŠã‚¹ï¼‰
            category_pages = [
                "æ—¥æœ¬ã®å°èª¬å®¶",
                "æ—¥æœ¬ã®ä½œå®¶"
            ]
            
            for category in category_pages:
                try:
                    print(f"ã‚«ãƒ†ã‚´ãƒªã€Œ{category}ã€ã‹ã‚‰è¿½åŠ å–å¾—ä¸­...")
                    cat_page = wikipedia.page(f"Category:{category}")
                    if hasattr(cat_page, 'links'):
                        cat_members = cat_page.links
                        additional_authors = [name for name in cat_members[:10] 
                                           if self._is_valid_author_name(name) and name not in authors]
                        authors.extend(additional_authors)
                        print(f"ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰{len(additional_authors)}åã‚’è¿½åŠ ")
                    time.sleep(1)  # APIè² è·è»½æ¸›
                except Exception as e:
                    print(f"ã‚«ãƒ†ã‚´ãƒªã€Œ{category}ã€ã®å–å¾—ã«å¤±æ•—: {e}")
                    continue
                    
        except Exception as e:
            print(f"ã‚«ãƒ†ã‚´ãƒªå–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        # é‡è¤‡é™¤å»ã¨åˆ¶é™
        authors = list(set(authors))
        authors = [name for name in authors if self._is_valid_author_name(name)]
        authors = authors[:self.max_authors]
        
        print(f"å–å¾—å®Œäº†: {len(authors)}åã®æ–‡è±ª")
        return authors
    
    def get_wikipedia_content(self, author_name: str) -> Optional[str]:
        """
        æŒ‡å®šã—ãŸä½œå®¶ã®Wikipediaãƒšãƒ¼ã‚¸ã®æœ¬æ–‡ã‚’å–å¾—
        
        Args:
            author_name (str): ä½œå®¶å
            
        Returns:
            Optional[str]: Wikipediaæœ¬æ–‡ã€å–å¾—å¤±æ•—æ™‚ã¯None
        """
        try:
            print(f"ã€Œ{author_name}ã€ã®Wikipediaãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
            page = wikipedia.page(author_name)
            content = page.content
            
            # æœ¬æ–‡ãŒçŸ­ã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if len(content) < 100:
                print(f"ã€Œ{author_name}ã€: æœ¬æ–‡ãŒçŸ­ã™ãã¾ã™")
                return None
                
            return content
            
        except wikipedia.exceptions.DisambiguationError as e:
            # æ›–æ˜§ã•å›é¿ãƒšãƒ¼ã‚¸ã®å ´åˆã€æœ€åˆã®å€™è£œã‚’è©¦ã™
            try:
                print(f"ã€Œ{author_name}ã€: æ›–æ˜§ã•å›é¿ãƒšãƒ¼ã‚¸ã§ã™ã€‚æœ€åˆã®å€™è£œã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                page = wikipedia.page(e.options[0])
                return page.content
            except Exception:
                print(f"ã€Œ{author_name}ã€: æ›–æ˜§ã•å›é¿ã®è§£æ±ºã«å¤±æ•—")
                return None
                
        except wikipedia.exceptions.PageError:
            print(f"ã€Œ{author_name}ã€: ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        except Exception as e:
            print(f"ã€Œ{author_name}ã€: å–å¾—ã‚¨ãƒ©ãƒ¼ - {e}")
            return None
    
    def extract_works_and_places(self, author_name: str, content: str) -> Dict[str, List[str]]:
        """
        æ­£è¦è¡¨ç¾ã‚’ä½¿ã£ã¦ä½œå“åã¨æ‰€ç¸ã®åœ°ã‚’æŠ½å‡ºï¼ˆGoogle Mapsé€£æºå¯¾å¿œç‰ˆï¼‰
        
        Args:
            author_name (str): ä½œå®¶å
            content (str): Wikipediaæœ¬æ–‡
            
        Returns:
            Dict[str, List[str]]: ä½œå“ã¨è©³ç´°ãªå ´æ‰€æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        result = {
            "works": [],
            "places": [],
            "detailed_places": []  # æ–°è¨­ï¼šè©³ç´°ä½æ‰€æƒ…å ±
        }
        
        # ä½œå“åæŠ½å‡ºï¼ˆã€ã€ã§å›²ã¾ã‚ŒãŸã‚‚ã®ï¼‰
        work_pattern = r'ã€([^ã€]+)ã€'
        works = re.findall(work_pattern, content)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆé•·ã™ãã‚‹ã€çŸ­ã™ãã‚‹ã€æ•°å­—ã®ã¿ãªã©ã‚’é™¤å¤–ï¼‰
        filtered_works = []
        for work in works:
            if 2 <= len(work) <= 30 and not work.isdigit():
                filtered_works.append(work)
        
        result["works"] = list(set(filtered_works))[:10]  # é‡è¤‡é™¤å»ã€æœ€å¤§10ä½œå“
        
        # === è©³ç´°åœ°åæŠ½å‡ºæ©Ÿèƒ½ ===
        
        # 1. éƒ½é“åºœçœŒæŠ½å‡º
        prefectures = [
            "åŒ—æµ·é“", "é’æ£®çœŒ", "å²©æ‰‹çœŒ", "å®®åŸçœŒ", "ç§‹ç”°çœŒ", "å±±å½¢çœŒ", "ç¦å³¶çœŒ",
            "èŒ¨åŸçœŒ", "æ ƒæœ¨çœŒ", "ç¾¤é¦¬çœŒ", "åŸ¼ç‰çœŒ", "åƒè‘‰çœŒ", "æ±äº¬éƒ½", "ç¥å¥ˆå·çœŒ",
            "æ–°æ½ŸçœŒ", "å¯Œå±±çœŒ", "çŸ³å·çœŒ", "ç¦äº•çœŒ", "å±±æ¢¨çœŒ", "é•·é‡çœŒ", "å²é˜œçœŒ",
            "é™å²¡çœŒ", "æ„›çŸ¥çœŒ", "ä¸‰é‡çœŒ", "æ»‹è³€çœŒ", "äº¬éƒ½åºœ", "å¤§é˜ªåºœ", "å…µåº«çœŒ",
            "å¥ˆè‰¯çœŒ", "å’Œæ­Œå±±çœŒ", "é³¥å–çœŒ", "å³¶æ ¹çœŒ", "å²¡å±±çœŒ", "åºƒå³¶çœŒ", "å±±å£çœŒ",
            "å¾³å³¶çœŒ", "é¦™å·çœŒ", "æ„›åª›çœŒ", "é«˜çŸ¥çœŒ", "ç¦å²¡çœŒ", "ä½è³€çœŒ", "é•·å´çœŒ",
            "ç†Šæœ¬çœŒ", "å¤§åˆ†çœŒ", "å®®å´çœŒ", "é¹¿å…å³¶çœŒ", "æ²–ç¸„çœŒ",
            # çŸ­ç¸®å½¢ã‚‚å«ã‚€
            "åŒ—æµ·é“", "é’æ£®", "å²©æ‰‹", "å®®åŸ", "ç§‹ç”°", "å±±å½¢", "ç¦å³¶",
            "èŒ¨åŸ", "æ ƒæœ¨", "ç¾¤é¦¬", "åŸ¼ç‰", "åƒè‘‰", "æ±äº¬", "ç¥å¥ˆå·",
            "æ–°æ½Ÿ", "å¯Œå±±", "çŸ³å·", "ç¦äº•", "å±±æ¢¨", "é•·é‡", "å²é˜œ",
            "é™å²¡", "æ„›çŸ¥", "ä¸‰é‡", "æ»‹è³€", "äº¬éƒ½", "å¤§é˜ª", "å…µåº«",
            "å¥ˆè‰¯", "å’Œæ­Œå±±", "é³¥å–", "å³¶æ ¹", "å²¡å±±", "åºƒå³¶", "å±±å£",
            "å¾³å³¶", "é¦™å·", "æ„›åª›", "é«˜çŸ¥", "ç¦å²¡", "ä½è³€", "é•·å´",
            "ç†Šæœ¬", "å¤§åˆ†", "å®®å´", "é¹¿å…å³¶", "æ²–ç¸„"
        ]
        
        # 2. å¸‚åŒºç”ºæ‘æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        city_patterns = [
            r'([^\s]+å¸‚)',      # â—‹â—‹å¸‚
            r'([^\s]+åŒº)',      # â—‹â—‹åŒº  
            r'([^\s]+ç”º)',      # â—‹â—‹ç”º
            r'([^\s]+æ‘)',      # â—‹â—‹æ‘
            r'([^\s]+éƒ¡)',      # â—‹â—‹éƒ¡
        ]
        
        # 3. æ–‡å­¦é–¢é€£æ–½è¨­ãƒ»è¨˜å¿µé¤¨ã®æŠ½å‡º
        facility_patterns = [
            r'([^\s]+è¨˜å¿µé¤¨)',
            r'([^\s]+æ–‡å­¦é¤¨)',
            r'([^\s]+åšç‰©é¤¨)',
            r'([^\s]+è³‡æ–™é¤¨)',
            r'([^\s]+ç”Ÿå®¶)',
            r'([^\s]+æ—§å±…)',
            r'([^\s]+å¢“æ‰€)',
            r'([^\s]+è¨˜å¿µç¢‘)',
        ]
        
        # 4. è©³ç´°ä½æ‰€ãƒ‘ã‚¿ãƒ¼ãƒ³
        address_patterns = [
            r'([^ã€‚ã€\s]+[éƒ½é“åºœçœŒ][^ã€‚ã€\s]*[å¸‚åŒºç”ºæ‘][^ã€‚ã€\s]*[0-9]+[-âˆ’â€][0-9]+)',  # å®Œå…¨ãªä½æ‰€
            r'([^ã€‚ã€\s]+[éƒ½é“åºœçœŒ][^ã€‚ã€\s]*[å¸‚åŒºç”ºæ‘][^ã€‚ã€\s]*)',  # å¸‚åŒºç”ºæ‘ã¾ã§
            r'([^ã€‚ã€\s]+[å¸‚åŒºç”ºæ‘][^ã€‚ã€\s]*[0-9]+[-âˆ’â€][0-9]+)',  # ç•ªåœ°ä»˜ã
        ]
        
        # 5. åœ°åã®ç¨®é¡åˆ†é¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        place_type_patterns = {
            'å‡ºç”Ÿåœ°': r'(ç”Ÿã¾ã‚Œ|å‡ºèº«|ç”Ÿå®¶|æ•…éƒ·)',
            'å±…ä½åœ°': r'(ä½|å±…|ç§»ä½|è»¢å±…|å®šä½)',
            'æ´»å‹•åœ°': r'(æ´»å‹•|åŸ·ç­†|å‰µä½œ|æ–‡å­¦æ´»å‹•)',
            'è¨˜å¿µé¤¨': r'(è¨˜å¿µé¤¨|æ–‡å­¦é¤¨|åšç‰©é¤¨|è³‡æ–™é¤¨)',
            'å¢“æ‰€': r'(å¢“|çœ ã‚‹|åŸ‹è‘¬)',
            'ä½œå“èˆå°': r'(èˆå°|è¨­å®š|æ|ãƒ¢ãƒ‡ãƒ«)',
        }
        
        found_places = []
        detailed_places = []
        
        # éƒ½é“åºœçœŒã®æŠ½å‡º
        for prefecture in prefectures:
            if prefecture in content:
                found_places.append(prefecture)
        
        # å¸‚åŒºç”ºæ‘ã®æŠ½å‡º
        for pattern in city_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if len(match) >= 2 and not match.isdigit():
                    found_places.append(match)
        
        # æ–‡å­¦é–¢é€£æ–½è¨­ã®æŠ½å‡º
        for pattern in facility_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if len(match) >= 3:
                    detailed_places.append({
                        'name': match,
                        'type': 'è¨˜å¿µé¤¨ãƒ»æ–‡å­¦æ–½è¨­',
                        'context': self._extract_context(content, match)
                    })
        
        # è©³ç´°ä½æ‰€ã®æŠ½å‡º
        for pattern in address_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if len(match) >= 5:
                    detailed_places.append({
                        'name': match,
                        'type': 'ä½æ‰€',
                        'context': self._extract_context(content, match)
                    })
        
        # åœ°åã®ç¨®é¡åˆ†é¡
        for location in found_places:
            for place_type, type_pattern in place_type_patterns.items():
                if re.search(type_pattern + r'.*' + re.escape(location), content) or \
                   re.search(re.escape(location) + r'.*' + type_pattern, content):
                    detailed_places.append({
                        'name': location,
                        'type': place_type,
                        'context': self._extract_context(content, location)
                    })
                    break
            else:
                # ç¨®é¡ãŒç‰¹å®šã§ããªã„å ´åˆã¯ä¸€èˆ¬çš„ãªåœ°åã¨ã—ã¦è¿½åŠ 
                detailed_places.append({
                    'name': location,
                    'type': 'é–¢é€£åœ°',
                    'context': self._extract_context(content, location)
                })
        
        # é‡è¤‡é™¤å»ã¨æ•´ç†
        result["places"] = list(set(found_places))[:8]  # å¾“æ¥ã®åœ°åãƒªã‚¹ãƒˆ
        
        # è©³ç´°åœ°åã®é‡è¤‡é™¤å»
        seen_names = set()
        filtered_detailed = []
        for place in detailed_places:
            if place['name'] not in seen_names:
                seen_names.add(place['name'])
                filtered_detailed.append(place)
        
        result["detailed_places"] = filtered_detailed[:10]  # æœ€å¤§10ç®‡æ‰€
        
        print(f"ã€Œ{author_name}ã€è©³ç´°æŠ½å‡ºçµæœ: ä½œå“{len(result['works'])}ä»¶, " +
              f"åœ°å{len(result['places'])}ä»¶, è©³ç´°åœ°å{len(result['detailed_places'])}ä»¶")
        
        return result
    
    def _extract_context(self, content: str, location: str, context_length: int = 50) -> str:
        """
        åœ°åã®å‰å¾Œã®æ–‡è„ˆã‚’æŠ½å‡ºã—ã¦Google Mapsæ¤œç´¢ã«å½¹ç«‹ã¤æƒ…å ±ã‚’æä¾›
        
        Args:
            content (str): å…¨æ–‡
            location (str): å¯¾è±¡åœ°å
            context_length (int): å‰å¾Œã®æ–‡å­—æ•°
            
        Returns:
            str: æ–‡è„ˆæƒ…å ±
        """
        try:
            index = content.find(location)
            if index != -1:
                start = max(0, index - context_length)
                end = min(len(content), index + len(location) + context_length)
                context = content[start:end].replace('\n', ' ').strip()
                return context
            return ""
        except Exception:
            return ""
    
    def enhance_with_ai(self, author_name: str, extracted_data: Dict[str, List[str]]) -> Dict[str, List[str]]:
        """
        OpenAI APIã‚’ä½¿ç”¨ã—ã¦æƒ…å ±ã‚’è£œå®Œãƒ»è¦ç´„
        
        Args:
            author_name (str): ä½œå®¶å
            extracted_data (Dict): æŠ½å‡ºæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            Dict[str, List[str]]: è£œå®Œã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        if not OPENAI_AVAILABLE:
            print(f"ã€Œ{author_name}ã€: OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä½¿ç”¨ã—ã¾ã™ã€‚")
            return extracted_data
            
        if not self.openai_api_key:
            print(f"ã€Œ{author_name}ã€: OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä½¿ç”¨ã—ã¾ã™ã€‚")
            return extracted_data
            
        try:
            print(f"ã€Œ{author_name}ã€: AIè£œå®Œä¸­...")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆè©³ç´°ä½æ‰€å¯¾å¿œç‰ˆï¼‰
            detailed_places_info = ""
            if 'detailed_places' in extracted_data and extracted_data['detailed_places']:
                detailed_places_info = "è©³ç´°åœ°åæƒ…å ±:\n"
                for place in extracted_data['detailed_places'][:5]:  # æœ€å¤§5ä»¶è¡¨ç¤º
                    detailed_places_info += f"- {place['name']} ({place['type']})\n"
            
            prompt = f"""
æ—¥æœ¬ã®æ–‡è±ªã€Œ{author_name}ã€ã«ã¤ã„ã¦ã€Google Mapsé€£æºã‚’æƒ³å®šã—ãŸè©³ç´°ãªåœ°ç†æƒ…å ±ã‚’å«ã‚ã¦æ•´ç†ãƒ»è£œå®Œã—ã¦ãã ã•ã„ã€‚

ç¾åœ¨æŠ½å‡ºã•ã‚Œã¦ã„ã‚‹æƒ…å ±:
- ä»£è¡¨ä½œ: {', '.join(extracted_data['works']) if extracted_data['works'] else 'ãªã—'}
- æ‰€ç¸ã®åœ°: {', '.join(extracted_data['places']) if extracted_data['places'] else 'ãªã—'}
{detailed_places_info}

ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:

ä»£è¡¨ä½œ:
- ä½œå“å1
- ä½œå“å2
- ä½œå“å3

è©³ç´°æ‰€ç¸ã®åœ°ï¼ˆGoogle Mapsæ¤œç´¢å¯èƒ½ãªå½¢å¼ã§ï¼‰:
- å…·ä½“çš„ãªä½æ‰€ã¾ãŸã¯åœ°å1ï¼ˆç¨®é¡ï¼šå‡ºç”Ÿåœ°ãƒ»å±…ä½åœ°ãƒ»è¨˜å¿µé¤¨ãƒ»å¢“æ‰€ãªã©ï¼‰
- å…·ä½“çš„ãªä½æ‰€ã¾ãŸã¯åœ°å2ï¼ˆç¨®é¡ï¼‰
- å…·ä½“çš„ãªä½æ‰€ã¾ãŸã¯åœ°å3ï¼ˆç¨®é¡ï¼‰

è¨˜å¿µé¤¨ãƒ»æ–‡å­¦æ–½è¨­:
- æ–½è¨­å1ï¼ˆä½æ‰€ãŒã‚ã‹ã‚Œã°ä½µè¨˜ï¼‰
- æ–½è¨­å2ï¼ˆä½æ‰€ãŒã‚ã‹ã‚Œã°ä½µè¨˜ï¼‰

æ³¨æ„äº‹é …:
- ä»£è¡¨ä½œã¯æœ€å¤§5ä½œå“
- è©³ç´°æ‰€ç¸ã®åœ°ã¯æœ€å¤§5ç®‡æ‰€ã€å¯èƒ½ãªé™ã‚Šã€Œâ—‹â—‹çœŒâ—‹â—‹å¸‚â—‹â—‹ç”ºã€ãƒ¬ãƒ™ãƒ«ã¾ã§å…·ä½“çš„ã«
- è¨˜å¿µé¤¨ãƒ»æ–‡å­¦æ–½è¨­ã¯æœ€å¤§3ç®‡æ‰€
- ä½æ‰€ã¯ç¾åœ¨ã®è¡Œæ”¿åŒºåˆ†ã§è¨˜è¼‰
- ä¸æ˜ãªå ´åˆã¯æ¨æ¸¬ã›ãšã€Œè©³ç´°ä¸æ˜ã€ã¨è¨˜è¼‰
"""
            
            # OpenAI API 1.0.0ä»¥é™ã®æ–°ã—ã„å½¢å¼
            client = OpenAI(api_key=self.openai_api_key)
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬æ–‡å­¦ã®å°‚é–€å®¶ã§ã™ã€‚æ­£ç¢ºã§ç°¡æ½”ãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            ai_response = response.choices[0].message.content
            
            # AIå¿œç­”ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            enhanced_data = self._parse_ai_response(ai_response)
            
            print(f"ã€Œ{author_name}ã€: AIè£œå®Œå®Œäº†")
            time.sleep(1)  # APIåˆ¶é™å¯¾ç­–
            
            return enhanced_data
            
        except Exception as e:
            print(f"ã€Œ{author_name}ã€: AIè£œå®Œã‚¨ãƒ©ãƒ¼ - {e}")
            return extracted_data
    
    def create_dataframe(self) -> pd.DataFrame:
        """
        åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›ï¼ˆè©³ç´°ä½æ‰€å¯¾å¿œç‰ˆï¼‰
        
        Returns:
            pd.DataFrame: æ•´ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        print("ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢ä¸­ï¼ˆè©³ç´°ä½æ‰€å¯¾å¿œç‰ˆï¼‰...")
        
        rows = []
        for author_data in self.authors_data:
            author_name = author_data['name']
            works = author_data['works']
            places = author_data['places']
            detailed_places = author_data.get('detailed_places', [])
            
            # ä½œå“ã”ã¨ã«è¡Œã‚’ä½œæˆï¼ˆä½œå“ãŒãªã„å ´åˆã¯1è¡Œã ã‘ï¼‰
            if not works:
                works = ['æƒ…å ±ãªã—']
            
            # è©³ç´°åœ°åæƒ…å ±ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            has_detailed_places = len(detailed_places) > 0
            
            for work in works:
                if has_detailed_places:
                    # è©³ç´°åœ°åæƒ…å ±ãŒã‚ã‚‹å ´åˆï¼šåœ°åã”ã¨ã«è¡Œã‚’ä½œæˆ
                    for place_info in detailed_places:
                        row = {
                            'ä½œå®¶å': author_name,
                            'ä»£è¡¨ä½œ': work,
                            'æ‰€ç¸ã®åœ°': place_info['name'],
                            'åœ°åç¨®é¡': place_info['type'],
                            'æ–‡è„ˆæƒ…å ±': place_info.get('context', '')[:100],  # 100æ–‡å­—ã¾ã§
                            'å¾“æ¥ã®åœ°å': ', '.join(places) if places else 'æƒ…å ±ãªã—'
                        }
                        rows.append(row)
                else:
                    # å¾“æ¥å½¢å¼ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    row = {
                        'ä½œå®¶å': author_name,
                        'ä»£è¡¨ä½œ': work,
                        'æ‰€ç¸ã®åœ°': ', '.join(places) if places else 'æƒ…å ±ãªã—',
                        'åœ°åç¨®é¡': 'é–¢é€£åœ°',
                        'æ–‡è„ˆæƒ…å ±': '',
                        'å¾“æ¥ã®åœ°å': ', '.join(places) if places else 'æƒ…å ±ãªã—'
                    }
                    rows.append(row)
        
        df = pd.DataFrame(rows)
        print(f"DataFrameä½œæˆå®Œäº†: {len(df)}è¡Œ, è©³ç´°åœ°åå¯¾å¿œ")
        return df
    
    def create_detailed_dataframe(self) -> pd.DataFrame:
        """
        Google Mapsé€£æºç‰¹åŒ–ç‰ˆã®DataFrameã‚’ä½œæˆ
        
        Returns:
            pd.DataFrame: Google Mapsç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        print("Google Mapsé€£æºç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆä¸­...")
        
        rows = []
        for author_data in self.authors_data:
            author_name = author_data['name']
            detailed_places = author_data.get('detailed_places', [])
            
            if detailed_places:
                for place_info in detailed_places:
                    row = {
                        'ä½œå®¶å': author_name,
                        'åœ°å': place_info['name'],
                        'ç¨®é¡': place_info['type'],
                        'æ¤œç´¢ç”¨åœ°å': self._normalize_place_name(place_info['name']),
                        'æ–‡è„ˆ': place_info.get('context', '')[:200],
                        'Google Mapsæº–å‚™æ¸ˆã¿': 'â—‹' if self._is_maps_ready(place_info['name']) else 'è¦ç¢ºèª'
                    }
                    rows.append(row)
        
        df = pd.DataFrame(rows)
        print(f"Google Mapsç”¨DataFrameä½œæˆå®Œäº†: {len(df)}è¡Œ")
        return df
    
    def _normalize_place_name(self, place_name: str) -> str:
        """
        Google Mapsæ¤œç´¢ç”¨ã«åœ°åã‚’æ­£è¦åŒ–
        
        Args:
            place_name (str): å…ƒã®åœ°å
            
        Returns:
            str: æ­£è¦åŒ–ã•ã‚ŒãŸåœ°å
        """
        # åŸºæœ¬çš„ãªæ­£è¦åŒ–
        normalized = place_name.strip()
        
        # è¨˜å¿µé¤¨ãƒ»æ–‡å­¦é¤¨ã®å ´åˆã¯ã€Œâ—‹â—‹è¨˜å¿µé¤¨ã€å½¢å¼ã«çµ±ä¸€
        if 'è¨˜å¿µé¤¨' in normalized or 'æ–‡å­¦é¤¨' in normalized:
            return normalized
        
        # ä½æ‰€å½¢å¼ã®å ´åˆã¯ãã®ã¾ã¾
        if 'å¸‚' in normalized or 'åŒº' in normalized or 'ç”º' in normalized:
            return normalized
        
        # éƒ½é“åºœçœŒåã®å ´åˆã¯ã€ŒçœŒã€ã‚’è¿½åŠ 
        prefectures_short = [
            "é’æ£®", "å²©æ‰‹", "å®®åŸ", "ç§‹ç”°", "å±±å½¢", "ç¦å³¶",
            "èŒ¨åŸ", "æ ƒæœ¨", "ç¾¤é¦¬", "åŸ¼ç‰", "åƒè‘‰", "ç¥å¥ˆå·",
            "æ–°æ½Ÿ", "å¯Œå±±", "çŸ³å·", "ç¦äº•", "å±±æ¢¨", "é•·é‡", "å²é˜œ",
            "é™å²¡", "æ„›çŸ¥", "ä¸‰é‡", "æ»‹è³€", "å…µåº«",
            "å¥ˆè‰¯", "å’Œæ­Œå±±", "é³¥å–", "å³¶æ ¹", "å²¡å±±", "åºƒå³¶", "å±±å£",
            "å¾³å³¶", "é¦™å·", "æ„›åª›", "é«˜çŸ¥", "ç¦å²¡", "ä½è³€", "é•·å´",
            "ç†Šæœ¬", "å¤§åˆ†", "å®®å´", "é¹¿å…å³¶", "æ²–ç¸„"
        ]
        
        if normalized in prefectures_short:
            return normalized + "çœŒ"
        
        return normalized
    
    def _is_maps_ready(self, place_name: str) -> bool:
        """
        Google Mapsæ¤œç´¢ã«é©ã—ãŸå½¢å¼ã‹ãƒã‚§ãƒƒã‚¯
        
        Args:
            place_name (str): åœ°å
            
        Returns:
            bool: Mapsæ¤œç´¢æº–å‚™æ¸ˆã¿ã‹ã©ã†ã‹
        """
        # è¨˜å¿µé¤¨ãƒ»æ–‡å­¦é¤¨ã¯æº–å‚™æ¸ˆã¿
        if any(facility in place_name for facility in ['è¨˜å¿µé¤¨', 'æ–‡å­¦é¤¨', 'åšç‰©é¤¨', 'è³‡æ–™é¤¨']):
            return True
        
        # å¸‚åŒºç”ºæ‘ãƒ¬ãƒ™ãƒ«ã®ä½æ‰€ã¯æº–å‚™æ¸ˆã¿
        if any(admin in place_name for admin in ['å¸‚', 'åŒº', 'ç”º', 'æ‘']):
            return True
        
        # ç•ªåœ°ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯æº–å‚™æ¸ˆã¿
        if re.search(r'\d+[-âˆ’â€]\d+', place_name):
            return True
        
        return False
    
    def save_to_csv(self, df: pd.DataFrame, filename: str = "authors.csv"):
        """
        DataFrameã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        Args:
            df (pd.DataFrame): ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿
            filename (str): ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        try:
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"CSVä¿å­˜å®Œäº†: {filename}")
        except Exception as e:
            print(f"CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def save_to_google_sheets(self, df: pd.DataFrame, sheet_name: str = "æ—¥æœ¬æ–‡è±ªãƒ‡ãƒ¼ã‚¿"):
        """
        DataFrameã‚’Google Sheetsã«ä¿å­˜ï¼ˆåŠ¹ç‡çš„ãƒãƒƒãƒå‡¦ç†ç‰ˆï¼‰
        
        Args:
            df (pd.DataFrame): ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿
            sheet_name (str): ã‚·ãƒ¼ãƒˆå
        """
        if not GSPREAD_AVAILABLE:
            print("Google Sheetsæ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚gspreadãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
            return
            
        if not os.path.exists(self.google_credentials_path):
            print(f"Googleèªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.google_credentials_path}")
            return
        
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                print(f"Google Sheetsã«ä¿å­˜ä¸­... (è©¦è¡Œ {retry_count + 1}/{max_retries})")
                
                # èªè¨¼è¨­å®š
                scope = ['https://spreadsheets.google.com/feeds',
                        'https://www.googleapis.com/auth/drive']
                creds = ServiceAccountCredentials.from_json_keyfile_name(
                    self.google_credentials_path, scope)
                client = gspread.authorize(creds)
                
                # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä½œæˆ
                sheet = client.create(sheet_name)
                worksheet = sheet.get_worksheet(0)
                
                # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
                data_to_write = []
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼è¿½åŠ 
                data_to_write.append(df.columns.tolist())
                
                # ãƒ‡ãƒ¼ã‚¿è¡Œè¿½åŠ 
                for _, row in df.iterrows():
                    # NaNå€¤ã‚’ç©ºæ–‡å­—ã«å¤‰æ›
                    row_data = [str(val) if pd.notna(val) else '' for val in row.tolist()]
                    data_to_write.append(row_data)
                
                print(f"ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(data_to_write)}è¡Œï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼å«ã‚€ï¼‰")
                
                # ãƒãƒƒãƒæ›¸ãè¾¼ã¿ï¼ˆAPIåˆ¶é™å¯¾ç­–ï¼‰
                batch_size = 100  # 100è¡Œãšã¤å‡¦ç†
                total_batches = (len(data_to_write) + batch_size - 1) // batch_size
                
                for i in range(0, len(data_to_write), batch_size):
                    batch_data = data_to_write[i:i + batch_size]
                    batch_num = (i // batch_size) + 1
                    
                    print(f"ãƒãƒƒãƒ {batch_num}/{total_batches} æ›¸ãè¾¼ã¿ä¸­... ({len(batch_data)}è¡Œ)")
                    
                    if i == 0:
                        # æœ€åˆã®ãƒãƒƒãƒï¼šç¯„å›²ã‚’æŒ‡å®šã—ã¦ä¸€æ‹¬æ›´æ–°
                        end_row = len(batch_data)
                        end_col_letter = self._number_to_column_letter(len(df.columns))
                        range_name = f'A1:{end_col_letter}{end_row}'
                        worksheet.update(range_name, batch_data)
                    else:
                        # å¾Œç¶šã®ãƒãƒƒãƒï¼šappend_rowsä½¿ç”¨
                        try:
                            worksheet.append_rows(batch_data)
                        except Exception as append_error:
                            print(f"append_rowså¤±æ•—ã€å€‹åˆ¥è¿½åŠ ã«åˆ‡ã‚Šæ›¿ãˆ: {append_error}")
                            # fallback: 1è¡Œãšã¤è¿½åŠ 
                            for row_data in batch_data:
                                worksheet.append_row(row_data)
                                time.sleep(0.1)  # çŸ­ã„å¾…æ©Ÿ
                    
                    # ãƒãƒƒãƒé–“ã®å¾…æ©Ÿï¼ˆAPIåˆ¶é™å¯¾ç­–ï¼‰
                    if batch_num < total_batches:
                        print("APIåˆ¶é™å¯¾ç­–ã§å¾…æ©Ÿä¸­...")
                        time.sleep(2)
                
                # å…±æœ‰è¨­å®šï¼ˆèª°ã§ã‚‚é–²è¦§å¯èƒ½ï¼‰
                print("å…±æœ‰è¨­å®šä¸­...")
                sheet.share('', perm_type='anyone', role='reader')
                
                print(f"âœ… Google Sheetsä¿å­˜å®Œäº†: {sheet.url}")
                return sheet.url  # æˆåŠŸã—ãŸå ´åˆã¯URLã‚’è¿”ã—ã¦çµ‚äº†
                
            except Exception as e:
                retry_count += 1
                error_msg = str(e)
                
                if "RATE_LIMIT_EXCEEDED" in error_msg or "429" in error_msg:
                    wait_time = 60 * retry_count  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                    print(f"âš ï¸ APIåˆ¶é™ã‚¨ãƒ©ãƒ¼ã€‚{wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                    time.sleep(wait_time)
                elif "RESOURCE_EXHAUSTED" in error_msg:
                    wait_time = 120  # 2åˆ†å¾…æ©Ÿ
                    print(f"âš ï¸ ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ã‚¨ãƒ©ãƒ¼ã€‚{wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                    time.sleep(wait_time)
                else:
                    print(f"âŒ Google Sheetsä¿å­˜ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {retry_count}): {e}")
                    if retry_count < max_retries:
                        print(f"10ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                        time.sleep(10)
        
        print(f"âŒ Google Sheetsä¿å­˜å¤±æ•—ï¼š{max_retries}å›è©¦è¡Œã—ã¾ã—ãŸãŒæˆåŠŸã—ã¾ã›ã‚“ã§ã—ãŸ")
        return None
    
    def _number_to_column_letter(self, num: int) -> str:
        """
        æ•°å€¤ã‚’åˆ—æ–‡å­—ï¼ˆA, B, C, ... AA, AB, ...ï¼‰ã«å¤‰æ›
        
        Args:
            num (int): åˆ—ç•ªå·ï¼ˆ1ã‹ã‚‰é–‹å§‹ï¼‰
            
        Returns:
            str: åˆ—æ–‡å­—
        """
        result = ""
        while num > 0:
            num -= 1
            result = chr(num % 26 + ord('A')) + result
            num //= 26
        return result
    
    def save_to_google_sheets_efficient(self, df: pd.DataFrame, sheet_name: str = "æ—¥æœ¬æ–‡è±ªãƒ‡ãƒ¼ã‚¿"):
        """
        ã‚ˆã‚ŠåŠ¹ç‡çš„ãªGoogle Sheetsä¿å­˜ï¼ˆå°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ï¼‰
        
        Args:
            df (pd.DataFrame): ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿
            sheet_name (str): ã‚·ãƒ¼ãƒˆå
        """
        if not GSPREAD_AVAILABLE:
            print("Google Sheetsæ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
            return
            
        try:
            print(f"åŠ¹ç‡çš„Google Sheetsä¿å­˜é–‹å§‹: {sheet_name}")
            
            # èªè¨¼
            scope = ['https://spreadsheets.google.com/feeds',
                    'https://www.googleapis.com/auth/drive']
            creds = ServiceAccountCredentials.from_json_keyfile_name(
                self.google_credentials_path, scope)
            client = gspread.authorize(creds)
            
            # æ—¢å­˜ã®ã‚·ãƒ¼ãƒˆã‚’æ¤œç´¢
            try:
                sheet = client.open(sheet_name)
                print(f"æ—¢å­˜ã®ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã‚’æ›´æ–°ã—ã¾ã™")
                worksheet = sheet.get_worksheet(0)
                worksheet.clear()  # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢
            except gspread.SpreadsheetNotFound:
                # æ–°è¦ä½œæˆ
                sheet = client.create(sheet_name)
                worksheet = sheet.get_worksheet(0)
                print(f"æ–°ã—ã„ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã‚’ä½œæˆã—ã¾ã—ãŸ")
            
            # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«æ›´æ–°
            all_data = [df.columns.tolist()] + df.values.tolist()
            
            # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ã¦å‡¦ç†æ–¹æ³•ã‚’é¸æŠ
            if len(all_data) <= 1000:  # 1000è¡Œä»¥ä¸‹ã¯ä¸€æ‹¬å‡¦ç†
                end_col = self._number_to_column_letter(len(df.columns))
                range_name = f'A1:{end_col}{len(all_data)}'
                worksheet.update(range_name, all_data)
                print(f"ä¸€æ‹¬æ›´æ–°å®Œäº†: {len(all_data)}è¡Œ")
            else:
                # å¤§ããªãƒ‡ãƒ¼ã‚¿ã¯åˆ†å‰²å‡¦ç†
                self.save_to_google_sheets(df, sheet_name)
                return
            
            # å…±æœ‰è¨­å®š
            sheet.share('', perm_type='anyone', role='reader')
            
            print(f"âœ… åŠ¹ç‡çš„ä¿å­˜å®Œäº†: {sheet.url}")
            return sheet.url
            
        except Exception as e:
            print(f"âŒ åŠ¹ç‡çš„ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šé€šå¸¸ã®ä¿å­˜æ–¹æ³•
            print("é€šå¸¸ã®ä¿å­˜æ–¹æ³•ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
            return self.save_to_google_sheets(df, sheet_name)
    
    def process_all_authors(self):
        """
        å…¨ä½“ã®å‡¦ç†ã‚’å®Ÿè¡Œï¼ˆè©³ç´°ä½æ‰€å¯¾å¿œç‰ˆï¼‰
        """
        print("=== æ—¥æœ¬æ–‡è±ªæƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ï¼ˆè©³ç´°ä½æ‰€å¯¾å¿œç‰ˆï¼‰===")
        
        # 1. ä½œå®¶ä¸€è¦§å–å¾—
        authors = self.get_authors_list()
        if not authors:
            print("ä½œå®¶ä¸€è¦§ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return
        
        # 2. å„ä½œå®¶ã®æƒ…å ±åé›†
        for i, author in enumerate(authors, 1):
            print(f"\n[{i}/{len(authors)}] å‡¦ç†ä¸­: {author}")
            
            # Wikipediaæœ¬æ–‡å–å¾—
            content = self.get_wikipedia_content(author)
            if not content:
                continue
            
            # åˆæœŸæŠ½å‡ºï¼ˆè©³ç´°åœ°åæƒ…å ±ã‚‚å«ã‚€ï¼‰
            extracted_data = self.extract_works_and_places(author, content)
            
            # AIè£œå®Œ
            enhanced_data = self.enhance_with_ai(author, extracted_data)
            
            # ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆè©³ç´°åœ°åæƒ…å ±ã‚‚ä¿å­˜ï¼‰
            author_record = {
                'name': author,
                'works': enhanced_data['works'],
                'places': enhanced_data['places']
            }
            
            # è©³ç´°åœ°åæƒ…å ±ã‚‚ä¿å­˜
            if 'detailed_places' in extracted_data:
                author_record['detailed_places'] = extracted_data['detailed_places']
            
            self.authors_data.append(author_record)
            
            # é€²æ—è¡¨ç¤º
            if i % 5 == 0:
                print(f"é€²æ—: {i}/{len(authors)} å®Œäº†")
        
        # 3. ãƒ‡ãƒ¼ã‚¿æ•´å½¢ã¨å‡ºåŠ›
        if self.authors_data:
            # æ¨™æº–ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆï¼ˆè©³ç´°ä½æ‰€å¯¾å¿œï¼‰
            df = self.create_dataframe()
            
            # Google Mapsç‰¹åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
            maps_df = self.create_detailed_dataframe()
            
            # CSVå‡ºåŠ›
            self.save_to_csv(df, "authors_detailed.csv")
            if not maps_df.empty:
                self.save_to_csv(maps_df, "authors_googlemaps.csv")
            
            # Google Sheetså‡ºåŠ›
            if GSPREAD_AVAILABLE:
                self.save_to_google_sheets(df, "æ—¥æœ¬æ–‡è±ªãƒ‡ãƒ¼ã‚¿ï¼ˆè©³ç´°ä½æ‰€ç‰ˆï¼‰")
                if not maps_df.empty:
                    self.save_to_google_sheets(maps_df, "æ—¥æœ¬æ–‡è±ªGoogleMapsç”¨ãƒ‡ãƒ¼ã‚¿")
            else:
                print("Google Sheetså‡ºåŠ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆgspreadãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ï¼‰")
            
            # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
            print(f"\n=== å‡¦ç†å®Œäº† ===")
            print(f"å‡¦ç†ã—ãŸä½œå®¶æ•°: {len(self.authors_data)}")
            print(f"æ¨™æº–ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›è¡Œæ•°: {len(df)}")
            print(f"Google Mapsç”¨ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›è¡Œæ•°: {len(maps_df)}")
            
            # è©³ç´°åœ°åæŠ½å‡ºçµ±è¨ˆ
            total_detailed_places = sum(len(data.get('detailed_places', [])) for data in self.authors_data)
            maps_ready_count = len([row for _, row in maps_df.iterrows() if row['Google Mapsæº–å‚™æ¸ˆã¿'] == 'â—‹'])
            
            print(f"æŠ½å‡ºã—ãŸè©³ç´°åœ°åæ•°: {total_detailed_places}")
            print(f"Google Mapsæº–å‚™æ¸ˆã¿åœ°åæ•°: {maps_ready_count}")
            print(f"Google Mapsæº–å‚™ç‡: {maps_ready_count/len(maps_df)*100:.1f}%" if len(maps_df) > 0 else "Google Mapsæº–å‚™ç‡: N/A")
            
        else:
            print("åé›†ã§ããŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    def _parse_ai_response(self, response: str) -> Dict[str, List[str]]:
        """
        AIå¿œç­”ã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        
        Args:
            response (str): AIå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            Dict[str, List[str]]: æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        result = {"works": [], "places": []}
        
        lines = response.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if 'ä»£è¡¨ä½œ:' in line:
                current_section = 'works'
            elif 'æ‰€ç¸ã®åœ°:' in line:
                current_section = 'places'
            elif line.startswith('- ') and current_section:
                item = line[2:].strip()
                if current_section == 'works':
                    result['works'].append(item)
                elif current_section == 'places':
                    # æ‹¬å¼§å†…ã®èª¬æ˜ã‚’é™¤å»
                    place = re.sub(r'ï¼ˆ.+?ï¼‰', '', item).strip()
                    result['places'].append(place)
        
        return result
    
    def _is_valid_author_name(self, name: str) -> bool:
        """
        æœ‰åŠ¹ãªæ–‡è±ªåã‹ãƒã‚§ãƒƒã‚¯
        
        Args:
            name (str): ãƒã‚§ãƒƒã‚¯ã™ã‚‹åå‰
            
        Returns:
            bool: æœ‰åŠ¹ãªåå‰ã®å ´åˆTrue
        """
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_patterns = [
            r'^\d',  # æ•°å­—ã§å§‹ã¾ã‚‹
            r'Category:',  # ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸
            r'Template:',  # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒšãƒ¼ã‚¸
            r'List of',  # ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸
            r'ä¸€è¦§$',  # ä¸€è¦§ã§çµ‚ã‚ã‚‹
            r'å¹´$',  # å¹´ã§çµ‚ã‚ã‚‹
        ]
        
        for pattern in exclude_patterns:
            if re.search(pattern, name):
                return False
        
        # æ—¥æœ¬èªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆUnicodeç¯„å›²ã§åˆ¤å®šï¼‰
        japanese_chars = False
        for char in name:
            # ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã®Unicodeç¯„å›²
            if ('\u3040' <= char <= '\u309F') or \
               ('\u30A0' <= char <= '\u30FF') or \
               ('\u4E00' <= char <= '\u9FFF'):
                japanese_chars = True
                break
        
        return japanese_chars and len(name) >= 2 and len(name) <= 10
    
    def export_for_googlemaps(self, output_file: str = "googlemaps_export.csv"):
        """
        Google Mapsç”¨ã®ç‰¹åˆ¥ãªã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
        
        Args:
            output_file (str): å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        print("Google Mapsç”¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä½œæˆä¸­...")
        
        maps_data = []
        for author_data in self.authors_data:
            author_name = author_data['name']
            detailed_places = author_data.get('detailed_places', [])
            
            for place_info in detailed_places:
                maps_entry = {
                    'ä½œå®¶å': author_name,
                    'åœ°å': place_info['name'],
                    'ç¨®é¡': place_info['type'],
                    'æ¤œç´¢ã‚¯ã‚¨ãƒª': self._create_maps_query(author_name, place_info),
                    'Maps URL': self._create_maps_url(place_info['name']),
                    'æ–‡è„ˆ': place_info.get('context', '')[:150],
                    'æº–å‚™çŠ¶æ³': 'â—‹' if self._is_maps_ready(place_info['name']) else 'è¦ç¢ºèª'
                }
                maps_data.append(maps_entry)
        
        if maps_data:
            df = pd.DataFrame(maps_data)
            self.save_to_csv(df, output_file)
            print(f"Google Mapsç”¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {output_file}")
            return df
        else:
            print("Google Mapsç”¨ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return pd.DataFrame()
    
    def _create_maps_query(self, author_name: str, place_info: Dict) -> str:
        """
        Google Mapsæ¤œç´¢ç”¨ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
        
        Args:
            author_name (str): ä½œå®¶å
            place_info (Dict): åœ°åæƒ…å ±
            
        Returns:
            str: æ¤œç´¢ã‚¯ã‚¨ãƒª
        """
        place_name = place_info['name']
        place_type = place_info['type']
        
        # è¨˜å¿µé¤¨ãƒ»æ–‡å­¦é¤¨ã®å ´åˆ
        if place_type == 'è¨˜å¿µé¤¨ãƒ»æ–‡å­¦æ–½è¨­' or 'è¨˜å¿µé¤¨' in place_name:
            return f"{place_name}"
        
        # å¢“æ‰€ã®å ´åˆ
        elif place_type == 'å¢“æ‰€':
            return f"{author_name} å¢“æ‰€ {place_name}"
        
        # å‡ºç”Ÿåœ°ãƒ»å±…ä½åœ°ã®å ´åˆ
        elif place_type in ['å‡ºç”Ÿåœ°', 'å±…ä½åœ°']:
            return f"{place_name} {author_name}"
        
        # ãã®ä»–ã®å ´åˆ
        else:
            return place_name
    
    def _create_maps_url(self, place_name: str) -> str:
        """
        Google Maps URLã‚’ç”Ÿæˆ
        
        Args:
            place_name (str): åœ°å
            
        Returns:
            str: Google Maps URL
        """
        import urllib.parse
        query = urllib.parse.quote(place_name)
        return f"https://www.google.com/maps/search/{query}"
    
    def generate_maps_summary(self) -> Dict[str, int]:
        """
        Google Mapsé€£æºç”¨ã®çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆ
        
        Returns:
            Dict[str, int]: çµ±è¨ˆæƒ…å ±
        """
        stats = {
            'ç·åœ°åæ•°': 0,
            'è¨˜å¿µé¤¨ãƒ»æ–‡å­¦æ–½è¨­': 0,
            'å‡ºç”Ÿåœ°': 0,
            'å±…ä½åœ°': 0,
            'å¢“æ‰€': 0,
            'Mapsæº–å‚™æ¸ˆã¿': 0,
            'è¦ç¢ºèª': 0
        }
        
        for author_data in self.authors_data:
            detailed_places = author_data.get('detailed_places', [])
            stats['ç·åœ°åæ•°'] += len(detailed_places)
            
            for place_info in detailed_places:
                place_type = place_info['type']
                if place_type in stats:
                    stats[place_type] += 1
                
                if self._is_maps_ready(place_info['name']):
                    stats['Mapsæº–å‚™æ¸ˆã¿'] += 1
                else:
                    stats['è¦ç¢ºèª'] += 1
        
        return stats
        
if __name__ == "__main__":
    collector = BungoCollector()
    collector.process_all_authors() 