#!/usr/bin/env python3
"""
æ–‡è±ªã‚†ã‹ã‚Šåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  - å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å…¨ä½œå®¶ã®æœ€å¤§ä½œå“æ•°ã§ã®ä¸€æ°—é€šè²«å®Ÿè¡Œ:
1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
2. å…¨ä½œå®¶ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆæœ€å¤§ä½œå“æ•°ï¼‰
3. åœ°åæŠ½å‡ºãƒ»ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
4. å…¨å½¢å¼ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆCSVã€GeoJSONã€çµ±è¨ˆæƒ…å ±ï¼‰
5. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

ä½¿ç”¨æ–¹æ³•:
    python scripts/full_pipeline.py [--backup] [--max-works N] [--verbose]
"""

import os
import sys
import time
import argparse
import logging
from datetime import datetime
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src" / "core"))
sys.path.insert(0, str(project_root / "src" / "export"))
sys.path.insert(0, str(project_root / "src" / "utils"))

try:
    from collect import BungoDataCollector
    from export_csv import export_db_to_csv
    from export_geojson import GeoJSONExporter
    from db_utils import BungoDatabase
except ImportError as e:
    print(f"âŒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("PYTHONPATH ã‚’è¨­å®šã™ã‚‹ã‹ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„")
    sys.exit(1)


class FullPipelineExecutor:
    """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå™¨"""
    
    # å¯¾è±¡ä½œå®¶ï¼ˆæœ€å¤§ç¶²ç¾…ï¼‰
    MAJOR_AUTHORS = [
        "å¤ç›®æ¼±çŸ³", "èŠ¥å·é¾ä¹‹ä»‹", "å¤ªå®°æ²»", "å·ç«¯åº·æˆ", "å®®æ²¢è³¢æ²»",
        "æ£®é´å¤–", "æ¨‹å£ä¸€è‘‰", "çŸ³å·å•„æœ¨", "ä¸è¬é‡æ™¶å­", "æ­£å²¡å­è¦",
        "è°·å´æ½¤ä¸€éƒ", "ä¸‰å³¶ç”±ç´€å¤«", "å¿—è³€ç›´å“‰", "å³¶å´è—¤æ‘", "åªå†…é€é¥",
        "æ³‰é¡èŠ±", "å°¾å´ç´…è‘‰", "å¹¸ç”°éœ²ä¼´", "å›½æœ¨ç”°ç‹¬æ­©", "å¾³å†¨è˜†èŠ±"
    ]
    
    def __init__(self, max_works: int = 10, backup: bool = True, verbose: bool = False):
        self.max_works = max_works
        self.backup = backup
        self.verbose = verbose
        self.start_time = datetime.now()
        
        # ãƒ­ã‚°è¨­å®š
        log_level = logging.DEBUG if verbose else logging.INFO
        log_filename = f"full_pipeline_{int(time.time())}.log"
        
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler(log_filename)
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"ğŸš€ å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œé–‹å§‹: {self.start_time}")
        self.logger.info(f"   å¯¾è±¡ä½œå®¶: {len(self.MAJOR_AUTHORS)}å")
        self.logger.info(f"   æœ€å¤§ä½œå“æ•°: {max_works}")
        self.logger.info(f"   ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup}")
        
        self.stats = {
            'start_time': self.start_time,
            'phases_completed': [],
            'total_authors': len(self.MAJOR_AUTHORS),
            'successful_authors': 0,
            'total_works': 0,
            'total_places': 0,
            'total_geocoded': 0,
            'export_files': [],
            'errors': []
        }

    def phase1_backup_and_initialize(self) -> bool:
        """ãƒ•ã‚§ãƒ¼ã‚º1: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        self.logger.info("ğŸ“¦ ãƒ•ã‚§ãƒ¼ã‚º1: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»åˆæœŸåŒ–é–‹å§‹")
        
        try:
            if self.backup:
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
                backup_dir = f"data/backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                os.makedirs(backup_dir, exist_ok=True)
                
                # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                backup_files = [
                    "data/bungo_production.db",
                    "data/geocode_cache.json", 
                    "data/output/bungo_production_export.geojson",
                    "data/output/bungo_stats.json"
                ]
                
                for file_path in backup_files:
                    if os.path.exists(file_path):
                        backup_path = os.path.join(backup_dir, os.path.basename(file_path))
                        os.rename(file_path, backup_path)
                        self.logger.info(f"   ğŸ“ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {file_path} -> {backup_path}")
                
                self.logger.info(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_dir}")
            
            # å¤ã„å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«ç§»å‹•
            if os.path.exists("data/output"):
                archive_dir = "data/output/archived"
                os.makedirs(archive_dir, exist_ok=True)
                
                for file in os.listdir("data/output"):
                    if file.startswith("bungo_") and file.endswith((".csv", ".geojson", ".json")):
                        if file != "README.md":
                            src = f"data/output/{file}"
                            dst = f"{archive_dir}/{file}"
                            if os.path.exists(src):
                                os.rename(src, dst)
                                self.logger.info(f"   ğŸ—‚ï¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: {file}")
            
            self.stats['phases_completed'].append('backup_initialize')
            self.logger.info("âœ… ãƒ•ã‚§ãƒ¼ã‚º1å®Œäº†")
            return True
            
        except Exception as e:
            error_msg = f"âŒ ãƒ•ã‚§ãƒ¼ã‚º1ã‚¨ãƒ©ãƒ¼: {e}"
            self.logger.error(error_msg)
            self.stats['errors'].append(error_msg)
            return False

    def phase2_collect_data(self) -> bool:
        """ãƒ•ã‚§ãƒ¼ã‚º2: å…¨ä½œå®¶ãƒ‡ãƒ¼ã‚¿åé›†"""
        self.logger.info("ğŸ“š ãƒ•ã‚§ãƒ¼ã‚º2: ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        
        try:
            collector = BungoDataCollector("data/bungo_production.db")
            
            # å…¨ä½œå®¶ä¸€æ‹¬åé›†
            collection_stats = collector.collect_multiple_authors(
                self.MAJOR_AUTHORS, 
                self.max_works
            )
            
            # çµ±è¨ˆæ›´æ–°
            self.stats['successful_authors'] = collection_stats['successful_authors']
            self.stats['total_works'] = collection_stats['total_works']
            self.stats['total_places'] = collection_stats['total_places']
            self.stats['total_geocoded'] = collection_stats['total_geocoded']
            
            # è©³ç´°ãƒ­ã‚°
            self.logger.info(f"ğŸ“Š åé›†çµæœ:")
            self.logger.info(f"   æˆåŠŸä½œå®¶: {self.stats['successful_authors']}/{self.stats['total_authors']}")
            self.logger.info(f"   ç·ä½œå“æ•°: {self.stats['total_works']}")
            self.logger.info(f"   ç·åœ°åæ•°: {self.stats['total_places']}")
            self.logger.info(f"   ã‚¸ã‚ªã‚³ãƒ¼ãƒ‰: {self.stats['total_geocoded']}")
            
            collector.close()
            
            self.stats['phases_completed'].append('data_collection')
            self.logger.info("âœ… ãƒ•ã‚§ãƒ¼ã‚º2å®Œäº†")
            return True
            
        except Exception as e:
            error_msg = f"âŒ ãƒ•ã‚§ãƒ¼ã‚º2ã‚¨ãƒ©ãƒ¼: {e}"
            self.logger.error(error_msg)
            self.stats['errors'].append(error_msg)
            return False

    def phase3_export_all_formats(self) -> bool:
        """ãƒ•ã‚§ãƒ¼ã‚º3: å…¨å½¢å¼ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        self.logger.info("ğŸ“¤ ãƒ•ã‚§ãƒ¼ã‚º3: å…¨å½¢å¼ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–‹å§‹")
        
        try:
            db = BungoDatabase("sqlite", "data/bungo_production.db")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # 1. CSVå…¨å½¢å¼ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            csv_results = export_db_to_csv(
                db_path="data/bungo_production.db",
                output_dir="data/output",
                export_type="all",
                prefix=f"bungo_export_{timestamp}"
            )
            
            # CSVçµæœã®å‡¦ç†
            for export_type, filepath in csv_results.items():
                self.stats['export_files'].append(filepath)
            self.logger.info(f"âœ… CSV ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {len(csv_results)}ãƒ•ã‚¡ã‚¤ãƒ«")
            
            # 2. GeoJSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            geojson_exporter = GeoJSONExporter("data/output")
            
            geojson_file = geojson_exporter.export_from_database(
                db, f"bungo_production_export_{timestamp}.geojson"
            )
            if geojson_file:
                self.stats['export_files'].append(geojson_file)
                self.logger.info(f"âœ… GeoJSON ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {geojson_file}")
            
            # 3. çµ±è¨ˆæƒ…å ±ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            stats_file = geojson_exporter.export_summary_stats(
                db, f"bungo_stats_{timestamp}.json"
            )
            if stats_file:
                self.stats['export_files'].append(stats_file)
                self.logger.info(f"âœ… çµ±è¨ˆæƒ…å ±ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {stats_file}")
            
            # 4. åˆ†æç”¨CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            analysis_file = geojson_exporter.export_csv_for_analysis(
                db, f"bungo_analysis_{timestamp}.csv"
            )
            if analysis_file:
                self.stats['export_files'].append(analysis_file)
                self.logger.info(f"âœ… åˆ†æç”¨CSV ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {analysis_file}")
            
            db.close()
            
            self.stats['phases_completed'].append('export_all_formats')
            self.logger.info("âœ… ãƒ•ã‚§ãƒ¼ã‚º3å®Œäº†")
            return True
            
        except Exception as e:
            error_msg = f"âŒ ãƒ•ã‚§ãƒ¼ã‚º3ã‚¨ãƒ©ãƒ¼: {e}"
            self.logger.error(error_msg)
            self.stats['errors'].append(error_msg)
            return False

    def phase4_generate_report(self) -> bool:
        """ãƒ•ã‚§ãƒ¼ã‚º4: å®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        self.logger.info("ğŸ“‹ ãƒ•ã‚§ãƒ¼ã‚º4: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
        
        try:
            end_time = datetime.now()
            duration = end_time - self.start_time
            
            report = {
                "execution_summary": {
                    "start_time": self.start_time.isoformat(),
                    "end_time": end_time.isoformat(),
                    "duration_seconds": duration.total_seconds(),
                    "duration_formatted": str(duration),
                    "success": len(self.stats['errors']) == 0
                },
                "pipeline_stats": {
                    "phases_completed": self.stats['phases_completed'],
                    "target_authors": self.stats['total_authors'],
                    "successful_authors": self.stats['successful_authors'],
                    "total_works": self.stats['total_works'],
                    "total_places": self.stats['total_places'],
                    "total_geocoded": self.stats['total_geocoded'],
                    "success_rate": f"{(self.stats['successful_authors']/self.stats['total_authors']*100):.1f}%"
                },
                "export_summary": {
                    "total_files": len(self.stats['export_files']),
                    "files": self.stats['export_files']
                },
                "errors": self.stats['errors']
            }
            
            # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
            import json
            report_file = f"data/output/pipeline_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_file}")
            
            # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
            print(f"\nğŸ‰ å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå®Œäº†ï¼")
            print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration}")
            print(f"ğŸ‘¥ å‡¦ç†ä½œå®¶: {self.stats['successful_authors']}/{self.stats['total_authors']} ({(self.stats['successful_authors']/self.stats['total_authors']*100):.1f}%)")
            print(f"ğŸ“š åé›†ä½œå“: {self.stats['total_works']}ä½œå“")
            print(f"ğŸ—ºï¸  æŠ½å‡ºåœ°å: {self.stats['total_places']}ç®‡æ‰€")
            print(f"ğŸ“ ã‚¸ã‚ªã‚³ãƒ¼ãƒ‰: {self.stats['total_geocoded']}ç®‡æ‰€")
            print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {len(self.stats['export_files'])}ãƒ•ã‚¡ã‚¤ãƒ«")
            print(f"ğŸ“‹ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")
            
            if self.stats['errors']:
                print(f"âš ï¸  ã‚¨ãƒ©ãƒ¼: {len(self.stats['errors'])}ä»¶")
                for error in self.stats['errors'][:3]:  # æœ€åˆã®3ä»¶ã®ã¿è¡¨ç¤º
                    print(f"   - {error}")
                if len(self.stats['errors']) > 3:
                    print(f"   ...ä»–{len(self.stats['errors'])-3}ä»¶")
            
            self.stats['phases_completed'].append('generate_report')
            return True
            
        except Exception as e:
            error_msg = f"âŒ ãƒ•ã‚§ãƒ¼ã‚º4ã‚¨ãƒ©ãƒ¼: {e}"
            self.logger.error(error_msg)
            self.stats['errors'].append(error_msg)
            return False

    def execute(self) -> bool:
        """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        self.logger.info("ğŸš€ğŸš€ğŸš€ å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œé–‹å§‹ ğŸš€ğŸš€ğŸš€")
        
        phases = [
            ("ãƒ•ã‚§ãƒ¼ã‚º1: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»åˆæœŸåŒ–", self.phase1_backup_and_initialize),
            ("ãƒ•ã‚§ãƒ¼ã‚º2: ãƒ‡ãƒ¼ã‚¿åé›†", self.phase2_collect_data),
            ("ãƒ•ã‚§ãƒ¼ã‚º3: å…¨å½¢å¼ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", self.phase3_export_all_formats),
            ("ãƒ•ã‚§ãƒ¼ã‚º4: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ", self.phase4_generate_report)
        ]
        
        for phase_name, phase_func in phases:
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"é–‹å§‹: {phase_name}")
            self.logger.info(f"{'='*60}")
            
            success = phase_func()
            if not success:
                self.logger.error(f"âŒ {phase_name} å¤±æ•— - ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸­æ–­")
                return False
            
            self.logger.info(f"âœ… {phase_name} å®Œäº†")
        
        self.logger.info("\nğŸ‰ğŸ‰ğŸ‰ å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡ŒæˆåŠŸ ğŸ‰ğŸ‰ğŸ‰")
        return True


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="æ–‡è±ªã‚†ã‹ã‚Šåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  - å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å®Ÿè¡Œä¾‹:
  # åŸºæœ¬å®Ÿè¡Œï¼ˆæœ€å¤§10ä½œå“/ä½œå®¶ï¼‰
  python scripts/full_pipeline.py

  # æœ€å¤§ä½œå“æ•°æŒ‡å®š
  python scripts/full_pipeline.py --max-works 15

  # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãªã—ãƒ»è©³ç´°ãƒ­ã‚°
  python scripts/full_pipeline.py --no-backup --verbose

å‡¦ç†å†…å®¹:
  1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
  2. 20åã®ä¸»è¦ä½œå®¶ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åé›†
  3. GiNZAåœ°åæŠ½å‡ºãƒ»ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
  4. CSV/GeoJSON/çµ±è¨ˆæƒ…å ±ã®å…¨å½¢å¼ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
  5. å®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        """
    )
    
    parser.add_argument('--max-works', type=int, default=10, 
                       help='ä½œå®¶ã‚ãŸã‚Šã®æœ€å¤§ä½œå“æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10)')
    parser.add_argument('--no-backup', action='store_true',
                       help='æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è©³ç´°ãƒ­ã‚°å‡ºåŠ›')
    
    args = parser.parse_args()
    
    try:
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå™¨åˆæœŸåŒ–
        executor = FullPipelineExecutor(
            max_works=args.max_works,
            backup=not args.no_backup,
            verbose=args.verbose
        )
        
        # å®Ÿè¡Œ
        success = executor.execute()
        
        return 0 if success else 1
        
    except KeyboardInterrupt:
        print("\nâš ï¸ å‡¦ç†ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        return 130
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main()) 