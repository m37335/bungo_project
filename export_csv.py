#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSV ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…å®¹ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
"""

import csv
import os
from datetime import datetime
from typing import List, Dict, Optional
from pathlib import Path

from db_utils import BungoDatabase

class CSVExporter:
    """CSV ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, db_path: str = "test_ginza.db", output_dir: str = "output"):
        self.db_path = db_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
    def export_authors(self, filename: Optional[str] = None) -> str:
        """ä½œè€…ãƒ‡ãƒ¼ã‚¿ã‚’CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"authors_{timestamp}.csv"
        
        filepath = self.output_dir / filename
        
        db = BungoDatabase("sqlite", self.db_path)
        authors = db.search_authors("")  # å…¨ä½œè€…å–å¾—
        db.close()
        
        # CSVãƒ˜ãƒƒãƒ€ãƒ¼
        fieldnames = ['author_id', 'name', 'birth_year', 'death_year', 'wikipedia_url']
        
        with open(filepath, 'w', encoding='utf-8', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for author in authors:
                writer.writerow({
                    'author_id': author.get('author_id', ''),
                    'name': author.get('name', ''),
                    'birth_year': author.get('birth_year', ''),
                    'death_year': author.get('death_year', ''),
                    'wikipedia_url': author.get('wikipedia_url', '')
                })
        
        print(f"âœ… ä½œè€…ãƒ‡ãƒ¼ã‚¿CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {filepath}")
        print(f"   ä»¶æ•°: {len(authors)}ä»¶")
        return str(filepath)
    
    def export_works(self, filename: Optional[str] = None) -> str:
        """ä½œå“ãƒ‡ãƒ¼ã‚¿ã‚’CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"works_{timestamp}.csv"
        
        filepath = self.output_dir / filename
        
        db = BungoDatabase("sqlite", self.db_path)
        works = db.search_works("")  # å…¨ä½œå“å–å¾—
        db.close()
        
        # CSVãƒ˜ãƒƒãƒ€ãƒ¼
        fieldnames = ['work_id', 'author_id', 'author_name', 'title', 'publication_year', 'genre', 'aozora_url']
        
        with open(filepath, 'w', encoding='utf-8', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for work in works:
                writer.writerow({
                    'work_id': work.get('work_id', ''),
                    'author_id': work.get('author_id', ''),
                    'author_name': work.get('author_name', ''),
                    'title': work.get('title', ''),
                    'publication_year': work.get('publication_year', ''),
                    'genre': work.get('genre', ''),
                    'aozora_url': work.get('aozora_url', '')
                })
        
        print(f"âœ… ä½œå“ãƒ‡ãƒ¼ã‚¿CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {filepath}")
        print(f"   ä»¶æ•°: {len(works)}ä»¶")
        return str(filepath)
    
    def export_places(self, filename: Optional[str] = None, geocoded_only: bool = False) -> str:
        """åœ°åãƒ‡ãƒ¼ã‚¿ã‚’CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            suffix = "_geocoded" if geocoded_only else ""
            filename = f"places{suffix}_{timestamp}.csv"
        
        filepath = self.output_dir / filename
        
        db = BungoDatabase("sqlite", self.db_path)
        places = db.search_places("")  # å…¨åœ°åå–å¾—
        db.close()
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if geocoded_only:
            places = [p for p in places if p.get('latitude') and p.get('longitude')]
        
        # CSVãƒ˜ãƒƒãƒ€ãƒ¼
        fieldnames = [
            'place_id', 'work_id', 'author_name', 'work_title', 'place_name',
            'latitude', 'longitude', 'address', 'sentence', 'before_text', 'after_text',
            'relevance_score', 'maps_url'
        ]
        
        with open(filepath, 'w', encoding='utf-8', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for place in places:
                writer.writerow({
                    'place_id': place.get('place_id', ''),
                    'work_id': place.get('work_id', ''),
                    'author_name': place.get('author_name', ''),
                    'work_title': place.get('work_title', ''),
                    'place_name': place.get('place_name', ''),
                    'latitude': place.get('latitude', ''),
                    'longitude': place.get('longitude', ''),
                    'address': place.get('address', ''),
                    'sentence': place.get('sentence', ''),
                    'before_text': place.get('before_text', ''),
                    'after_text': place.get('after_text', ''),
                    'relevance_score': place.get('relevance_score', ''),
                    'maps_url': place.get('maps_url', '')
                })
        
        print(f"âœ… åœ°åãƒ‡ãƒ¼ã‚¿CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {filepath}")
        print(f"   ä»¶æ•°: {len(places)}ä»¶")
        if geocoded_only:
            print(f"   ï¼ˆã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¸ˆã¿ã®ã¿ï¼‰")
        return str(filepath)
    
    def export_combined_data(self, filename: Optional[str] = None, author_filter: Optional[str] = None, 
                           work_filter: Optional[str] = None) -> str:
        """çµåˆãƒ‡ãƒ¼ã‚¿ã‚’CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆä½œè€…-ä½œå“-åœ°åã®é–¢é€£ãƒ‡ãƒ¼ã‚¿ï¼‰"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"bungo_combined_{timestamp}.csv"
        
        filepath = self.output_dir / filename
        
        db = BungoDatabase("sqlite", self.db_path)
        
        # çµåˆã‚¯ã‚¨ãƒªã§å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—
        query = """
        SELECT 
            a.name as author_name,
            a.birth_year,
            a.death_year,
            w.title as work_title,
            w.publication_year,
            w.genre,
            w.aozora_url,
            p.place_name,
            p.latitude,
            p.longitude,
            p.address,
            p.sentence,
            p.before_text,
            p.after_text
        FROM places p
        LEFT JOIN works w ON p.work_id = w.work_id
        LEFT JOIN authors a ON w.author_id = a.author_id
        """
        
        # ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶è¿½åŠ 
        conditions = []
        if author_filter:
            conditions.append(f"a.name LIKE '%{author_filter}%'")
        if work_filter:
            conditions.append(f"w.title LIKE '%{work_filter}%'")
        
        if conditions:
            query += " WHERE " + " AND ".join(conditions)
        
        query += " ORDER BY a.name, w.title, p.place_name"
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        import sqlite3
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(query)
        rows = cursor.fetchall()
        conn.close()
        db.close()
        
        # CSVãƒ˜ãƒƒãƒ€ãƒ¼
        fieldnames = [
            'author_name', 'birth_year', 'death_year',
            'work_title', 'publication_year', 'genre', 'aozora_url',
            'place_name', 'latitude', 'longitude', 'address',
            'sentence', 'before_text', 'after_text'
        ]
        
        with open(filepath, 'w', encoding='utf-8', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for row in rows:
                writer.writerow(dict(row))
        
        print(f"âœ… çµåˆãƒ‡ãƒ¼ã‚¿CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {filepath}")
        print(f"   ä»¶æ•°: {len(rows)}ä»¶")
        if author_filter:
            print(f"   ä½œè€…ãƒ•ã‚£ãƒ«ã‚¿: {author_filter}")
        if work_filter:
            print(f"   ä½œå“ãƒ•ã‚£ãƒ«ã‚¿: {work_filter}")
        return str(filepath)
    
    def export_all(self, prefix: Optional[str] = None) -> Dict[str, str]:
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        if not prefix:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            prefix = f"bungo_export_{timestamp}"
        
        print(f"ğŸš€ æ–‡è±ªã‚†ã‹ã‚Šåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  - å…¨ãƒ‡ãƒ¼ã‚¿CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–‹å§‹")
        print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        print("=" * 50)
        
        results = {}
        
        # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        results['authors'] = self.export_authors(f"{prefix}_authors.csv")
        results['works'] = self.export_works(f"{prefix}_works.csv")
        results['places'] = self.export_places(f"{prefix}_places.csv")
        results['places_geocoded'] = self.export_places(f"{prefix}_places_geocoded.csv", geocoded_only=True)
        results['combined'] = self.export_combined_data(f"{prefix}_combined.csv")
        
        print("=" * 50)
        print(f"ğŸ‰ å…¨ãƒ‡ãƒ¼ã‚¿CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†ï¼")
        print(f"å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(results)}ä»¶")
        
        return results
    
    def get_export_stats(self) -> Dict[str, int]:
        """ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆçµ±è¨ˆæƒ…å ±å–å¾—"""
        db = BungoDatabase("sqlite", self.db_path)
        
        authors = db.search_authors("")
        works = db.search_works("")
        places = db.search_places("")
        geocoded_places = [p for p in places if p.get('latitude') and p.get('longitude')]
        
        db.close()
        
        return {
            'authors_count': len(authors),
            'works_count': len(works),
            'places_count': len(places),
            'geocoded_places_count': len(geocoded_places),
            'geocoding_rate': (len(geocoded_places) / len(places) * 100) if places else 0
        }


def export_db_to_csv(db_path: str = "test_ginza.db", output_dir: str = "output", 
                     export_type: str = "all", **kwargs) -> Dict[str, str]:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’CSVã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ä¾¿åˆ©é–¢æ•°
    
    Args:
        db_path: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        export_type: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç¨®åˆ¥ ("all", "authors", "works", "places", "combined")
        **kwargs: è¿½åŠ ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    
    Returns:
        ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¾æ›¸
    """
    exporter = CSVExporter(db_path, output_dir)
    
    if export_type == "all":
        return exporter.export_all(kwargs.get('prefix'))
    elif export_type == "authors":
        return {'authors': exporter.export_authors(kwargs.get('filename'))}
    elif export_type == "works":
        return {'works': exporter.export_works(kwargs.get('filename'))}
    elif export_type == "places":
        return {'places': exporter.export_places(kwargs.get('filename'), kwargs.get('geocoded_only', False))}
    elif export_type == "combined":
        return {'combined': exporter.export_combined_data(
            kwargs.get('filename'), 
            kwargs.get('author_filter'), 
            kwargs.get('work_filter')
        )}
    else:
        raise ValueError(f"æœªå¯¾å¿œã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç¨®åˆ¥: {export_type}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œï¼ˆCLIãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ–‡è±ªã‚†ã‹ã‚Šåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  CSV ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    parser.add_argument("--db", default="test_ginza.db", help="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹")
    parser.add_argument("--output", default="output", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--type", choices=["all", "authors", "works", "places", "combined"], 
                       default="all", help="ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç¨®åˆ¥")
    parser.add_argument("--author", help="ä½œè€…ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆcombinedæ™‚ã®ã¿ï¼‰")
    parser.add_argument("--work", help="ä½œå“ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆcombinedæ™‚ã®ã¿ï¼‰")
    parser.add_argument("--geocoded-only", action="store_true", help="ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¸ˆã¿ã®ã¿ï¼ˆplacesæ™‚ã®ã¿ï¼‰")
    
    args = parser.parse_args()
    
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Ÿè¡Œ
    exporter = CSVExporter(args.db, args.output)
    
    # çµ±è¨ˆè¡¨ç¤º
    stats = exporter.get_export_stats()
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ:")
    print(f"   ä½œè€…: {stats['authors_count']}å")
    print(f"   ä½œå“: {stats['works_count']}ä»¶")
    print(f"   åœ°å: {stats['places_count']}ç®‡æ‰€")
    print(f"   ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¸ˆã¿: {stats['geocoded_places_count']}ç®‡æ‰€ ({stats['geocoding_rate']:.1f}%)")
    print()
    
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Ÿè¡Œ
    results = export_db_to_csv(
        db_path=args.db,
        output_dir=args.output,
        export_type=args.type,
        author_filter=args.author,
        work_filter=args.work,
        geocoded_only=args.geocoded_only
    )
    
    print(f"\nğŸ“ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆçµæœ:")
    for export_type, filepath in results.items():
        print(f"   {export_type}: {filepath}")


if __name__ == "__main__":
    main() 