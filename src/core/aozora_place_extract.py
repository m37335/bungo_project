#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é’ç©ºæ–‡åº«åœ°åæŠ½å‡ºï¼ˆæ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹ + GiNZA NER ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
ä»•æ§˜æ›¸ bungo_update_spec_draft01.md 5ç« ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹æˆã«åŸºã¥ãå®Ÿè£…
"""

import re
import logging
from typing import Dict, List, Optional, Tuple

try:
    import spacy
    from spacy.lang.ja import Japanese
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False

try:
    import ginza
    GINZA_AVAILABLE = True
except ImportError:
    GINZA_AVAILABLE = False


class AozoraPlaceExtractor:
    """é’ç©ºæ–‡åº«ã‹ã‚‰åœ°åæŠ½å‡ºï¼ˆGiNZA NERä½¿ç”¨ï¼‰"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # GiNZAãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        self.nlp = None
        if SPACY_AVAILABLE and GINZA_AVAILABLE:
            try:
                # æœ€æ–°ã®GiNZAã¯ç›´æ¥ja_ginzaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                import spacy
                self.nlp = spacy.load("ja_ginza")
                self.logger.info("âœ… GiNZAãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
            except OSError:
                try:
                    # ja_ginzaãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ginzaã‹ã‚‰åˆæœŸåŒ–
                    import ginza
                    import spacy
                    self.nlp = spacy.blank("ja")
                    ginza.set_lang_cls(self.nlp, "ja")
                    self.logger.info("âœ… GiNZAç›´æ¥åˆæœŸåŒ–å®Œäº†")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ GiNZAåˆæœŸåŒ–å¤±æ•—: {e}ã€‚æ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹ã§å‹•ä½œã—ã¾ã™")
                    self.nlp = None
        else:
            self.logger.info("ğŸ“ æ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹ã®åœ°åæŠ½å‡ºã‚’ä½¿ç”¨ã—ã¾ã™")
        
        # åœ°åãƒ©ãƒ™ãƒ«ï¼ˆGiNZAä½¿ç”¨æ™‚ï¼‰
        # GiNZAãŒå®Ÿéš›ã«ä½¿ç”¨ã™ã‚‹ãƒ©ãƒ™ãƒ«ï¼šProvinceï¼ˆéƒ½é“åºœçœŒï¼‰ã€Cityï¼ˆå¸‚åŒºç”ºæ‘ï¼‰ã€Locationï¼ˆå ´æ‰€ï¼‰
        self.location_labels = ["Province", "City", "Location", "GPE", "LOC", "FAC"]  # GiNZAåœ°åé–¢é€£ãƒ©ãƒ™ãƒ«
        
        # ç„¡åŠ¹åœ°åãƒªã‚¹ãƒˆ
        self.invalid_places = {
            "æ—¥", "æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "å¹´", "æ™‚", "åˆ†", "ç§’",
            "æ˜¥", "å¤", "ç§‹", "å†¬", "æœ", "æ˜¼", "å¤œ", "æ™©",
            "ä»Š", "æ˜¨", "æ˜", "å‰", "å¾Œ", "é–“", "ä¸­", "å†…", "å¤–", "ä¸Š", "ä¸‹",
            "å·¦", "å³", "æ±", "è¥¿", "å—", "åŒ—", "æ–°", "æ—§", "å¤§", "å°", "é«˜", "ä½"
        }
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå®Ÿåœ¨ã—ãªã„åœ°åãƒ»ä¸€èˆ¬åè©ï¼‰
        self.exclude_patterns = {
            # æ–¹å‘ãƒ»æŠ½è±¡æ¦‚å¿µ
            'æ±', 'è¥¿', 'å—', 'åŒ—', 'ä¸Š', 'ä¸‹', 'å·¦', 'å³', 'å‰', 'å¾Œ', 'ä¸­', 'å†…', 'å¤–',
            # ä¸€èˆ¬çš„ãªå ´æ‰€å
            'å®¶', 'åº­', 'éƒ¨å±‹', 'éšæ®µ', 'å»Šä¸‹', 'å°æ‰€', 'å¯å®¤', 'æ›¸æ–', 'ç„é–¢', 'çª“',
            # æŠ½è±¡çš„ãªå ´æ‰€
            'å¿ƒ', 'é ­', 'æ‰‹', 'è¶³', 'ç›®', 'è€³', 'å£', 'é¡”', 'ä½“', 'èº«ä½“',
            # æ™‚é–“é–¢é€£
            'æœ', 'æ˜¼', 'å¤œ', 'å¤•æ–¹', 'æ˜æ—¥', 'ä»Šæ—¥', 'æ˜¨æ—¥', 'ä»Šå¹´', 'å»å¹´', 'æ¥å¹´'
        }
    
    def extract_places_with_context(self, text: str, context_window: int = 1) -> List[Dict]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åœ°åã‚’å‰å¾Œæ–‡ä»˜ãã§æŠ½å‡º
        ä»•æ§˜æ›¸ Seq 5: GiNZA NER ã§ LOC/GPE æŠ½å‡ºã€å‰å¾Œ 1 æ–‡ä»˜ä¸
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            context_window: å‰å¾Œæ–‡ã®æ–‡æ•°
            
        Returns:
            åœ°åæŠ½å‡ºçµæœã®ãƒªã‚¹ãƒˆ
        """
        if not self.nlp:
            self.logger.error("âŒ GiNZAãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return []
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡ã«åˆ†å‰²
        sentences = self._split_into_sentences(text)
        
        places = []
        
        for i, sentence in enumerate(sentences):
            # NERå®Ÿè¡Œ
            doc = self.nlp(sentence)
            
            for ent in doc.ents:
                if ent.label_ in self.location_labels:
                    place_name = ent.text.strip()
                    
                    # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                    if self._should_exclude_place(place_name):
                        continue
                    
                    # å‰å¾Œæ–‡æŠ½å‡º
                    before_text = self._get_context_sentences(sentences, i, -context_window, 0)
                    after_text = self._get_context_sentences(sentences, i, 1, context_window + 1)
                    
                    place_info = {
                        'place_name': place_name,
                        'sentence': sentence.strip(),
                        'before_text': before_text,
                        'after_text': after_text,
                        'entity_label': ent.label_,
                        'confidence': self._calculate_confidence(ent, sentence),
                        'sentence_index': i,
                        'char_start': ent.start_char,
                        'char_end': ent.end_char
                    }
                    
                    places.append(place_info)
        
        # é‡è¤‡é™¤å»ï¼ˆåŒã˜åœ°åãƒ»åŒã˜æ–‡ï¼‰
        unique_places = self._remove_duplicates(places)
        
        self.logger.info(f"ğŸ—ºï¸ åœ°åæŠ½å‡ºå®Œäº†: {len(unique_places)}ç®‡æ‰€")
        return unique_places
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡ã«åˆ†å‰²"""
        # å¥èª­ç‚¹ã§åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
        
        # ç©ºæ–‡å­—ãƒ»çŸ­ã™ãã‚‹æ–‡ã‚’é™¤å¤–
        sentences = [s.strip() for s in sentences if len(s.strip()) > 5]
        
        return sentences
    
    def _get_context_sentences(self, sentences: List[str], current_idx: int, 
                             start_offset: int, end_offset: int) -> str:
        """å‰å¾Œæ–‡ã‚’å–å¾—"""
        start_idx = max(0, current_idx + start_offset)
        end_idx = min(len(sentences), current_idx + end_offset)
        
        context_sentences = sentences[start_idx:end_idx]
        return ''.join(context_sentences).strip()
    
    def _should_exclude_place(self, place_name: str) -> bool:
        """åœ°åã‚’é™¤å¤–ã™ã¹ãã‹ã‚’åˆ¤å®š"""
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´
        if place_name in self.exclude_patterns:
            return True
        
        # 1æ–‡å­—ã®åœ°åã¯é™¤å¤–ï¼ˆæ–¹è§’ãªã©ï¼‰
        if len(place_name) <= 1:
            return True
        
        # æ•°å­—ã®ã¿ã¯é™¤å¤–
        if place_name.isdigit():
            return True
        
        # ã²ã‚‰ãŒãªã®ã¿ã®çŸ­ã„åœ°åã¯é™¤å¤–
        if len(place_name) <= 2 and re.match(r'^[ã‚-ã‚“]+$', place_name):
            return True
        
        return False
    
    def _calculate_confidence(self, entity, sentence: str) -> float:
        """åœ°åã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
        confidence = 0.5  # ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ©ãƒ™ãƒ«ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
        if entity.label_ == 'GPE':  # åœ°æ”¿å­¦çš„å®Ÿä½“
            confidence += 0.3
        elif entity.label_ == 'LOC':  # å ´æ‰€
            confidence += 0.2
        elif entity.label_ == 'FAC':  # æ–½è¨­
            confidence += 0.1
        
        # åœ°åã®é•·ã•ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
        if len(entity.text) >= 3:
            confidence += 0.1
        
        # æ–‡è„ˆã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
        location_keywords = ['å¸‚', 'çœŒ', 'éƒ½', 'åºœ', 'ç”º', 'æ‘', 'é§…', 'å·', 'å±±', 'æµ·', 'æ¹–', 'å…¬åœ’', 'å¯º', 'ç¥ç¤¾']
        for keyword in location_keywords:
            if keyword in sentence:
                confidence += 0.1
                break
        
        return min(confidence, 1.0)  # æœ€å¤§å€¤ã¯1.0
    
    def _remove_duplicates(self, places: List[Dict]) -> List[Dict]:
        """é‡è¤‡åœ°åã‚’é™¤å»"""
        seen = set()
        unique_places = []
        
        for place in places:
            # åœ°åã¨æ–‡ã®ãƒšã‚¢ã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
            key = (place['place_name'], place['sentence'])
            
            if key not in seen:
                seen.add(key)
                unique_places.append(place)
        
        return unique_places
    
    def extract_from_work(self, work_text: str, work_info: Dict) -> List[Dict]:
        """
        ä½œå“ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åœ°åæŠ½å‡ºï¼ˆä½œå“æƒ…å ±ä»˜ãï¼‰
        
        Args:
            work_text: ä½œå“ãƒ†ã‚­ã‚¹ãƒˆ
            work_info: ä½œå“æƒ…å ±ï¼ˆauthor_name, titleç­‰ï¼‰
            
        Returns:
            åœ°åæŠ½å‡ºçµæœã®ãƒªã‚¹ãƒˆï¼ˆä½œå“æƒ…å ±ä»˜ãï¼‰
        """
        if not work_text:
            return []
        
        self.logger.info(f"ğŸ“– åœ°åæŠ½å‡ºé–‹å§‹: {work_info.get('author_name', '')} - {work_info.get('title', '')}")
        
        # ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã„å ´åˆã¯ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        chunks = self._chunk_text(work_text, max_chars=2000)
        
        all_places = []
        
        for chunk_idx, chunk in enumerate(chunks):
            chunk_places = self.extract_places_with_context(chunk)
            
            # ä½œå“æƒ…å ±ã‚’ä»˜ä¸
            for place in chunk_places:
                place.update({
                    'author_name': work_info.get('author_name', ''),
                    'work_title': work_info.get('title', ''),
                    'aozora_id': work_info.get('aozora_id', ''),
                    'aozora_url': work_info.get('file_url', ''),
                    'chunk_index': chunk_idx,
                    'extraction_method': 'ginza'
                })
            
            all_places.extend(chunk_places)
        
        self.logger.info(f"âœ… åœ°åæŠ½å‡ºå®Œäº†: {len(all_places)}ç®‡æ‰€")
        return all_places
    
    def _chunk_text(self, text: str, max_chars: int = 2000) -> List[str]:
        """
        é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†å¯èƒ½ãªã‚µã‚¤ã‚ºã«ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        å¥èª­ç‚¹ã‚’è€ƒæ…®ã—ã¦è‡ªç„¶ãªåˆ†å‰²ç‚¹ã‚’æ¢ã™
        """
        if len(text) <= max_chars:
            return [text]
        
        chunks = []
        current_chunk = ""
        
        # æ–‡ã«åˆ†å‰²
        sentences = self._split_into_sentences(text)
        
        for sentence in sentences:
            # ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ ã—ã¦ã‚‚åˆ¶é™å†…ã‹ç¢ºèª
            if len(current_chunk) + len(sentence) <= max_chars:
                current_chunk += sentence + "ã€‚"
            else:
                # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
                if current_chunk:
                    chunks.append(current_chunk.strip())
                
                # æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯é–‹å§‹
                current_chunk = sentence + "ã€‚"
        
        # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def batch_extract_from_works(self, work_results: List[Dict]) -> List[Dict]:
        """
        è¤‡æ•°ä½œå“ã‹ã‚‰ä¸€æ‹¬åœ°åæŠ½å‡º
        
        Args:
            work_results: aozora_utils.batch_download_works() ã®çµæœ
            
        Returns:
            å…¨ä½œå“ã®åœ°åæŠ½å‡ºçµæœ
        """
        all_places = []
        
        for work_result in work_results:
            if not work_result.get('success') or not work_result.get('text'):
                continue
            
            self.logger.info(f"ğŸ“– åœ°åæŠ½å‡ºé–‹å§‹: {work_result.get('author_name')} - {work_result.get('title')}")
            
            # work_resultã‹ã‚‰ä½œå“æƒ…å ±ã‚’æŠ½å‡º
            work_info = {
                'author_name': work_result.get('author_name', ''),
                'title': work_result.get('title', ''),
                'aozora_id': work_result.get('aozora_id', ''),
            }
            
            work_places = self.extract_places_from_text(work_result['text'], work_info)
            all_places.extend(work_places)
            
            self.logger.info(f"âœ… åœ°åæŠ½å‡ºå®Œäº†: {len(work_places)}ç®‡æ‰€")
        
        self.logger.info(f"ğŸ¯ ä¸€æ‹¬åœ°åæŠ½å‡ºå®Œäº†: {len(all_places)}ç®‡æ‰€")
        return all_places

    def extract_places_from_text(self, text: str, work_info: Dict) -> List[Dict]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åœ°åã‚’æŠ½å‡º
        
        Args:
            text: å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            work_info: ä½œå“æƒ…å ±
            
        Returns:
            åœ°åãƒªã‚¹ãƒˆ
        """
        places = []
        
        if self.nlp is not None:
            # GiNZA NERã‚’ä½¿ç”¨ã—ãŸåœ°åæŠ½å‡º
            places.extend(self._extract_places_with_ginza(text, work_info))
        else:
            # æ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹ã®åœ°åæŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            places.extend(self._extract_places_with_regex(text, work_info))
        
        return places
    
    def _extract_places_with_ginza(self, text: str, work_info: Dict) -> List[Dict]:
        """
        GiNZA NERã‚’ä½¿ç”¨ã—ãŸåœ°åæŠ½å‡º
        """
        places = []
        
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’é©å½“ãªé•·ã•ã«åˆ†å‰²ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ï¼‰
            chunk_size = 5000
            for i in range(0, len(text), chunk_size):
                chunk = text[i:i + chunk_size]
                doc = self.nlp(chunk)
                
                for ent in doc.ents:
                    if ent.label_ in self.location_labels:
                        place_name = ent.text.strip()
                        if self._is_valid_place_name(place_name):
                            place_info = {
                                'place_name': place_name,
                                'author_name': work_info.get('author_name', ''),
                                'work_title': work_info.get('title', ''),
                                'extraction_method': f'ginza_ner_{ent.label_}',
                                'confidence': 0.8,  # GiNZA NERã®ä¿¡é ¼åº¦
                                'context': self._get_context(text, ent.start_char + i, ent.end_char + i)
                            }
                            places.append(place_info)
                           
        except Exception as e:
            self.logger.error(f"âŒ GiNZAåœ°åæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return places
    
    def _extract_places_with_regex(self, text: str, work_info: Dict) -> List[Dict]:
        """
        æ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹ã®åœ°åæŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        """
        places = []
        
        try:
            # æ—¥æœ¬ã®åœ°åãƒ‘ã‚¿ãƒ¼ãƒ³
            # éƒ½é“åºœçœŒ
            prefecture_patterns = [
                r'[åŒ—æµ·é’æ£®å²©æ‰‹å®®åŸç§‹ç”°å±±å½¢ç¦å³¶èŒ¨åŸæ ƒæœ¨ç¾¤é¦¬åŸ¼ç‰åƒè‘‰æ±äº¬ç¥å¥ˆå·æ–°æ½Ÿå¯Œå±±çŸ³å·ç¦äº•å±±æ¢¨é•·é‡å²é˜œé™å²¡æ„›çŸ¥ä¸‰é‡æ»‹è³€äº¬éƒ½å¤§é˜ªå…µåº«å¥ˆè‰¯å’Œæ­Œå±±é³¥å–å³¶æ ¹å²¡å±±åºƒå³¶å±±å£å¾³å³¶é¦™å·æ„›åª›é«˜çŸ¥ç¦å²¡ä½è³€é•·å´ç†Šæœ¬å¤§åˆ†å®®å´é¹¿å…å³¶æ²–ç¸„][éƒ½é“åºœçœŒ]',\
                r'[åŒ—æµ·é’æ£®å²©æ‰‹å®®åŸç§‹ç”°å±±å½¢ç¦å³¶èŒ¨åŸæ ƒæœ¨ç¾¤é¦¬åŸ¼ç‰åƒè‘‰æ±äº¬ç¥å¥ˆå·æ–°æ½Ÿå¯Œå±±çŸ³å·ç¦äº•å±±æ¢¨é•·é‡å²é˜œé™å²¡æ„›çŸ¥ä¸‰é‡æ»‹è³€äº¬éƒ½å¤§é˜ªå…µåº«å¥ˆè‰¯å’Œæ­Œå±±é³¥å–å³¶æ ¹å²¡å±±åºƒå³¶å±±å£å¾³å³¶é¦™å·æ„›åª›é«˜çŸ¥ç¦å²¡ä½è³€é•·å´ç†Šæœ¬å¤§åˆ†å®®å´é¹¿å…å³¶æ²–ç¸„]'\
            ]
            
            # å¸‚åŒºç”ºæ‘
            city_patterns = [
                r'[\\u4e00-\\u9faf]+[å¸‚åŒºç”ºæ‘]',  # æ¼¢å­—+å¸‚åŒºç”ºæ‘
                r'[\\u4e00-\\u9faf]{2,}[éƒ¡]',     # æ¼¢å­—+éƒ¡
            ]
            
            # æœ‰åãªåœ°åãƒ»é§…åãªã©
            famous_places = [
                r'éŠ€åº§', r'æ–°å®¿', r'æ¸‹è°·', r'ä¸Šé‡', r'æµ…è‰', r'å“å·', r'æ± è¢‹',\
                r'æ¨ªæµœ', r'å·å´', r'åƒè‘‰', r'åŸ¼ç‰', r'å¤§å®®',\
                r'äº¬éƒ½', r'å¤§é˜ª', r'ç¥æˆ¸', r'å¥ˆè‰¯', r'åå¤å±‹',\
                r'ä»™å°', r'æœ­å¹Œ', r'ç¦å²¡', r'åºƒå³¶', r'é‡‘æ²¢',\
                r'éŒå€‰', r'æ—¥å…‰', r'ç®±æ ¹', r'ç†±æµ·', r'è»½äº•æ²¢'\
            ]
            
            all_patterns = prefecture_patterns + city_patterns + [pattern for pattern in famous_places]
            
            for pattern in all_patterns:
                matches = re.finditer(pattern, text)
                for match in matches:
                    place_name = match.group()
                    if self._is_valid_place_name(place_name):
                        place_info = {
                            'place_name': place_name,
                            'author_name': work_info.get('author_name', ''),
                            'work_title': work_info.get('title', ''),
                            'extraction_method': 'regex_pattern',
                            'confidence': 0.6,  # æ­£è¦è¡¨ç¾ã®ä¿¡é ¼åº¦
                            'context': self._get_context(text, match.start(), match.end())
                        }
                        places.append(place_info)
       
        except Exception as e:
            self.logger.error(f"âŒ æ­£è¦è¡¨ç¾åœ°åæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return places

    def _is_valid_place_name(self, place_name: str) -> bool:
        """
        åœ°åã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        """
        if not place_name or len(place_name) < 2:
            return False
        
        # ç„¡åŠ¹ãªåœ°åã‚’ãƒ•ã‚£ãƒ«ã‚¿
        if place_name in self.invalid_places:
            return False
        
        # æ•°å­—ã®ã¿ã¯é™¤å¤–
        if place_name.isdigit():
            return False
        
        return True
    
    def _get_context(self, text: str, start: int, end: int, context_len: int = 50) -> str:
        """
        åœ°åå‘¨è¾ºã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        """
        context_start = max(0, start - context_len)
        context_end = min(len(text), end + context_len)
        
        context = text[context_start:context_end]
        # åœ°åéƒ¨åˆ†ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
        place_part = text[start:end]
        context = context.replace(place_part, f"ã€{place_part}ã€‘")
        
        return context.strip()


def test_place_extractor():
    """åœ°åæŠ½å‡ºæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª åœ°åæŠ½å‡ºãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    extractor = AozoraPlaceExtractor()
    
    # ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
    test_text = """
    å¤ç›®æ¼±çŸ³ã¯æ±äº¬ã§ç”Ÿã¾ã‚Œã€ç†Šæœ¬ã®ç¬¬äº”é«˜ç­‰å­¦æ ¡ã§æ•™é­ã‚’ã¨ã£ãŸã€‚
    ãã®å¾Œã€ãƒ­ãƒ³ãƒ‰ãƒ³ã«ç•™å­¦ã—ã€å¸°å›½å¾Œã¯æ±äº¬å¸å›½å¤§å­¦ã§è¬›ç¾©ã‚’ãŠã“ãªã£ãŸã€‚
    éŠ€åº§ã‚„æ–°å®¿ã¨ã„ã£ãŸéƒ½å¸‚éƒ¨ã‚’èˆå°ã«ã—ãŸä½œå“ã‚‚å¤šãæ›¸ã„ã¦ã„ã‚‹ã€‚
    éŒå€‰ã‚„ç®±æ ¹ã§ã®é™é¤Šä¸­ã«åŸ·ç­†ã•ã‚ŒãŸä½œå“ã‚‚ã‚ã‚‹ã€‚
    """
    
    test_work_info = {
        'author_name': 'å¤ç›®æ¼±çŸ³',
        'title': 'ãƒ†ã‚¹ãƒˆä½œå“',
    }
    
    # åœ°åæŠ½å‡ºãƒ†ã‚¹ãƒˆ
    places = extractor.extract_places_from_text(test_text, test_work_info)
    
    print(f"âœ… æŠ½å‡ºã•ã‚ŒãŸåœ°å: {len(places)}ä»¶")
    for place in places[:5]:  # æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º
        print(f"  - {place['place_name']} ({place['extraction_method']}, ä¿¡é ¼åº¦: {place['confidence']})")
    
    return places


if __name__ == "__main__":
    test_place_extractor() 