#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ—¢å­˜ã® bungo_enhanced_work_places.csv ã‚’æ–°ã—ã„3ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã«ç§»è¡Œ

ä»•æ§˜æ›¸ bungo_update_spec_draft01.md 8ç« ç§»è¡Œæ‰‹é †ã«åŸºã¥ãå®Ÿè£…
"""

import pandas as pd
import logging
from datetime import datetime
from db_utils import BungoDatabase
from urllib.parse import quote
import os

def migrate_legacy_csv_to_database(csv_file: str = "bungo_enhanced_work_places.csv", 
                                 db_path: str = "bungo_production.db"):
    """
    æ—¢å­˜CSVã‚’æ–°ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹é€ ã«ç§»è¡Œ
    
    Args:
        csv_file: ç§»è¡Œå…ƒCSVãƒ•ã‚¡ã‚¤ãƒ«
        db_path: ç§»è¡Œå…ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
    """
    
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    
    logger.info(f"ğŸš€ ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ‡ãƒ¼ã‚¿ç§»è¡Œé–‹å§‹: {csv_file} â†’ {db_path}")
    
    # æ—¢å­˜CSVã®ç¢ºèª
    if not os.path.exists(csv_file):
        logger.error(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file}")
        return False
    
    # CSVãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    try:
        df = pd.read_csv(csv_file, encoding='utf-8')
        logger.info(f"ğŸ“Š CSVãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ")
        print(f"åˆ—å: {list(df.columns)}")
    except Exception as e:
        logger.error(f"âŒ CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
    db = BungoDatabase("sqlite", db_path)
    
    # çµ±è¨ˆã‚«ã‚¦ãƒ³ã‚¿
    stats = {
        'authors_added': 0,
        'works_added': 0, 
        'places_added': 0,
        'errors': 0
    }
    
    # ä½œè€…ãƒªã‚¹ãƒˆæŠ½å‡ºãƒ»ç™»éŒ²
    logger.info("ğŸ‘¤ ä½œè€…ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»ç™»éŒ²ä¸­...")
    unique_authors = df['author'].unique()
    author_id_map = {}
    
    for author_name in unique_authors:
        try:
            author_id = db.insert_author(author_name)
            author_id_map[author_name] = author_id
            stats['authors_added'] += 1
            logger.info(f"  âœ… ä½œè€…ç™»éŒ²: {author_name} (ID: {author_id})")
        except Exception as e:
            logger.error(f"  âŒ ä½œè€…ç™»éŒ²ã‚¨ãƒ©ãƒ¼ {author_name}: {e}")
            stats['errors'] += 1
    
    # ä½œå“ãƒªã‚¹ãƒˆæŠ½å‡ºãƒ»ç™»éŒ²
    logger.info("ğŸ“š ä½œå“ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»ç™»éŒ²ä¸­...")
    unique_works = df[['author', 'work_title']].drop_duplicates()
    work_id_map = {}
    
    for _, row in unique_works.iterrows():
        author_name = row['author']
        work_title = row['work_title']
        
        try:
            author_id = author_id_map.get(author_name)
            if author_id:
                work_id = db.insert_work(author_id, work_title)
                work_id_map[(author_name, work_title)] = work_id
                stats['works_added'] += 1
                logger.info(f"  âœ… ä½œå“ç™»éŒ²: {author_name} - {work_title} (ID: {work_id})")
            else:
                logger.error(f"  âŒ ä½œè€…IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {author_name}")
                stats['errors'] += 1
        except Exception as e:
            logger.error(f"  âŒ ä½œå“ç™»éŒ²ã‚¨ãƒ©ãƒ¼ {author_name}-{work_title}: {e}")
            stats['errors'] += 1
    
    # åœ°åãƒ‡ãƒ¼ã‚¿ç™»éŒ²
    logger.info("ğŸ—ºï¸ åœ°åãƒ‡ãƒ¼ã‚¿ç™»éŒ²ä¸­...")
    
    for idx, row in df.iterrows():
        try:
            author_name = row['author']
            work_title = row['work_title']
            place_name = row['place_name']
            
            work_id = work_id_map.get((author_name, work_title))
            if not work_id:
                logger.error(f"  âŒ ä½œå“IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {author_name}-{work_title}")
                stats['errors'] += 1
                continue
            
            # åœ°åãƒ‡ãƒ¼ã‚¿å¤‰æ›
            latitude = row.get('latitude')
            longitude = row.get('longitude')
            address = row.get('address', '')
            
            # æ–‡è„ˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã§ã¯ content_excerpt ã‚’ sentence ã¨ã—ã¦ä½¿ç”¨ï¼‰
            content_excerpt = row.get('content_excerpt', '')
            text_quote = row.get('text_quote', '')
            
            # before_text, sentence, after_text ã¸ã®åˆ†å‰²
            # ç°¡å˜ãªåˆ†å‰²ï¼ˆæ”¹å–„ã®ä½™åœ°ã‚ã‚Šï¼‰
            before_text = ""
            sentence = content_excerpt[:200] if content_excerpt else text_quote[:200] if text_quote else ""
            after_text = ""
            
            # Maps URLä½œæˆ
            maps_url = f"https://www.google.com/maps/search/{quote(place_name)}"
            
            # åœ°åç™»éŒ²
            place_id = db.upsert_place(
                work_id=work_id,
                place_name=place_name,
                latitude=latitude if pd.notna(latitude) else None,
                longitude=longitude if pd.notna(longitude) else None,
                address=address,
                before_text=before_text,
                sentence=sentence,
                after_text=after_text,
                extraction_method="legacy_llm",
                confidence=0.8,
                maps_url=maps_url
            )
            
            stats['places_added'] += 1
            
            if idx % 10 == 0:  # 10ä»¶ã”ã¨ã«é€²æ—è¡¨ç¤º
                logger.info(f"  é€²æ—: {idx+1}/{len(df)} ({(idx+1)/len(df)*100:.1f}%)")
                
        except Exception as e:
            logger.error(f"  âŒ åœ°åç™»éŒ²ã‚¨ãƒ©ãƒ¼ {idx}: {e}")
            stats['errors'] += 1
    
    # æœ€çµ‚çµ±è¨ˆ
    final_stats = db.get_stats()
    
    logger.info("ğŸ‰ ç§»è¡Œå®Œäº†!")
    logger.info(f"ğŸ“Š ç§»è¡Œçµ±è¨ˆ:")
    logger.info(f"  ä½œè€…: {stats['authors_added']}åè¿½åŠ ")
    logger.info(f"  ä½œå“: {stats['works_added']}ä½œå“è¿½åŠ ") 
    logger.info(f"  åœ°å: {stats['places_added']}ç®‡æ‰€è¿½åŠ ")
    logger.info(f"  ã‚¨ãƒ©ãƒ¼: {stats['errors']}ä»¶")
    
    logger.info(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€çµ‚çŠ¶æ…‹:")
    logger.info(f"  ä½œè€…ç·æ•°: {final_stats['authors_count']}å")
    logger.info(f"  ä½œå“ç·æ•°: {final_stats['works_count']}ä½œå“")
    logger.info(f"  åœ°åç·æ•°: {final_stats['places_count']}ç®‡æ‰€")
    logger.info(f"  ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç‡: {final_stats['geocoded_rate']:.1f}%")
    
    # CSVå‡ºåŠ›ãƒ†ã‚¹ãƒˆ
    export_file = db.export_to_csv("migrated_bungo_data.csv")
    logger.info(f"âœ… ç§»è¡Œãƒ‡ãƒ¼ã‚¿CSVå‡ºåŠ›: {export_file}")
    
    db.close()
    return True

def test_migration():
    """ç§»è¡Œãƒ†ã‚¹ãƒˆï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰"""
    print("ğŸ§ª ç§»è¡Œãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    test_data = pd.DataFrame([
        {
            'author': 'å¤ç›®æ¼±çŸ³',
            'work_title': 'åŠã£ã¡ã‚ƒã‚“', 
            'place_name': 'æ¾å±±å¸‚',
            'address': 'æ„›åª›çœŒæ¾å±±å¸‚',
            'latitude': 33.8395,
            'longitude': 132.7654,
            'content_excerpt': 'å››å›½ã¯æ¾å±±ã®ä¸­å­¦æ ¡ã«æ•°å­¦ã®æ•™å¸«ã¨ã—ã¦èµ´ä»»ã™ã‚‹ã“ã¨ã«ãªã£ãŸã€‚',
            'text_quote': 'è¦ªè­²ã‚Šã®ç„¡é‰„ç ²ã§å°ä¾›ã®æ™‚ã‹ã‚‰æã°ã‹ã‚Šã—ã¦ã„ã‚‹ã€‚',
            'context': 'ã€ŒåŠã£ã¡ã‚ƒã‚“ã€ã«ç™»å ´ã™ã‚‹æ¾å±±å¸‚',
            'maps_url': 'https://www.google.com/maps/search/æ¾å±±å¸‚',
            'geocoded': True
        },
        {
            'author': 'å¤ç›®æ¼±çŸ³',
            'work_title': 'å¾è¼©ã¯çŒ«ã§ã‚ã‚‹',
            'place_name': 'æœ¬éƒ·',
            'address': 'æ±äº¬éƒ½æ–‡äº¬åŒºæœ¬éƒ·',
            'latitude': 35.7077,
            'longitude': 139.7602,
            'content_excerpt': 'æ±äº¬ã®æœ¬éƒ·ã‚ãŸã‚Šã®æ›¸ç”Ÿã®å®¶ã§é£¼ã‚ã‚Œã‚‹ã“ã¨ã«ãªã£ãŸã€‚',
            'text_quote': 'å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚',
            'context': 'ã€Œå¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€ã«ç™»å ´ã™ã‚‹æœ¬éƒ·',
            'maps_url': 'https://www.google.com/maps/search/æœ¬éƒ·',
            'geocoded': True
        }
    ])
    
    test_data.to_csv("test_migration_data.csv", index=False, encoding='utf-8')
    
    # ç§»è¡Œå®Ÿè¡Œ
    success = migrate_legacy_csv_to_database("test_migration_data.csv", "test_migration.db")
    
    if success:
        print("âœ… ãƒ†ã‚¹ãƒˆç§»è¡ŒæˆåŠŸ")
        
        # ç§»è¡Œçµæœç¢ºèª
        db = BungoDatabase("sqlite", "test_migration.db")
        authors = db.search_authors("å¤ç›®")
        print(f"ç§»è¡Œå¾Œä½œè€…æ¤œç´¢: {len(authors)}ä»¶")
        
        places = db.search_places("æ¾å±±")
        print(f"ç§»è¡Œå¾Œåœ°åæ¤œç´¢: {len(places)}ä»¶")
        
        df = db.get_all_places()
        print(f"ç§»è¡Œå¾Œç·ãƒ‡ãƒ¼ã‚¿: {len(df)}ä»¶")
        print(df.head())
        
        db.close()
    else:
        print("âŒ ãƒ†ã‚¹ãƒˆç§»è¡Œå¤±æ•—")

if __name__ == "__main__":
    # ã¾ãšãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_migration()
    
    # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ç§»è¡Œï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
    # migrate_legacy_csv_to_database() 