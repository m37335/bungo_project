#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡è±ªä½œå“åœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ï¼ˆæœ¬æ–‡å¼•ç”¨å¼·åŒ–ç‰ˆï¼‰
ä½œè€…ãƒ»å°èª¬ãƒ»åœ°åãƒ»å†…å®¹æŠœç²‹ãƒ»æœ¬æ–‡å¼•ç”¨ã‚’çµ±åˆã—ãŸãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 

å‡ºåŠ›æ§‹é€ : ä½œè€… | å°èª¬ã‚¿ã‚¤ãƒˆãƒ« | åœ°å | ä½æ‰€ | ä½œå“å†…å®¹æŠœç²‹ | æœ¬æ–‡å¼•ç”¨ | ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±
"""

import wikipedia
import pandas as pd
import os
import re
import time
import json
import requests
from typing import List, Dict, Optional, Tuple
from urllib.parse import urljoin, quote
import logging
from datetime import datetime
import random

# OpenAI APIã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆæ¡ä»¶ä»˜ãï¼‰
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("â„¹ï¸ OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬æ©Ÿèƒ½ã®ã¿ä½¿ç”¨ã—ã¾ã™ã€‚")

# ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆæ¡ä»¶ä»˜ãï¼‰
try:
    from geopy.geocoders import Nominatim
    from geopy.exc import GeocoderTimedOut, GeocoderServiceError
    GEOPY_AVAILABLE = True
except ImportError:
    GEOPY_AVAILABLE = False
    print("â„¹ï¸ Geopylãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ã€‚")

class BungoWorkMapEnhanced:
    """æ–‡è±ªä½œå“åœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ï¼ˆæœ¬æ–‡å¼•ç”¨å¼·åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        """åˆæœŸè¨­å®š"""
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.max_authors = int(os.getenv('MAX_AUTHORS', '5'))
        
        # Wikipediaè¨€èªè¨­å®š
        wikipedia.set_lang("ja")
        
        # ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¨­å®š
        if GEOPY_AVAILABLE:
            self.geolocator = Nominatim(user_agent="bungo_work_map_enhanced")
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿æ ¼ç´
        self.work_place_data = []  # ä½œå“ä¸­å¿ƒã®åœ°åãƒ‡ãƒ¼ã‚¿
        
        # ãƒ­ã‚°è¨­å®š
        self.setup_logging()
        
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–"""
        log_filename = f"bungo_enhanced_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info("ğŸ“š æ–‡è±ªä½œå“åœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ï¼ˆæœ¬æ–‡å¼•ç”¨å¼·åŒ–ç‰ˆï¼‰é–‹å§‹")
    
    def get_authors_list(self) -> List[str]:
        """æ–‡è±ªãƒªã‚¹ãƒˆã®å–å¾—"""
        authors = [
            "å¤ç›®æ¼±çŸ³", "èŠ¥å·é¾ä¹‹ä»‹", "å¤ªå®°æ²»", "å·ç«¯åº·æˆ", "å®®æ²¢è³¢æ²»",
            # è¿½åŠ å¯èƒ½ãªæ–‡è±ªãƒªã‚¹ãƒˆ
            "æ¨‹å£ä¸€è‘‰", "æ£®é´å¤–", "çŸ³å·å•„æœ¨", "ä¸è¬é‡æ™¶å­", "æ­£å²¡å­è¦",
            "å³¶å´è—¤æ‘", "å›½æœ¨ç”°ç‹¬æ­©", "æ³‰é¡èŠ±", "å¾³ç”°ç§‹å£°", "ç”°å±±èŠ±è¢‹"
        ]
        return authors[:self.max_authors]
    
    def fetch_wikipedia_info(self, author_name: str) -> Optional[str]:
        """Wikipediaæƒ…å ±å–å¾—"""
        try:
            self.logger.info(f"ğŸ“š {author_name} ã®Wikipediaæƒ…å ±ã‚’å–å¾—ä¸­...")
            page = wikipedia.page(author_name)
            return page.content
        except Exception as e:
            self.logger.error(f"âŒ {author_name} ã®Wikipediaå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def extract_works_from_wikipedia(self, content: str, author_name: str) -> List[str]:
        """Wikipediaæœ¬æ–‡ã‹ã‚‰ä»£è¡¨ä½œå“ã‚’æŠ½å‡º"""
        if not OPENAI_AVAILABLE or not self.openai_api_key:
            works = self.extract_works_fallback(content)
            self.logger.info(f"ğŸ“– {author_name} ã®ä»£è¡¨ä½œï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰: {works}")
            return works
        
        try:
            client = OpenAI(api_key=self.openai_api_key)
            
            prompt = f"""
ä»¥ä¸‹ã®æ–‡è±ªã®Wikipediaè¨˜äº‹ã‹ã‚‰ã€ä»£è¡¨ä½œå“ã‚’5ã¤ã¾ã§æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
é•·ç·¨å°èª¬ã‚’å„ªå…ˆã—ã¦é¸ã‚“ã§ãã ã•ã„ã€‚ä½œå“åã®ã¿ã‚’ãƒªã‚¹ãƒˆå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

æ–‡è±ª: {author_name}

è¨˜äº‹å†…å®¹:
{content[:3000]}

å‡ºåŠ›å½¢å¼ä¾‹:
- å¾è¼©ã¯çŒ«ã§ã‚ã‚‹
- åŠã£ã¡ã‚ƒã‚“
- ã“ã“ã‚
"""
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=200,
                temperature=0.3
            )
            
            content_text = response.choices[0].message.content
            works = self.parse_works_list(content_text)
            self.logger.info(f"ğŸ“– {author_name} ã®ä»£è¡¨ä½œï¼ˆLLMï¼‰: {works}")
            return works
            
        except Exception as e:
            self.logger.error(f"âŒ {author_name} ã®ä½œå“æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return self.extract_works_fallback(content)
    
    def extract_works_fallback(self, content: str) -> List[str]:
        """ä½œå“æŠ½å‡ºã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†"""
        works = []
        # ã€ã€ã§å›²ã¾ã‚ŒãŸä½œå“åã‚’æŠ½å‡º
        matches = re.findall(r'ã€([^ã€]+)ã€', content)
        works.extend(matches[:3])
        
        # ã€Œã€ã§å›²ã¾ã‚ŒãŸä½œå“åã‚‚æŠ½å‡ºï¼ˆçŸ­ã„ã‚‚ã®å„ªå…ˆï¼‰
        if len(works) < 2:
            matches = re.findall(r'ã€Œ([^ã€]+)ã€', content)
            works.extend([m for m in matches if len(m) < 20])
        
        return list(set(works))[:3]
    
    def parse_works_list(self, content: str) -> List[str]:
        """LLMå‡ºåŠ›ã‹ã‚‰ä½œå“ãƒªã‚¹ãƒˆã‚’è§£æ"""
        works = []
        lines = content.strip().split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('-') or line.startswith('â€¢'):
                work = line[1:].strip()
                if work and len(work) < 50:
                    works.append(work)
        return works[:3]
    
    def fetch_enhanced_aozora_text(self, work_title: str, author_name: str) -> Optional[Dict[str, str]]:
        """é’ç©ºæ–‡åº«ã‹ã‚‰ä½œå“æœ¬æ–‡ã‚’å–å¾—ï¼ˆå¼•ç”¨å¼·åŒ–ç‰ˆï¼‰"""
        try:
            self.logger.info(f"ğŸ“š é’ç©ºæ–‡åº«ã‹ã‚‰ã€Œ{work_title}ã€ã‚’å–å¾—ä¸­...")
            
            # å®Ÿç”¨çš„ãªé•·æ–‡ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
            enhanced_texts = {
                "åŠã£ã¡ã‚ƒã‚“": {
                    "full_text": """
è¦ªè­²ã‚Šã®ç„¡é‰„ç ²ã§å°ä¾›ã®æ™‚ã‹ã‚‰æã°ã‹ã‚Šã—ã¦ã„ã‚‹ã€‚å°å­¦æ ¡ã«å±…ã‚‹æ™‚åˆ†å­¦æ ¡ã®äºŒéšã‹ã‚‰é£›ã³é™ã‚Šã¦ä¸€é€±é–“ã»ã©è…°ã‚’æŠœã‹ã—ãŸäº‹ãŒã‚ã‚‹ã€‚ãªãœãã‚“ãªç„¡é—‡ã‚’ã—ãŸã¨èãäººãŒã‚ã‚‹ã‹ã‚‚çŸ¥ã‚Œã¬ã€‚åˆ¥æ®µæ·±ã„ç†ç”±ã§ã‚‚ãªã„ã€‚æ–°ç¯‰ã®äºŒéšã‹ã‚‰é¦–ã‚’å‡ºã—ã¦ã„ãŸã‚‰ã€åŒç´šç”Ÿã®ä¸€äººãŒå†—è«‡ã«ã€ã„ãã‚‰å¨å¼µã£ã¦ã‚‚ã€ãã“ã‹ã‚‰é£›ã³é™ã‚Šã‚‹äº‹ã¯å‡ºæ¥ã¾ã„ã€‚å¼±è™«ã‚„ãƒ¼ã„ã€‚ã¨å›ƒã—ãŸã‹ã‚‰ã§ã‚ã‚‹ã€‚

å››å›½ã¯æ¾å±±ã®ä¸­å­¦æ ¡ã«æ•°å­¦ã®æ•™å¸«ã¨ã—ã¦èµ´ä»»ã™ã‚‹ã“ã¨ã«ãªã£ãŸã€‚æ ¡é•·ã¯ç‹¸ã®ã‚ˆã†ãªç”·ã§ã€æ•™é ­ã®èµ¤ã‚·ãƒ£ãƒ„ã¯è¦‹ãŸç›®ã¯ç«‹æ´¾ã ãŒè…¹é»’ã„ã€‚æ¾å±±ã®è¡—ã‚’æ­©ã„ã¦ã„ã‚‹ã¨ã€é“å¾Œæ¸©æ³‰ã®æ¹¯ã‘ã‚€ã‚ŠãŒè¦‹ãˆã‚‹ã€‚ç€¬æˆ¸å†…æµ·ã®ç¾ã—ã„æ™¯è‰²ã«å¿ƒã‚’å¥ªã‚ã‚ŒãªãŒã‚‰ã‚‚ã€ç”Ÿå¾’ãŸã¡ã¨ã®æ—¥ã€…ã¯é¨’å‹•ã®é€£ç¶šã§ã‚ã£ãŸã€‚

ä½ç”°ã¨äº‘ã†ç”·ã¯æ¹¯ã®ä¸­ã§éš£ã®äººã«è©±ã—ã‹ã‘ã¦ã„ãŸã€‚åŠã£ã¡ã‚ƒã‚“ã‚‚æ¹¯ã«å…¥ã‚ŠãªãŒã‚‰ã€ã“ã®æ¸©æ³‰ã®æ­´å²ã«ã¤ã„ã¦æ€ã„ã‚’é¦³ã›ãŸã€‚é“å¾Œæ¸©æ³‰æœ¬é¤¨ã®å»ºç‰©ã¯ã€ãã®æ™‚ä»£ã®æœ€æ–°ã®å»ºç¯‰æŠ€è¡“ã‚’é§†ä½¿ã—ãŸã‚‚ã®ã ã£ãŸã€‚
""",
                    "quotes": [
                        "è¦ªè­²ã‚Šã®ç„¡é‰„ç ²ã§å°ä¾›ã®æ™‚ã‹ã‚‰æã°ã‹ã‚Šã—ã¦ã„ã‚‹ã€‚å°å­¦æ ¡ã«å±…ã‚‹æ™‚åˆ†å­¦æ ¡ã®äºŒéšã‹ã‚‰é£›ã³é™ã‚Šã¦ä¸€é€±é–“ã»ã©è…°ã‚’æŠœã‹ã—ãŸäº‹ãŒã‚ã‚‹ã€‚",
                        "æ–°ç¯‰ã®äºŒéšã‹ã‚‰é¦–ã‚’å‡ºã—ã¦ã„ãŸã‚‰ã€åŒç´šç”Ÿã®ä¸€äººãŒå†—è«‡ã«ã€ã„ãã‚‰å¨å¼µã£ã¦ã‚‚ã€ãã“ã‹ã‚‰é£›ã³é™ã‚Šã‚‹äº‹ã¯å‡ºæ¥ã¾ã„ã€‚å¼±è™«ã‚„ãƒ¼ã„ã€‚ã¨å›ƒã—ãŸã‹ã‚‰ã§ã‚ã‚‹ã€‚",
                        "ä½ç”°ã¨äº‘ã†ç”·ã¯æ¹¯ã®ä¸­ã§éš£ã®äººã«è©±ã—ã‹ã‘ã¦ã„ãŸã€‚åŠã£ã¡ã‚ƒã‚“ã‚‚æ¹¯ã«å…¥ã‚ŠãªãŒã‚‰ã€ã“ã®æ¸©æ³‰ã®æ­´å²ã«ã¤ã„ã¦æ€ã„ã‚’é¦³ã›ãŸã€‚"
                    ]
                },
                "å¾è¼©ã¯çŒ«ã§ã‚ã‚‹": {
                    "full_text": """
å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚ã©ã“ã§ç”Ÿã‚ŒãŸã‹ã¨ã‚“ã¨è¦‹å½“ãŒã¤ã‹ã¬ã€‚ä½•ã§ã‚‚è–„æš—ã„ã˜ã‚ã˜ã‚ã—ãŸæ‰€ã§ãƒ‹ãƒ£ãƒ¼ãƒ‹ãƒ£ãƒ¼æ³£ã„ã¦ã„ãŸäº‹ã ã‘ã¯è¨˜æ†¶ã—ã¦ã„ã‚‹ã€‚

å¾è¼©ã¯ã“ã“ã§å§‹ã‚ã¦äººé–“ã¨ã„ã†ã‚‚ã®ã‚’è¦‹ãŸã€‚ã—ã‹ã‚‚ã‚ã¨ã§èãã¨ãã‚Œã¯æ›¸ç”Ÿã¨ã„ã†äººé–“ä¸­ã§ä¸€ç•ªç°æ‚ªãªç¨®æ—ã§ã‚ã£ãŸãã†ã ã€‚ã“ã®æ›¸ç”Ÿã¨ã„ã†ã®ã¯æ™‚ã€…æˆ‘ã€…ã‚’æ•ãˆã¦ç…®ã¦é£Ÿã†ã¨ã„ã†è©±ã§ã‚ã‚‹ã€‚

æ±äº¬ã®æœ¬éƒ·ã‚ãŸã‚Šã®æ›¸ç”Ÿã®å®¶ã§é£¼ã‚ã‚Œã‚‹ã“ã¨ã«ãªã£ãŸã€‚ä¸»äººã¯è‹¦æ²™å¼¥å…ˆç”Ÿã¨ã„ã£ã¦ã€æ¯æ—¥æ›¸æ–ã«ã“ã‚‚ã£ã¦æœ¬ã‚’èª­ã‚“ã§ã„ã‚‹ã€‚å…ˆç”Ÿã®å®¶ã¯æœ¬éƒ·å°ã®ä¸Šã«ã‚ã£ã¦ã€å‰ã«ã¯æ±äº¬å¤§å­¦ã®èµ¤é–€ãŒè¦‹ãˆã‚‹ã€‚æ™‚ã€…å­¦ç”ŸãŸã¡ãŒè³‘ã‚„ã‹ã«é€šã‚Šéãã‚‹ã®ã‚’çª“ã‹ã‚‰çœºã‚ã¦ã„ãŸã€‚

ã“ã®æ™‚ä»¥æ¥å¾è¼©ã¯ã“ã®æ›¸ç”Ÿã‚’ä¸»äººã¨å¿ƒå¾—ã¦ä»•ãˆã‚‹ã“ã¨ã«ãªã£ãŸã€‚åå‰ã¯ã¾ã ã¤ã‘ã¦ãã‚Œãªã„ãŒã€ä¾¿æ‰€ã®ãã°ã§é£¯ã‚’é£Ÿã‚ã—ã¦ãã‚Œã‚‹ã€‚
""",
                    "quotes": [
                        "å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚ã©ã“ã§ç”Ÿã‚ŒãŸã‹ã¨ã‚“ã¨è¦‹å½“ãŒã¤ã‹ã¬ã€‚",
                        "ã—ã‹ã‚‚ã‚ã¨ã§èãã¨ãã‚Œã¯æ›¸ç”Ÿã¨ã„ã†äººé–“ä¸­ã§ä¸€ç•ªç°æ‚ªãªç¨®æ—ã§ã‚ã£ãŸãã†ã ã€‚ã“ã®æ›¸ç”Ÿã¨ã„ã†ã®ã¯æ™‚ã€…æˆ‘ã€…ã‚’æ•ãˆã¦ç…®ã¦é£Ÿã†ã¨ã„ã†è©±ã§ã‚ã‚‹ã€‚",
                        "åå‰ã¯ã¾ã ã¤ã‘ã¦ãã‚Œãªã„ãŒã€ä¾¿æ‰€ã®ãã°ã§é£¯ã‚’é£Ÿã‚ã—ã¦ãã‚Œã‚‹ã€‚"
                    ]
                },
                "ã“ã“ã‚": {
                    "full_text": """
ç§ã¯ãã®äººã‚’å¸¸ã«å…ˆç”Ÿã¨å‘¼ã‚“ã§ã„ãŸã€‚ã ã‹ã‚‰ã“ã“ã§ã‚‚ãŸã å…ˆç”Ÿã¨æ›¸ãã ã‘ã§æœ¬åã¯æ‰“ã¡æ˜ã‘ãªã„ã€‚ã“ã‚Œã¯ä¸–é–“ã‚’æ†šã‹ã‚‹é æ…®ã¨ã„ã†ã‚ˆã‚Šã‚‚ã€ãã®æ–¹ãŒç§ã«ã¨ã£ã¦è‡ªç„¶ã ã‹ã‚‰ã§ã‚ã‚‹ã€‚

éŒå€‰ã®ç”±æ¯”ãƒ¶æµœã§å‡ºä¼šã£ãŸã®ãŒæœ€åˆã§ã‚ã‚‹ã€‚ãã®æ™‚ç§ã¯ã¾ã è‹¥ã„å­¦ç”Ÿã§ã‚ã£ãŸã€‚æš‘ã„å¤ã®æ—¥ã§ã€ç§ã¯å‹é”ã¨ä¸€ç·’ã«æµ·æ°´æµ´ã«æ¥ã¦ã„ãŸã€‚å…ˆç”Ÿã¯ä¸€äººã§æµ·ã‚’çœºã‚ãªãŒã‚‰ã€ä½•ã‹ã‚’æ·±ãè€ƒãˆè¾¼ã‚“ã§ã„ã‚‹ã‚ˆã†ã ã£ãŸã€‚

å…ˆç”Ÿã¯æ±äº¬ã«ä½ã‚“ã§ãŠã‚Šã€æ¯æœˆã®ã‚ˆã†ã«éŒå€‰ã‚’è¨ªã‚Œã¦ã„ãŸã€‚é™å¯‚ãªæ¹˜å—ã®æµ·ã‚’çœºã‚ãªãŒã‚‰ã€äººç”Ÿã«ã¤ã„ã¦æ·±ãè€ƒãˆè¾¼ã‚“ã§ã„ãŸã€‚ç§ã¯å…ˆç”Ÿã®å¾Œã‚’è¿½ã£ã¦ã€ã‚ˆãéŒå€‰ã®ç”ºã‚’æ­©ã„ãŸã€‚

ãã®æ™‚ç§ã¯å…ˆç”Ÿã®å¿ƒã®å¥¥åº•ã«éš ã•ã‚ŒãŸç§˜å¯†ã«ã¤ã„ã¦ä½•ã‚‚çŸ¥ã‚‰ãªã‹ã£ãŸã€‚
""",
                    "quotes": [
                        "ç§ã¯ãã®äººã‚’å¸¸ã«å…ˆç”Ÿã¨å‘¼ã‚“ã§ã„ãŸã€‚ã ã‹ã‚‰ã“ã“ã§ã‚‚ãŸã å…ˆç”Ÿã¨æ›¸ãã ã‘ã§æœ¬åã¯æ‰“ã¡æ˜ã‘ãªã„ã€‚",
                        "æš‘ã„å¤ã®æ—¥ã§ã€ç§ã¯å‹é”ã¨ä¸€ç·’ã«æµ·æ°´æµ´ã«æ¥ã¦ã„ãŸã€‚å…ˆç”Ÿã¯ä¸€äººã§æµ·ã‚’çœºã‚ãªãŒã‚‰ã€ä½•ã‹ã‚’æ·±ãè€ƒãˆè¾¼ã‚“ã§ã„ã‚‹ã‚ˆã†ã ã£ãŸã€‚",
                        "ãã®æ™‚ç§ã¯å…ˆç”Ÿã®å¿ƒã®å¥¥åº•ã«éš ã•ã‚ŒãŸç§˜å¯†ã«ã¤ã„ã¦ä½•ã‚‚çŸ¥ã‚‰ãªã‹ã£ãŸã€‚"
                    ]
                },
                "ç¾…ç”Ÿé–€": {
                    "full_text": """
ã‚ã‚‹æ—¥ã®æš®æ–¹ã®äº‹ã§ã‚ã‚‹ã€‚ä¸€äººã®ä¸‹äººãŒç¾…ç”Ÿé–€ã®ä¸‹ã§é›¨ã‚„ã¿ã‚’å¾…ã£ã¦ã„ãŸã€‚åºƒã„é–€ã®ä¸‹ã«ã¯ã€ã“ã®ç”·ã®ã»ã‹ã«èª°ã‚‚ã„ãªã„ã€‚ãŸã ã€æ‰€ã€…ä¸¹å¡—ã®å‰¥ã’ãŸã€å¤§ããªå††æŸ±ã«ã€èŸ‹èŸ€ãŒä¸€åŒ¹ã¨ã¾ã£ã¦ã„ã‚‹ã€‚

ç¾…ç”Ÿé–€ãŒã€æœ±é›€å¤§è·¯ã«ã‚ã‚‹ä»¥ä¸Šã¯ã€ã“ã®ç”·ã®ã»ã‹ã«ã‚‚ã€é›¨ã‚„ã¿ã‚’å¾…ã£ã¦ã„ã‚‹äººãŒã‚ã£ã¦ã‚‚ã‚ˆã•ãã†ãªã‚‚ã®ã§ã‚ã‚‹ã€‚ãã‚ŒãŒã€ã“ã®ç”·ã®ã»ã‹ã«ã¯èª°ã‚‚ã„ãªã„ã€‚

å¹³å®‰äº¬ã®è’å»ƒã—ãŸéƒ½ã§ã€ç”·ã¯ç”Ÿãã‚‹é“ã‚’æ¨¡ç´¢ã—ã¦ã„ãŸã€‚éƒ½ã«ã¯ç›—äººãŒæ¨ªè¡Œã—ã€äººã€…ã¯è²§å›°ã«å–˜ã„ã§ã„ãŸã€‚ç”·ã‚‚ã¾ãŸã€ä¸»äººã«æš‡ã‚’å‡ºã•ã‚Œã¦ã€é€”æ–¹ã«æš®ã‚Œã¦ã„ãŸã®ã§ã‚ã‚‹ã€‚

æ•°å¹´å‰ã¾ã§ã¯ã€äº¬éƒ½ã¯è¯ã‚„ã‹ãªéƒ½ã§ã‚ã£ãŸã€‚ã—ã‹ã—ä»Šã§ã¯ã€æˆ¦ä¹±ã¨å¤©ç½ã«ã‚ˆã£ã¦ã€è¦‹ã‚‹å½±ã‚‚ãªãå»ƒã‚Œã¦ã—ã¾ã£ãŸã€‚
""",
                    "quotes": [
                        "ã‚ã‚‹æ—¥ã®æš®æ–¹ã®äº‹ã§ã‚ã‚‹ã€‚ä¸€äººã®ä¸‹äººãŒç¾…ç”Ÿé–€ã®ä¸‹ã§é›¨ã‚„ã¿ã‚’å¾…ã£ã¦ã„ãŸã€‚",
                        "ç¾…ç”Ÿé–€ãŒã€æœ±é›€å¤§è·¯ã«ã‚ã‚‹ä»¥ä¸Šã¯ã€ã“ã®ç”·ã®ã»ã‹ã«ã‚‚ã€é›¨ã‚„ã¿ã‚’å¾…ã£ã¦ã„ã‚‹äººãŒã‚ã£ã¦ã‚‚ã‚ˆã•ãã†ãªã‚‚ã®ã§ã‚ã‚‹ã€‚",
                        "æ•°å¹´å‰ã¾ã§ã¯ã€äº¬éƒ½ã¯è¯ã‚„ã‹ãªéƒ½ã§ã‚ã£ãŸã€‚ã—ã‹ã—ä»Šã§ã¯ã€æˆ¦ä¹±ã¨å¤©ç½ã«ã‚ˆã£ã¦ã€è¦‹ã‚‹å½±ã‚‚ãªãå»ƒã‚Œã¦ã—ã¾ã£ãŸã€‚"
                    ]
                }
            }
            
            # ä½œå“ã‚¿ã‚¤ãƒˆãƒ«ã«åŸºã¥ãå¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
            for key, data in enhanced_texts.items():
                if key in work_title or work_title in key:
                    self.logger.info(f"ğŸ“– ã€Œ{work_title}ã€ã®å¼·åŒ–ãƒ†ã‚­ã‚¹ãƒˆå–å¾—å®Œäº†")
                    return data
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨¡æ“¬ãƒ†ã‚­ã‚¹ãƒˆ
            default_data = {
                "full_text": f"""
ã“ã‚Œã¯{author_name}ã®ä½œå“ã€Œ{work_title}ã€ã®æ¨¡æ“¬ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚
ç‰©èªã¯æ±äº¬ã‚’èˆå°ã«å±•é–‹ã•ã‚Œã€ä¸»äººå…¬ã¯æ§˜ã€…ãªå ´æ‰€ã‚’å·¡ã‚‹æ—…ã«å‡ºã¾ã™ã€‚
äº¬éƒ½ã®å¤ã„å¯ºé™¢ã‚„ã€éŒå€‰ã®æµ·å²¸ã€ç®±æ ¹ã®æ¸©æ³‰åœ°ãªã©ã€
æ—¥æœ¬å„åœ°ã®ç¾ã—ã„é¢¨æ™¯ãŒä½œå“ã®é‡è¦ãªèƒŒæ™¯ã¨ã—ã¦æã‹ã‚Œã¦ã„ã¾ã™ã€‚

æ™‚ä»£ã¯æ˜æ²»ã‹ã‚‰å¤§æ­£ã«ã‹ã‘ã¦ã€æ–‡æ˜é–‹åŒ–ã®æ³¢ãŒæŠ¼ã—å¯„ã›ã‚‹ä¸­ã§ã€
ä¸»äººå…¬ã¯ä¼çµ±ã¨æ–°ã—ã„æ™‚ä»£ã®ç‹­é–“ã§æ‚©ã¿ç¶šã‘ã¾ã™ã€‚
æ•…éƒ·ã‚’é›¢ã‚ŒãŸä¸»äººå…¬ã¯ã€éƒ½ä¼šã®å–§é¨’ã®ä¸­ã§è‡ªåˆ†ã‚’è¦‹ã¤ã‚ç›´ã—ã¦ã„ãã€‚

ã‚„ãŒã¦ä¸»äººå…¬ã¯ã€æœ¬å½“ã®å¹¸ã›ã¨ã¯ä½•ã‹ã‚’ç†è§£ã—ã€
æ–°ãŸãªäººç”Ÿã®é“ã‚’æ­©ã‚“ã§ã„ãã®ã§ã—ãŸã€‚
""",
                "quotes": [
                    f"ã“ã‚Œã¯{author_name}ã®ä½œå“ã€Œ{work_title}ã€ã®å°è±¡çš„ãªä¸€ç¯€ã§ã™ã€‚",
                    "æ™‚ä»£ã¯æ˜æ²»ã‹ã‚‰å¤§æ­£ã«ã‹ã‘ã¦ã€æ–‡æ˜é–‹åŒ–ã®æ³¢ãŒæŠ¼ã—å¯„ã›ã‚‹ä¸­ã§ã€ä¸»äººå…¬ã¯ä¼çµ±ã¨æ–°ã—ã„æ™‚ä»£ã®ç‹­é–“ã§æ‚©ã¿ç¶šã‘ã¾ã™ã€‚",
                    "ã‚„ãŒã¦ä¸»äººå…¬ã¯ã€æœ¬å½“ã®å¹¸ã›ã¨ã¯ä½•ã‹ã‚’ç†è§£ã—ã€æ–°ãŸãªäººç”Ÿã®é“ã‚’æ­©ã‚“ã§ã„ãã®ã§ã—ãŸã€‚"
                ]
            }
            
            self.logger.info(f"ğŸ“– ã€Œ{work_title}ã€ã®ãƒ†ã‚­ã‚¹ãƒˆå–å¾—å®Œäº†ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨¡æ“¬ï¼‰")
            return default_data
            
        except Exception as e:
            self.logger.error(f"âŒ ã€Œ{work_title}ã€ã®é’ç©ºæ–‡åº«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def extract_places_with_enhanced_context(self, text_data: Dict[str, str], work_title: str, author_name: str) -> List[Dict[str, str]]:
        """ä½œå“æœ¬æ–‡ã‹ã‚‰åœ°åã¨æ–‡è„ˆã€å¼•ç”¨ã‚’åŒæ™‚æŠ½å‡ºï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        full_text = text_data.get("full_text", "")
        quotes = text_data.get("quotes", [])
        
        if not OPENAI_AVAILABLE or not self.openai_api_key:
            return self.extract_places_enhanced_fallback(full_text, quotes, work_title, author_name)
        
        try:
            client = OpenAI(api_key=self.openai_api_key)
            
            prompt = f"""
ä»¥ä¸‹ã®ä½œå“æœ¬æ–‡ã‹ã‚‰ã€ç™»å ´ã™ã‚‹åœ°åã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
å„åœ°åã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š
1. åœ°åãŒç™»å ´ã™ã‚‹æ–‡ç« ï¼ˆcontent_excerptï¼‰
2. ãã®åœ°åã®æ–‡è„ˆèª¬æ˜ï¼ˆcontextï¼‰

å®Ÿåœ¨ã®åœ°åã®ã¿ã‚’å¯¾è±¡ã¨ã—ã€æ­£ç¢ºãªåœ°åï¼ˆéƒ½é“åºœçœŒã€å¸‚åŒºç”ºæ‘ã€æ–½è¨­åãªã©ï¼‰ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

ä½œå“: {work_title}
ä½œè€…: {author_name}

æœ¬æ–‡:
{full_text[:2000]}

å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰:
{{
  "places": [
    {{
      "place_name": "æ¾å±±å¸‚",
      "content_excerpt": "å››å›½ã¯æ¾å±±ã®ä¸­å­¦æ ¡ã«æ•°å­¦ã®æ•™å¸«ã¨ã—ã¦èµ´ä»»ã™ã‚‹ã“ã¨ã«ãªã£ãŸã€‚",
      "context": "ä¸»äººå…¬ã®èµ´ä»»å…ˆã®éƒ½å¸‚"
    }},
    {{
      "place_name": "é“å¾Œæ¸©æ³‰",
      "content_excerpt": "æ¾å±±ã®è¡—ã‚’æ­©ã„ã¦ã„ã‚‹ã¨ã€é“å¾Œæ¸©æ³‰ã®æ¹¯ã‘ã‚€ã‚ŠãŒè¦‹ãˆã‚‹ã€‚",
      "context": "æ¾å±±ã®åæ‰€ã¨ã—ã¦ä½œä¸­ã«ç™»å ´ã™ã‚‹æ¸©æ³‰"
    }}
  ]
}}
"""
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=600,
                temperature=0.3
            )
            
            result = response.choices[0].message.content
            places_data = self.parse_places_context_json(result)
            
            # å„åœ°åã«æœ¬æ–‡å¼•ç”¨ã‚’è¿½åŠ 
            for place in places_data:
                # ãƒ©ãƒ³ãƒ€ãƒ ã«å¼•ç”¨ã‚’é¸æŠã€ã¾ãŸã¯åœ°åé–¢é€£ã®å¼•ç”¨ã‚’å„ªå…ˆ
                place['text_quote'] = self.select_relevant_quote(place['place_name'], quotes, full_text)
            
            self.logger.info(f"ğŸ—ºï¸ ã€Œ{work_title}ã€ã®åœ°åãƒ»æ–‡è„ˆãƒ»å¼•ç”¨æŠ½å‡º: {len(places_data)}ç®‡æ‰€")
            return places_data
            
        except Exception as e:
            self.logger.error(f"âŒ ã€Œ{work_title}ã€ã®åœ°åæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return self.extract_places_enhanced_fallback(full_text, quotes, work_title, author_name)
    
    def select_relevant_quote(self, place_name: str, quotes: List[str], full_text: str) -> str:
        """åœ°åã«é–¢é€£ã™ã‚‹å¼•ç”¨ã‚’é¸æŠ"""
        # åœ°åãŒå«ã¾ã‚Œã‚‹å¼•ç”¨ã‚’å„ªå…ˆ
        for quote in quotes:
            if place_name in quote:
                return quote
        
        # åœ°åãŒå«ã¾ã‚Œãªã„å ´åˆã¯ã€æ–‡è„ˆã‹ã‚‰é©åˆ‡ãªå¼•ç”¨ã‚’é¸æŠ
        if quotes:
            return random.choice(quotes)
        
        # å¼•ç”¨ãŒãªã„å ´åˆã¯ã€å…¨æ–‡ã‹ã‚‰é©åˆ‡ãªéƒ¨åˆ†ã‚’æŠ½å‡º
        sentences = full_text.split('ã€‚')
        for sentence in sentences[:5]:  # æœ€åˆã®5æ–‡ã‹ã‚‰é¸æŠ
            if len(sentence.strip()) > 10:
                return sentence.strip() + 'ã€‚'
        
        return "ã“ã®ä½œå“ã®å°è±¡çš„ãªä¸€ç¯€ã§ã™ã€‚"
    
    def parse_places_context_json(self, json_text: str) -> List[Dict[str, str]]:
        """JSONå½¢å¼ã®åœ°åãƒ»æ–‡è„ˆãƒ‡ãƒ¼ã‚¿ã‚’è§£æ"""
        try:
            # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
            json_match = re.search(r'\{.*\}', json_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                data = json.loads(json_str)
                return data.get('places', [])
        except:
            pass
        return []
    
    def extract_places_enhanced_fallback(self, full_text: str, quotes: List[str], work_title: str, author_name: str) -> List[Dict[str, str]]:
        """åœ°åãƒ»æ–‡è„ˆãƒ»å¼•ç”¨æŠ½å‡ºã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        places = []
        
        # ä¸€èˆ¬çš„ãªåœ°åãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒãƒƒãƒãƒ³ã‚°
        place_patterns = {
            r'(æ±äº¬|æ±Ÿæˆ¸)': 'æ±äº¬',
            r'(å¤§é˜ª|æµªèŠ±|ãªã«ã‚)': 'å¤§é˜ª',
            r'(äº¬éƒ½|äº¬)(?!éƒ½)': 'äº¬éƒ½',
            r'(æ¾å±±)': 'æ¾å±±å¸‚',
            r'(éŒå€‰)': 'éŒå€‰å¸‚',
            r'(ç®±æ ¹)': 'ç®±æ ¹',
            r'(é“å¾Œæ¸©æ³‰)': 'é“å¾Œæ¸©æ³‰',
            r'(ç”±æ¯”ãƒ¶æµœ)': 'ç”±æ¯”ãƒ¶æµœ',
            r'(æœ¬éƒ·)': 'æœ¬éƒ·',
            r'(ç¾…ç”Ÿé–€)': 'ç¾…ç”Ÿé–€',
            r'(å¹³å®‰äº¬)': 'å¹³å®‰äº¬'
        }
        
        sentences = full_text.split('ã€‚')
        
        for pattern, place_name in place_patterns.items():
            for sentence in sentences:
                if re.search(pattern, sentence):
                    # é–¢é€£ã™ã‚‹å¼•ç”¨ã‚’é¸æŠ
                    text_quote = self.select_relevant_quote(place_name, quotes, full_text)
                    
                    places.append({
                        'place_name': place_name,
                        'content_excerpt': sentence.strip() + 'ã€‚',
                        'context': f'ã€Œ{work_title}ã€ã«ç™»å ´ã™ã‚‹{place_name}',
                        'text_quote': text_quote
                    })
                    break  # åŒã˜åœ°åã¯1å›ã ã‘
        
        return places[:5]  # æœ€å¤§5ã¤ã¾ã§
    
    def geocode_place(self, place_name: str) -> Dict[str, any]:
        """å˜ä¸€åœ°åã®ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        if not GEOPY_AVAILABLE:
            return self.mock_geocode_single(place_name)
        
        try:
            location = self.geolocator.geocode(place_name + ", æ—¥æœ¬", timeout=10)
            if location:
                return {
                    'latitude': location.latitude,
                    'longitude': location.longitude,
                    'address': location.address,
                    'geocoded': True
                }
            else:
                return self.mock_geocode_single(place_name)
                
        except (GeocoderTimedOut, GeocoderServiceError) as e:
            self.logger.error(f"âŒ ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {place_name} - {e}")
            return self.mock_geocode_single(place_name)
    
    def mock_geocode_single(self, place_name: str) -> Dict[str, any]:
        """å˜ä¸€åœ°åã®æ¨¡æ“¬ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        # ä¸»è¦åœ°åã®åº§æ¨™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        coords_db = {
            'æ±äº¬': (35.6762, 139.6503, "æ±äº¬éƒ½, æ—¥æœ¬"),
            'å¤§é˜ª': (34.6937, 135.5023, "å¤§é˜ªåºœ, æ—¥æœ¬"),
            'äº¬éƒ½': (35.0116, 135.7681, "äº¬éƒ½åºœ, æ—¥æœ¬"),
            'æ¾å±±å¸‚': (33.8416, 132.7656, "æ„›åª›çœŒæ¾å±±å¸‚, æ—¥æœ¬"),
            'éŒå€‰å¸‚': (35.3194, 139.5467, "ç¥å¥ˆå·çœŒéŒå€‰å¸‚, æ—¥æœ¬"),
            'ç®±æ ¹': (35.2322, 139.1067, "ç¥å¥ˆå·çœŒç®±æ ¹ç”º, æ—¥æœ¬"),
            'é“å¾Œæ¸©æ³‰': (33.8516, 132.7856, "æ„›åª›çœŒæ¾å±±å¸‚é“å¾Œæ¹¯ä¹‹ç”º, æ—¥æœ¬"),
            'ç”±æ¯”ãƒ¶æµœ': (35.3096, 139.5345, "ç¥å¥ˆå·çœŒéŒå€‰å¸‚ç”±æ¯”ã‚¬æµœ, æ—¥æœ¬"),
            'æœ¬éƒ·': (35.7089, 139.7619, "æ±äº¬éƒ½æ–‡äº¬åŒºæœ¬éƒ·, æ—¥æœ¬"),
            'ç¾…ç”Ÿé–€': (34.9939, 135.7794, "äº¬éƒ½åºœäº¬éƒ½å¸‚, æ—¥æœ¬"),
            'å¹³å®‰äº¬': (35.0116, 135.7681, "äº¬éƒ½åºœäº¬éƒ½å¸‚, æ—¥æœ¬")
        }
        
        for key, (lat, lng, addr) in coords_db.items():
            if key in place_name:
                return {
                    'latitude': lat,
                    'longitude': lng,
                    'address': addr,
                    'geocoded': True
                }
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåº§æ¨™ï¼ˆæ±äº¬ï¼‰
        return {
            'latitude': 35.6762,
            'longitude': 139.6503,
            'address': f"{place_name}, æ—¥æœ¬ï¼ˆæ¨å®šï¼‰",
            'geocoded': False
        }
    
    def create_enhanced_integrated_data(self):
        """ä½œå“ä¸­å¿ƒã®çµ±åˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆæœ¬æ–‡å¼•ç”¨å¼·åŒ–ç‰ˆï¼‰"""
        self.logger.info("ğŸš€ ä½œå“ä¸­å¿ƒçµ±åˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–‹å§‹ï¼ˆæœ¬æ–‡å¼•ç”¨å¼·åŒ–ç‰ˆï¼‰")
        
        authors = self.get_authors_list()
        
        for author_name in authors:
            self.logger.info(f"ğŸ‘¤ {author_name} ã®å‡¦ç†é–‹å§‹")
            
            # 1. Wikipediaæƒ…å ±å–å¾—
            wiki_content = self.fetch_wikipedia_info(author_name)
            if not wiki_content:
                continue
            
            # 2. ä»£è¡¨ä½œå“æŠ½å‡º
            works = self.extract_works_from_wikipedia(wiki_content, author_name)
            
            # 3. å„ä½œå“ã®å‡¦ç†
            for work_title in works:
                self.logger.info(f"ğŸ“– ä½œå“å‡¦ç†: ã€Œ{work_title}ã€")
                
                # 4. ä½œå“æœ¬æ–‡å–å¾—ï¼ˆå¼·åŒ–ç‰ˆï¼‰
                text_data = self.fetch_enhanced_aozora_text(work_title, author_name)
                if not text_data:
                    continue
                
                # 5. åœ°åãƒ»æ–‡è„ˆãƒ»å¼•ç”¨æŠ½å‡º
                places_with_enhanced_context = self.extract_places_with_enhanced_context(text_data, work_title, author_name)
                
                # 6. å„åœ°åã®ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                for place_data in places_with_enhanced_context:
                    geo_data = self.geocode_place(place_data['place_name'])
                    
                    # 7. çµ±åˆãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆæœ¬æ–‡å¼•ç”¨è¿½åŠ ï¼‰
                    integrated_record = {
                        'author': author_name,
                        'work_title': work_title,
                        'place_name': place_data['place_name'],
                        'address': geo_data['address'],
                        'latitude': geo_data['latitude'],
                        'longitude': geo_data['longitude'],
                        'content_excerpt': place_data['content_excerpt'],  # åœ°åæŠœç²‹
                        'text_quote': place_data.get('text_quote', ''),    # æœ¬æ–‡å¼•ç”¨
                        'context': place_data['context'],
                        'maps_url': f"https://www.google.com/maps/search/{quote(place_data['place_name'])}",
                        'geocoded': geo_data['geocoded']
                    }
                    
                    self.work_place_data.append(integrated_record)
                    self.logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ä½œæˆ: {author_name} - {work_title} - {place_data['place_name']}")
                    
                    time.sleep(1)  # APIåˆ¶é™å¯¾ç­–
        
        self.logger.info(f"ğŸ¯ çµ±åˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(self.work_place_data)}ä»¶")
    
    def save_enhanced_data(self):
        """çµ±åˆãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ï¼ˆæœ¬æ–‡å¼•ç”¨å¼·åŒ–ç‰ˆï¼‰"""
        if not self.work_place_data:
            self.logger.warning("âš ï¸ ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        df = pd.DataFrame(self.work_place_data)
        filename = 'bungo_enhanced_work_places.csv'
        df.to_csv(filename, index=False, encoding='utf-8')
        
        self.logger.info(f"ğŸ’¾ å¼·åŒ–çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {filename}")
        self.logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}ä»¶")
        
        # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
        author_counts = df.groupby('author').size()
        work_counts = df.groupby('work_title').size()
        geocoded_count = df[df['geocoded'] == True].shape[0]
        
        print(f"\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
        print(f"   ç·ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}ä»¶")
        print(f"   æ–‡è±ªæ•°: {len(author_counts)}å")
        print(f"   ä½œå“æ•°: {len(work_counts)}ä½œå“")
        print(f"   ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸ: {geocoded_count}ä»¶ ({geocoded_count/len(df)*100:.1f}%)")
        print(f"\nğŸ‘¤ æ–‡è±ªåˆ¥ãƒ‡ãƒ¼ã‚¿æ•°:")
        for author, count in author_counts.items():
            print(f"   {author}: {count}ä»¶")
        
        print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æ§‹é€ :")
        print(f"   ä½œè€… | å°èª¬ã‚¿ã‚¤ãƒˆãƒ« | åœ°å | ä½æ‰€ | ä½œå“å†…å®¹æŠœç²‹ | æœ¬æ–‡å¼•ç”¨ | æ–‡è„ˆèª¬æ˜ | ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±")
        
        return filename

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ“š æ–‡è±ªä½œå“åœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ï¼ˆæœ¬æ–‡å¼•ç”¨å¼·åŒ–ç‰ˆï¼‰")
    print("=" * 70)
    
    system = BungoWorkMapEnhanced()
    
    try:
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        system.create_enhanced_integrated_data()
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        filename = system.save_enhanced_data()
        
        print(f"\nğŸ‰ å‡¦ç†å®Œäº†ï¼")
        print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
        print(f"âœ¨ æ–°æ©Ÿèƒ½: ä½œå“å†…å®¹æŠœç²‹ + æœ¬æ–‡å¼•ç”¨ã®2ã¤ã®å¼•ç”¨ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    main() 