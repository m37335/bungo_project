#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥æœ¬ã®æ–‡è±ªåœ°å›³ã‚«ãƒ¼ãƒ‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆã‚¹ãƒªãƒ ç‰ˆï¼‰
åœ°å›³è¡¨ç¤ºå°‚ç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³
"""

import wikipedia
import pandas as pd
import os
import re
import time
from typing import List, Dict, Optional

# OpenAI APIã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆæ¡ä»¶ä»˜ãï¼‰
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("â„¹ï¸ OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬æ©Ÿèƒ½ã®ã¿ä½¿ç”¨ã—ã¾ã™ã€‚")

class BungoCardCollector:
    """æ—¥æœ¬ã®æ–‡è±ªåœ°å›³ã‚«ãƒ¼ãƒ‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆåœ°å›³è¡¨ç¤ºç‰¹åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        """åˆæœŸè¨­å®š"""
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.max_authors = int(os.getenv('MAX_AUTHORS', '15'))  # åœ°å›³è¡¨ç¤ºã«ã¯é©é‡
        
        # Wikipediaè¨€èªè¨­å®š
        wikipedia.set_lang("ja")
        
        self.authors_data = []
        
    def get_authors_list(self) -> List[str]:
        """
        æ–‡è±ªä¸€è¦§ã‚’å–å¾—ï¼ˆåœ°å›³è¡¨ç¤ºç”¨ã«å³é¸ï¼‰
        
        Returns:
            List[str]: æ–‡è±ªåã®ãƒªã‚¹ãƒˆ
        """
        print("ğŸ“š æ–‡è±ªä¸€è¦§ã‚’å–å¾—ä¸­...")
        
        # åœ°å›³è¡¨ç¤ºã«é©ã—ãŸæœ‰åæ–‡è±ªãƒªã‚¹ãƒˆï¼ˆå³é¸ç‰ˆï¼‰
        famous_authors = [
            "å¤ç›®æ¼±çŸ³", "èŠ¥å·é¾ä¹‹ä»‹", "å¤ªå®°æ²»", "å·ç«¯åº·æˆ", "ä¸‰å³¶ç”±ç´€å¤«",
            "å®®æ²¢è³¢æ²»", "è°·å´æ½¤ä¸€éƒ", "æ£®é´å¤–", "æ¨‹å£ä¸€è‘‰", "çŸ³å·å•„æœ¨",
            "ä¸è¬é‡æ™¶å­", "æ­£å²¡å­è¦", "ä¸­åŸä¸­ä¹Ÿ", "å¿—è³€ç›´å“‰", "æ­¦è€…å°è·¯å®Ÿç¯¤",
            "æœ‰å³¶æ­¦éƒ", "å³¶å´è—¤æ‘", "å›½æœ¨ç”°ç‹¬æ­©", "å°¾å´ç´…è‘‰", "äº•ä¼é±’äºŒ"
        ]
        
        # æŒ‡å®šæ•°ã«åˆ¶é™
        authors = famous_authors[:self.max_authors]
        
        print(f"âœ… åœ°å›³è¡¨ç¤ºç”¨æ–‡è±ª: {len(authors)}å")
        return authors
    
    def get_wikipedia_content(self, author_name: str) -> Optional[str]:
        """
        æŒ‡å®šã—ãŸä½œå®¶ã®Wikipediaãƒšãƒ¼ã‚¸ã®æœ¬æ–‡ã‚’å–å¾—
        
        Args:
            author_name (str): ä½œå®¶å
            
        Returns:
            Optional[str]: Wikipediaæœ¬æ–‡ã€å–å¾—å¤±æ•—æ™‚ã¯None
        """
        try:
            print(f"ğŸ“– ã€Œ{author_name}ã€ã®Wikipediaæƒ…å ±å–å¾—ä¸­...")
            page = wikipedia.page(author_name)
            content = page.content
            
            if len(content) < 100:
                print(f"âš ï¸ ã€Œ{author_name}ã€: æœ¬æ–‡ãŒçŸ­ã™ãã¾ã™")
                return None
                
            return content
            
        except wikipedia.exceptions.DisambiguationError as e:
            try:
                print(f"ğŸ”€ ã€Œ{author_name}ã€: æ›–æ˜§ã•å›é¿ãƒšãƒ¼ã‚¸ - æœ€åˆã®å€™è£œã‚’ä½¿ç”¨")
                page = wikipedia.page(e.options[0])
                return page.content
            except Exception:
                print(f"âŒ ã€Œ{author_name}ã€: æ›–æ˜§ã•å›é¿ã®è§£æ±ºã«å¤±æ•—")
                return None
                
        except wikipedia.exceptions.PageError:
            print(f"âŒ ã€Œ{author_name}ã€: ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        except Exception as e:
            print(f"âŒ ã€Œ{author_name}ã€: å–å¾—ã‚¨ãƒ©ãƒ¼ - {e}")
            return None
    
    def extract_works_and_places(self, author_name: str, content: str) -> Dict[str, List[str]]:
        """
        ä½œå“åã¨åœ°åã‚’æŠ½å‡ºï¼ˆåœ°å›³ã‚«ãƒ¼ãƒ‰ç”¨ã«æœ€é©åŒ–ï¼‰
        
        Args:
            author_name (str): ä½œå®¶å
            content (str): Wikipediaæœ¬æ–‡
            
        Returns:
            Dict[str, List[str]]: ä½œå“ã¨åœ°åã®è©³ç´°æƒ…å ±
        """
        result = {
            "works": [],
            "places": [],
            "detailed_places": []
        }
        
        # ä½œå“åæŠ½å‡ºï¼ˆã€ã€ã§å›²ã¾ã‚ŒãŸã‚‚ã®ï¼‰
        work_pattern = r'ã€([^ã€]+)ã€'
        works = re.findall(work_pattern, content)
        
        # ä½œå“åãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_works = []
        for work in works:
            if 2 <= len(work) <= 30 and not work.isdigit():
                filtered_works.append(work)
        
        result["works"] = list(set(filtered_works))[:5]  # åœ°å›³è¡¨ç¤ºç”¨ã«5ä½œå“ã¾ã§
        
        # è©³ç´°åœ°åæŠ½å‡ºï¼ˆåœ°å›³è¡¨ç¤ºã«é‡è¦ï¼‰
        detailed_places = []
        
        # 1. éƒ½é“åºœçœŒ
        prefectures = [
            "åŒ—æµ·é“", "é’æ£®çœŒ", "å²©æ‰‹çœŒ", "å®®åŸçœŒ", "ç§‹ç”°çœŒ", "å±±å½¢çœŒ", "ç¦å³¶çœŒ",
            "èŒ¨åŸçœŒ", "æ ƒæœ¨çœŒ", "ç¾¤é¦¬çœŒ", "åŸ¼ç‰çœŒ", "åƒè‘‰çœŒ", "æ±äº¬éƒ½", "ç¥å¥ˆå·çœŒ",
            "æ–°æ½ŸçœŒ", "å¯Œå±±çœŒ", "çŸ³å·çœŒ", "ç¦äº•çœŒ", "å±±æ¢¨çœŒ", "é•·é‡çœŒ", "å²é˜œçœŒ",
            "é™å²¡çœŒ", "æ„›çŸ¥çœŒ", "ä¸‰é‡çœŒ", "æ»‹è³€çœŒ", "äº¬éƒ½åºœ", "å¤§é˜ªåºœ", "å…µåº«çœŒ",
            "å¥ˆè‰¯çœŒ", "å’Œæ­Œå±±çœŒ", "é³¥å–çœŒ", "å³¶æ ¹çœŒ", "å²¡å±±çœŒ", "åºƒå³¶çœŒ", "å±±å£çœŒ",
            "å¾³å³¶çœŒ", "é¦™å·çœŒ", "æ„›åª›çœŒ", "é«˜çŸ¥çœŒ", "ç¦å²¡çœŒ", "ä½è³€çœŒ", "é•·å´çœŒ",
            "ç†Šæœ¬çœŒ", "å¤§åˆ†çœŒ", "å®®å´çœŒ", "é¹¿å…å³¶çœŒ", "æ²–ç¸„çœŒ",
            # çŸ­ç¸®å½¢
            "æ±äº¬", "äº¬éƒ½", "å¤§é˜ª", "ç¥å¥ˆå·", "æ„›çŸ¥", "ç¦å²¡", "åŒ—æµ·é“"
        ]
        
        # 2. å¸‚åŒºç”ºæ‘
        city_patterns = [
            r'([^\s]+å¸‚)', r'([^\s]+åŒº)', r'([^\s]+ç”º)', r'([^\s]+æ‘)', r'([^\s]+éƒ¡)'
        ]
        
        # 3. æ–‡å­¦æ–½è¨­
        facility_patterns = [
            r'([^\s]*è¨˜å¿µé¤¨[^\s]*)', r'([^\s]*æ–‡å­¦é¤¨[^\s]*)', 
            r'([^\s]*åšç‰©é¤¨[^\s]*)', r'([^\s]*è³‡æ–™é¤¨[^\s]*)',
            r'([^\s]*ç”Ÿå®¶[^\s]*)', r'([^\s]*æ—§å±…[^\s]*)',
            r'([^\s]*å¢“æ‰€[^\s]*)', r'([^\s]*è¨˜å¿µç¢‘[^\s]*)'
        ]
        
        # åœ°åæŠ½å‡ºå®Ÿè¡Œ
        for prefecture in prefectures:
            if prefecture in content:
                context = self._extract_context(content, prefecture)
                place_type = self._classify_place_type(context)
                detailed_places.append({
                    'name': prefecture,
                    'type': place_type,
                    'context': context,
                    'maps_ready': self._is_maps_ready(prefecture)
                })
        
        for pattern in city_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if len(match) > 1:
                    context = self._extract_context(content, match)
                    place_type = self._classify_place_type(context)
                    detailed_places.append({
                        'name': match,
                        'type': place_type,
                        'context': context,
                        'maps_ready': self._is_maps_ready(match)
                    })
        
        for pattern in facility_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if len(match) > 2:
                    context = self._extract_context(content, match)
                    detailed_places.append({
                        'name': match,
                        'type': 'è¨˜å¿µé¤¨ãƒ»æ–‡å­¦æ–½è¨­',
                        'context': context,
                        'maps_ready': True  # æ–‡å­¦æ–½è¨­ã¯åœ°å›³è¡¨ç¤ºã«é©ã—ã¦ã„ã‚‹
                    })
        
        # é‡è¤‡é™¤å»ã¨åˆ¶é™
        unique_places = []
        seen = set()
        for place in detailed_places:
            if place['name'] not in seen and len(place['name']) >= 2:
                unique_places.append(place)
                seen.add(place['name'])
                if len(unique_places) >= 10:  # åœ°å›³è¡¨ç¤ºç”¨ã«åˆ¶é™
                    break
        
        result["detailed_places"] = unique_places
        result["places"] = [p['name'] for p in unique_places]
        
        return result
    
    def _extract_context(self, content: str, location: str, context_length: int = 80) -> str:
        """æ–‡è„ˆæƒ…å ±ã‚’æŠ½å‡º"""
        try:
            index = content.find(location)
            if index == -1:
                return ""
            
            start = max(0, index - context_length)
            end = min(len(content), index + len(location) + context_length)
            context = content[start:end].replace('\n', ' ').strip()
            
            return context
        except Exception:
            return ""
    
    def _classify_place_type(self, context: str) -> str:
        """åœ°åã®ç¨®é¡ã‚’åˆ†é¡"""
        context_lower = context.lower()
        
        if any(word in context_lower for word in ['ç”Ÿ', 'èª•ç”Ÿ', 'å‡ºèº«', 'æ•…éƒ·']):
            return 'å‡ºç”Ÿåœ°'
        elif any(word in context_lower for word in ['ä½', 'å±…ä½', 'æš®ã‚‰ã—', 'ç”Ÿæ´»']):
            return 'å±…ä½åœ°'
        elif any(word in context_lower for word in ['æ´»å‹•', 'åŸ·ç­†', 'å‰µä½œ', 'æ–‡å­¦']):
            return 'æ´»å‹•åœ°'
        elif any(word in context_lower for word in ['è¨˜å¿µé¤¨', 'æ–‡å­¦é¤¨', 'åšç‰©é¤¨']):
            return 'è¨˜å¿µé¤¨ãƒ»æ–‡å­¦æ–½è¨­'
        elif any(word in context_lower for word in ['å¢“', 'çœ ã‚‹', 'è‘¬']):
            return 'å¢“æ‰€'
        elif any(word in context_lower for word in ['èˆå°', 'èƒŒæ™¯', 'ä½œå“']):
            return 'ä½œå“èˆå°'
        else:
            return 'ã‚†ã‹ã‚Šã®åœ°'
    
    def _is_maps_ready(self, place_name: str) -> bool:
        """Google Mapsæ¤œç´¢ã«é©ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        # è¨˜å¿µé¤¨ãƒ»æ–‡å­¦é¤¨ã¯æº–å‚™æ¸ˆã¿
        if any(facility in place_name for facility in ['è¨˜å¿µé¤¨', 'æ–‡å­¦é¤¨', 'åšç‰©é¤¨', 'è³‡æ–™é¤¨']):
            return True
        
        # å¸‚åŒºç”ºæ‘ãƒ¬ãƒ™ãƒ«ã¯æº–å‚™æ¸ˆã¿
        if any(admin in place_name for admin in ['å¸‚', 'åŒº', 'ç”º', 'æ‘']):
            return True
        
        # æœ‰åãªéƒ½é“åºœçœŒã¯æº–å‚™æ¸ˆã¿
        famous_prefectures = ['æ±äº¬', 'äº¬éƒ½', 'å¤§é˜ª', 'ç¥å¥ˆå·', 'æ„›çŸ¥', 'ç¦å²¡', 'åŒ—æµ·é“']
        if any(pref in place_name for pref in famous_prefectures):
            return True
        
        return False
    
    def enhance_with_ai(self, author_name: str, extracted_data: Dict[str, List[str]]) -> Dict[str, List[str]]:
        """
        AIè£œå®Œæ©Ÿèƒ½ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Args:
            author_name (str): ä½œå®¶å
            extracted_data (Dict): æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
            
        Returns:
            Dict[str, List[str]]: è£œå®Œã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        if not OPENAI_AVAILABLE or not self.openai_api_key:
            print(f"â„¹ï¸ {author_name}: AIè£œå®Œã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆAPIã‚­ãƒ¼æœªè¨­å®šï¼‰")
            return extracted_data
        
        try:
            client = OpenAI(api_key=self.openai_api_key)
            
            prompt = f"""
            æ–‡è±ªã€Œ{author_name}ã€ã«ã¤ã„ã¦ã€åœ°å›³ã‚«ãƒ¼ãƒ‰è¡¨ç¤ºç”¨ã®æƒ…å ±ã‚’æ•´ç†ã—ã¦ãã ã•ã„ã€‚
            
            æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±:
            - ä»£è¡¨ä½œ: {', '.join(extracted_data['works'])}
            - æ‰€ç¸ã®åœ°: {', '.join(extracted_data['places'])}
            
            ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
            ä»£è¡¨ä½œ: [ä½œå“åã‚’3ã¤ã¾ã§ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š]
            æ‰€ç¸ã®åœ°: [åœ°åã‚’5ã¤ã¾ã§ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š]
            """
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=200,
                temperature=0.3
            )
            
            ai_response = response.choices[0].message.content.strip()
            enhanced_data = self._parse_ai_response(ai_response)
            
            # AIçµæœã¨ãƒãƒ¼ã‚¸
            if enhanced_data['works']:
                extracted_data['works'] = enhanced_data['works']
            if enhanced_data['places']:
                # æ—¢å­˜ã®è©³ç´°åœ°åæƒ…å ±ã¯ä¿æŒ
                extracted_data['places'].extend(enhanced_data['places'])
                extracted_data['places'] = list(set(extracted_data['places']))[:10]
            
            print(f"âœ¨ {author_name}: AIè£œå®Œå®Œäº†")
            
        except Exception as e:
            print(f"âš ï¸ {author_name}: AIè£œå®Œã‚¨ãƒ©ãƒ¼ - {e}")
        
        return extracted_data
    
    def _parse_ai_response(self, response: str) -> Dict[str, List[str]]:
        """AIå¿œç­”ã‚’è§£æ"""
        result = {"works": [], "places": []}
        
        lines = response.split('\n')
        for line in lines:
            line = line.strip()
            if 'ä»£è¡¨ä½œ:' in line:
                works_str = line.split('ä»£è¡¨ä½œ:')[1].strip()
                result['works'] = [w.strip() for w in works_str.split(',') if w.strip()]
            elif 'æ‰€ç¸ã®åœ°:' in line:
                places_str = line.split('æ‰€ç¸ã®åœ°:')[1].strip()
                result['places'] = [p.strip() for p in places_str.split(',') if p.strip()]
        
        return result
    
    def create_map_cards(self) -> pd.DataFrame:
        """
        åœ°å›³ã‚«ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        
        Returns:
            pd.DataFrame: åœ°å›³ã‚«ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿
        """
        print("ğŸ—ºï¸ åœ°å›³ã‚«ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
        
        card_data = []
        
        for author_data in self.authors_data:
            author_name = author_data['name']
            works = author_data.get('works', ['ä»£è¡¨ä½œ'])
            detailed_places = author_data.get('detailed_places', [])
            
            # å„åœ°åã«å¯¾ã—ã¦ã‚«ãƒ¼ãƒ‰ã‚’ä½œæˆ
            for place_info in detailed_places:
                place_name = place_info['name']
                place_type = place_info['type']
                context = place_info['context']
                maps_ready = place_info['maps_ready']
                
                # ä»£è¡¨ä½œã‚’é¸æŠï¼ˆæœ€åˆã®ä½œå“ã‚’ä½¿ç”¨ï¼‰
                representative_work = works[0] if works else "ä»£è¡¨ä½œå“"
                
                # ã‚«ãƒ¼ãƒ‰ç”¨èª¬æ˜æ–‡ã‚’ç”Ÿæˆ
                description = f"{author_name}ã®{representative_work}ã‚†ã‹ã‚Šã®{place_type}ã§ã™ã€‚"
                if context and len(context) > 20:
                    description += f" {context[:120]}..."
                
                # Google Maps URLç”Ÿæˆ
                maps_url = f"https://www.google.com/maps/search/{place_name}"
                
                card = {
                    'card_id': f"card_{len(card_data) + 1}",
                    'ä½œå“å': representative_work,
                    'ä½œè€…å': author_name,
                    'ã‚†ã‹ã‚Šã®åœŸåœ°': place_name,
                    'å†…å®¹èª¬æ˜': description,
                    'åœ°åç¨®é¡': place_type,
                    'Maps_URL': maps_url,
                    'Mapsæº–å‚™æ¸ˆã¿': 'â—‹' if maps_ready else 'è¦ç¢ºèª'
                }
                
                card_data.append(card)
        
        df = pd.DataFrame(card_data)
        
        print(f"âœ… åœ°å›³ã‚«ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†: {len(df)}æš")
        return df
    
    def save_map_cards(self, df: pd.DataFrame, filename: str = "map_cards.csv"):
        """åœ°å›³ã‚«ãƒ¼ãƒ‰ç”¨CSVã‚’ä¿å­˜"""
        try:
            df.to_csv(filename, index=False, encoding='utf-8')
            print(f"ğŸ’¾ åœ°å›³ã‚«ãƒ¼ãƒ‰ä¿å­˜å®Œäº†: {filename}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def process_all_authors(self):
        """
        å…¨ä½“å‡¦ç†ã‚’å®Ÿè¡Œï¼ˆåœ°å›³ã‚«ãƒ¼ãƒ‰ç”Ÿæˆç‰¹åŒ–ç‰ˆï¼‰
        """
        print("ğŸš€ æ—¥æœ¬æ–‡è±ªåœ°å›³ã‚«ãƒ¼ãƒ‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
        print("=" * 50)
        
        # 1. ä½œå®¶ä¸€è¦§å–å¾—
        authors = self.get_authors_list()
        if not authors:
            print("âŒ ä½œå®¶ä¸€è¦§ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # 2. å„ä½œå®¶ã®æƒ…å ±åé›†
        for i, author in enumerate(authors, 1):
            print(f"\n[{i}/{len(authors)}] å‡¦ç†ä¸­: {author}")
            
            # Wikipediaæœ¬æ–‡å–å¾—
            content = self.get_wikipedia_content(author)
            if not content:
                continue
            
            # ä½œå“ãƒ»åœ°åæŠ½å‡º
            extracted_data = self.extract_works_and_places(author, content)
            
            # AIè£œå®Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            enhanced_data = self.enhance_with_ai(author, extracted_data)
            
            # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            self.authors_data.append({
                'name': author,
                'works': enhanced_data['works'],
                'places': enhanced_data['places'],
                'detailed_places': enhanced_data['detailed_places']
            })
            
            # é€²æ—è¡¨ç¤º
            if i % 3 == 0:
                print(f"ğŸ“Š é€²æ—: {i}/{len(authors)} å®Œäº†")
        
        # 3. åœ°å›³ã‚«ãƒ¼ãƒ‰ç”Ÿæˆãƒ»ä¿å­˜
        if self.authors_data:
            # åœ°å›³ã‚«ãƒ¼ãƒ‰ç”Ÿæˆ
            cards_df = self.create_map_cards()
            
            # CSVä¿å­˜
            self.save_map_cards(cards_df, "map_cards.csv")
            
            # çµ±è¨ˆè¡¨ç¤º
            maps_ready_count = len(cards_df[cards_df['Mapsæº–å‚™æ¸ˆã¿'] == 'â—‹'])
            maps_ready_rate = maps_ready_count / len(cards_df) * 100 if len(cards_df) > 0 else 0
            
            print(f"\nğŸ¯ å‡¦ç†å®Œäº†ï¼")
            print(f"ğŸ“š å‡¦ç†æ–‡è±ªæ•°: {len(self.authors_data)}äºº")
            print(f"ğŸ—ºï¸ ç”Ÿæˆã‚«ãƒ¼ãƒ‰æ•°: {len(cards_df)}æš")
            print(f"âœ… Mapsæº–å‚™æ¸ˆã¿: {maps_ready_count}æš ({maps_ready_rate:.1f}%)")
            print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: map_cards.csv")
            print(f"\nğŸ‰ åœ°å›³è¡¨ç¤ºã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†ï¼")
            
            return cards_df
        else:
            print("âŒ åé›†ã§ããŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ—ºï¸ æ—¥æœ¬æ–‡è±ªåœ°å›³ã‚«ãƒ¼ãƒ‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆã‚¹ãƒªãƒ ç‰ˆï¼‰")
    print("åœ°å›³è¡¨ç¤ºã«ç‰¹åŒ–ã—ãŸè»½é‡ã‚·ã‚¹ãƒ†ãƒ ã§ã™")
    
    collector = BungoCardCollector()
    result = collector.process_all_authors()
    
    if result is not None:
        print("\nğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸã‚«ãƒ¼ãƒ‰ã‚µãƒ³ãƒ—ãƒ«:")
        for i, row in result.head(3).iterrows():
            print(f"\n--- ã‚«ãƒ¼ãƒ‰ {i+1} ---")
            print(f"ä½œå“: {row['ä½œå“å']}")
            print(f"ä½œè€…: {row['ä½œè€…å']}")
            print(f"å ´æ‰€: {row['ã‚†ã‹ã‚Šã®åœŸåœ°']}")
            print(f"èª¬æ˜: {row['å†…å®¹èª¬æ˜'][:100]}...")

if __name__ == "__main__":
    main() 