#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆSQLite & Google Sheetsçµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰
ä»•æ§˜æ›¸ bungo_update_spec_draft01.md 5ç« ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹æˆã«åŸºã¥ãå®Ÿè£…
"""

import sqlite3
import pandas as pd
import os
from datetime import datetime
from typing import List, Dict, Optional, Union, Tuple
import logging

# Google Sheetsé€£æºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import gspread
    from google.auth import default
    GSPREAD_AVAILABLE = True
except ImportError:
    GSPREAD_AVAILABLE = False

class BungoDatabase:
    """æ–‡è±ªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    
    def __init__(self, db_type: str = "sqlite", db_path: str = "bungo.db"):
        """
        Args:
            db_type: "sqlite" or "sheets"
            db_path: SQLiteãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯SpreadsheetID
        """
        self.db_type = db_type
        self.db_path = db_path
        self.logger = logging.getLogger(__name__)
        
        if db_type == "sqlite":
            self._init_sqlite()
        elif db_type == "sheets":
            self._init_sheets()
        else:
            raise ValueError(f"æœªå¯¾å¿œã®DBç¨®åˆ¥: {db_type}")
    
    def _init_sqlite(self):
        """SQLiteåˆæœŸåŒ–"""
        self.conn = sqlite3.connect(self.db_path)
        self.conn.execute("PRAGMA foreign_keys = ON")
        
        # ã‚¹ã‚­ãƒ¼ãƒä½œæˆ
        schema_file = "db_schema.sql"
        if os.path.exists(schema_file):
            with open(schema_file, 'r', encoding='utf-8') as f:
                schema = f.read()
                self.conn.executescript(schema)
                self.conn.commit()
                self.logger.info(f"âœ… SQLiteã‚¹ã‚­ãƒ¼ãƒåˆæœŸåŒ–å®Œäº†: {self.db_path}")
    
    def _init_sheets(self):
        """Google SheetsåˆæœŸåŒ–"""
        if not GSPREAD_AVAILABLE:
            raise ImportError("Google Sheetsã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯gspreadã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
        
        # èªè¨¼ã¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ¥ç¶š
        self.gc = gspread.service_account()
        self.spreadsheet = self.gc.open_by_key(self.db_path)
        self.logger.info(f"âœ… Google Sheetsæ¥ç¶šå®Œäº†: {self.db_path}")
    
    def insert_author(self, name: str, wikipedia_url: str = None, 
                     birth_year: int = None, death_year: int = None) -> int:
        """ä½œè€…æŒ¿å…¥"""
        if self.db_type == "sqlite":
            cursor = self.conn.execute(
                """INSERT OR IGNORE INTO authors (name, wikipedia_url, birth_year, death_year) 
                   VALUES (?, ?, ?, ?)""",
                (name, wikipedia_url, birth_year, death_year)
            )
            self.conn.commit()
            return cursor.lastrowid or self._get_author_id(name)
        
        elif self.db_type == "sheets":
            # Sheetsã®å ´åˆã¯ authors ã‚·ãƒ¼ãƒˆã«è¿½åŠ 
            try:
                authors_sheet = self.spreadsheet.worksheet("authors")
            except:
                authors_sheet = self.spreadsheet.add_worksheet("authors", rows=1000, cols=10)
                authors_sheet.update("A1:G1", [["author_id", "name", "wikipedia_url", "birth_year", "death_year", "created_at", "updated_at"]])
            
            # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
            existing_authors = authors_sheet.get_all_records()
            for i, author in enumerate(existing_authors):
                if author['name'] == name:
                    return author['author_id']
            
            # æ–°è¦è¿½åŠ 
            author_id = len(existing_authors) + 1
            authors_sheet.append_row([
                author_id, name, wikipedia_url or "", birth_year or "", 
                death_year or "", datetime.now().isoformat(), datetime.now().isoformat()
            ])
            return author_id
    
    def _get_author_id(self, name: str) -> Optional[int]:
        """ä½œè€…IDå–å¾—"""
        if self.db_type == "sqlite":
            cursor = self.conn.execute("SELECT author_id FROM authors WHERE name = ?", (name,))
            result = cursor.fetchone()
            return result[0] if result else None
    
    def insert_work(self, author_id: int, title: str, wikipedia_url: str = None,
                   aozora_url: str = None, publication_year: int = None, genre: str = None) -> int:
        """ä½œå“æŒ¿å…¥"""
        if self.db_type == "sqlite":
            cursor = self.conn.execute(
                """INSERT OR IGNORE INTO works (author_id, title, wikipedia_url, aozora_url, publication_year, genre)
                   VALUES (?, ?, ?, ?, ?, ?)""",
                (author_id, title, wikipedia_url, aozora_url, publication_year, genre)
            )
            self.conn.commit()
            return cursor.lastrowid or self._get_work_id(author_id, title)
        
        elif self.db_type == "sheets":
            try:
                works_sheet = self.spreadsheet.worksheet("works")
            except:
                works_sheet = self.spreadsheet.add_worksheet("works", rows=1000, cols=15)
                works_sheet.update("A1:I1", [["work_id", "author_id", "title", "wikipedia_url", "aozora_url", "publication_year", "genre", "created_at", "updated_at"]])
            
            # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
            existing_works = works_sheet.get_all_records()
            for work in existing_works:
                if work['author_id'] == author_id and work['title'] == title:
                    return work['work_id']
            
            # æ–°è¦è¿½åŠ 
            work_id = len(existing_works) + 1
            works_sheet.append_row([
                work_id, author_id, title, wikipedia_url or "", aozora_url or "",
                publication_year or "", genre or "", datetime.now().isoformat(), datetime.now().isoformat()
            ])
            return work_id
    
    def _get_work_id(self, author_id: int, title: str) -> Optional[int]:
        """ä½œå“IDå–å¾—"""
        if self.db_type == "sqlite":
            cursor = self.conn.execute("SELECT work_id FROM works WHERE author_id = ? AND title = ?", (author_id, title))
            result = cursor.fetchone()
            return result[0] if result else None
    
    def upsert_place(self, work_id: int, place_name: str, latitude: float = None, longitude: float = None,
                    address: str = None, before_text: str = None, sentence: str = None, after_text: str = None,
                    extraction_method: str = "llm", confidence: float = 0.8, maps_url: str = None) -> int:
        """åœ°åæŒ¿å…¥ãƒ»æ›´æ–°"""
        if self.db_type == "sqlite":
            # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
            cursor = self.conn.execute("SELECT place_id FROM places WHERE work_id = ? AND place_name = ?", (work_id, place_name))
            existing = cursor.fetchone()
            
            if existing:
                # æ›´æ–°
                self.conn.execute(
                    """UPDATE places SET latitude = ?, longitude = ?, address = ?, before_text = ?, 
                       sentence = ?, after_text = ?, extraction_method = ?, confidence = ?, 
                       maps_url = ?, geocoded = ?, updated_at = CURRENT_TIMESTAMP
                       WHERE place_id = ?""",
                    (latitude, longitude, address, before_text, sentence, after_text, 
                     extraction_method, confidence, maps_url, latitude is not None, existing[0])
                )
                self.conn.commit()
                return existing[0]
            else:
                # æ–°è¦æŒ¿å…¥
                cursor = self.conn.execute(
                    """INSERT INTO places (work_id, place_name, latitude, longitude, address, before_text,
                       sentence, after_text, extraction_method, confidence, maps_url, geocoded)
                       VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)""",
                    (work_id, place_name, latitude, longitude, address, before_text, sentence, after_text,
                     extraction_method, confidence, maps_url, latitude is not None)
                )
                self.conn.commit()
                return cursor.lastrowid
    
    def search_authors(self, query: str) -> List[Dict]:
        """ä½œè€…æ¤œç´¢"""
        if self.db_type == "sqlite":
            cursor = self.conn.execute(
                "SELECT * FROM authors WHERE name LIKE ? ORDER BY name",
                (f"%{query}%",)
            )
            columns = [desc[0] for desc in cursor.description]
            return [dict(zip(columns, row)) for row in cursor.fetchall()]
    
    def search_works(self, query: str) -> List[Dict]:
        """ä½œå“æ¤œç´¢"""
        if self.db_type == "sqlite":
            cursor = self.conn.execute(
                """SELECT w.*, a.name as author_name FROM works w 
                   JOIN authors a ON w.author_id = a.author_id
                   WHERE w.title LIKE ? ORDER BY a.name, w.title""",
                (f"%{query}%",)
            )
            columns = [desc[0] for desc in cursor.description]
            return [dict(zip(columns, row)) for row in cursor.fetchall()]
    
    def search_places(self, query: str) -> List[Dict]:
        """åœ°åæ¤œç´¢"""
        if self.db_type == "sqlite":
            cursor = self.conn.execute(
                """SELECT * FROM bungo_integrated WHERE place_name LIKE ? ORDER BY author_name, work_title""",
                (f"%{query}%",)
            )
            columns = [desc[0] for desc in cursor.description]
            return [dict(zip(columns, row)) for row in cursor.fetchall()]
    
    def get_all_places(self) -> pd.DataFrame:
        """å…¨åœ°åãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã§å–å¾—"""
        if self.db_type == "sqlite":
            return pd.read_sql_query("SELECT * FROM bungo_integrated", self.conn)
    
    def export_to_csv(self, filename: str = "bungo_database_export.csv"):
        """CSVå‡ºåŠ›"""
        df = self.get_all_places()
        df.to_csv(filename, index=False, encoding='utf-8')
        self.logger.info(f"âœ… CSVå‡ºåŠ›å®Œäº†: {filename} ({len(df)}ä»¶)")
        return filename
    
    def get_stats(self) -> Dict:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        if self.db_type == "sqlite":
            stats = {}
            stats['authors_count'] = self.conn.execute("SELECT COUNT(*) FROM authors").fetchone()[0]
            stats['works_count'] = self.conn.execute("SELECT COUNT(*) FROM works").fetchone()[0]
            stats['places_count'] = self.conn.execute("SELECT COUNT(*) FROM places").fetchone()[0]
            stats['geocoded_count'] = self.conn.execute("SELECT COUNT(*) FROM places WHERE geocoded = 1").fetchone()[0]
            stats['geocoded_rate'] = (stats['geocoded_count'] / stats['places_count'] * 100) if stats['places_count'] > 0 else 0
            return stats
    
    def close(self):
        """æ¥ç¶šã‚¯ãƒ­ãƒ¼ã‚º"""
        if self.db_type == "sqlite" and hasattr(self, 'conn'):
            self.conn.close()
            self.logger.info("âœ… SQLiteæ¥ç¶šã‚¯ãƒ­ãƒ¼ã‚º")

# ä½¿ç”¨ä¾‹ãƒ»ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
def test_database():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‹•ä½œãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # SQLiteç‰ˆãƒ†ã‚¹ãƒˆ
    db = BungoDatabase("sqlite", "test_bungo.db")
    
    # ä½œè€…è¿½åŠ 
    author_id = db.insert_author("å¤ç›®æ¼±çŸ³", "https://ja.wikipedia.org/wiki/å¤ç›®æ¼±çŸ³", 1867, 1916)
    print(f"ä½œè€…ID: {author_id}")
    
    # ä½œå“è¿½åŠ 
    work_id = db.insert_work(author_id, "åŠã£ã¡ã‚ƒã‚“", genre="å°èª¬")
    print(f"ä½œå“ID: {work_id}")
    
    # åœ°åè¿½åŠ 
    place_id = db.upsert_place(work_id, "æ¾å±±å¸‚", 33.84, 132.77, "æ„›åª›çœŒæ¾å±±å¸‚", 
                              sentence="å››å›½ã¯æ¾å±±ã®ä¸­å­¦æ ¡ã«æ•°å­¦ã®æ•™å¸«ã¨ã—ã¦èµ´ä»»ã™ã‚‹ã“ã¨ã«ãªã£ãŸã€‚")
    print(f"åœ°åID: {place_id}")
    
    # æ¤œç´¢ãƒ†ã‚¹ãƒˆ
    authors = db.search_authors("å¤ç›®")
    print(f"ä½œè€…æ¤œç´¢çµæœ: {len(authors)}ä»¶")
    
    places = db.search_places("æ¾å±±")
    print(f"åœ°åæ¤œç´¢çµæœ: {len(places)}ä»¶")
    
    # çµ±è¨ˆ
    stats = db.get_stats()
    print(f"çµ±è¨ˆ: {stats}")
    
    db.close()
    print("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    test_database() 