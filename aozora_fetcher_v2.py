#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é’ç©ºæ–‡åº«ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå…¬å¼ã‚µã‚¤ãƒˆç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ç‰ˆï¼‰
ä»•æ§˜æ›¸ bungo_update_spec_draft01.md S2ç«  é’ç©ºæ–‡åº«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«åŸºã¥ãå®Ÿè£…
"""

import requests
import re
import time
from typing import Dict, List, Optional, Tuple
import logging
from urllib.parse import urljoin, quote, unquote
import json
from bs4 import BeautifulSoup
import os

class AozoraFetcherV2:
    """é’ç©ºæ–‡åº«ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå…¬å¼ã‚µã‚¤ãƒˆç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ç‰ˆï¼‰"""
    
    def __init__(self, cache_dir: str = "aozora_cache"):
        self.base_url = "https://www.aozora.gr.jp"
        self.cache_dir = cache_dir
        self.logger = logging.getLogger(__name__)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'BungoMapSystem/1.0 (Educational Research Purpose)'
        })
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(cache_dir, exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
        
        # çŸ¥ååº¦ã®é«˜ã„ä½œå“ã®å›ºå®šãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
        self.famous_works = {
            "å¤ç›®æ¼±çŸ³": [
                {"title": "åŠã£ã¡ã‚ƒã‚“", "file_id": "752", "person_id": "148"},
                {"title": "å¾è¼©ã¯çŒ«ã§ã‚ã‚‹", "file_id": "789", "person_id": "148"},
                {"title": "ã“ã“ã‚", "file_id": "773", "person_id": "148"},
                {"title": "ä¸‰å››éƒ", "file_id": "794", "person_id": "148"},
                {"title": "ãã‚Œã‹ã‚‰", "file_id": "795", "person_id": "148"}
            ],
            "èŠ¥å·é¾ä¹‹ä»‹": [
                {"title": "ç¾…ç”Ÿé–€", "file_id": "127", "person_id": "879"},
                {"title": "é¼»", "file_id": "42", "person_id": "879"},
                {"title": "èœ˜è››ã®ç³¸", "file_id": "2", "person_id": "879"},
                {"title": "åœ°ç„å¤‰", "file_id": "128", "person_id": "879"},
                {"title": "è—ªã®ä¸­", "file_id": "180", "person_id": "879"}
            ],
            "å¤ªå®°æ²»": [
                {"title": "äººé–“å¤±æ ¼", "file_id": "301", "person_id": "35"},
                {"title": "èµ°ã‚Œãƒ¡ãƒ­ã‚¹", "file_id": "1567", "person_id": "35"},
                {"title": "æ´¥è»½", "file_id": "570", "person_id": "35"},
                {"title": "æ–œé™½", "file_id": "1565", "person_id": "35"},
                {"title": "å¥³ç”Ÿå¾’", "file_id": "1569", "person_id": "35"}
            ]
        }
    
    def search_author_works(self, author_name: str) -> List[Dict]:
        """
        ä½œè€…åã§ä½œå“æ¤œç´¢ï¼ˆå›ºå®šãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰
        
        Args:
            author_name: ä½œè€…å
            
        Returns:
            ä½œå“ãƒªã‚¹ãƒˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€IDã€URLç­‰ï¼‰
        """
        self.logger.info(f"ğŸ” é’ç©ºæ–‡åº«æ¤œç´¢: {author_name}")
        
        # å›ºå®šãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¤œç´¢
        if author_name in self.famous_works:
            works = []
            for work_data in self.famous_works[author_name]:
                work_info = {
                    'book_id': work_data['file_id'],
                    'title': work_data['title'],
                    'author': author_name,
                    'person_id': work_data['person_id'],
                    'text_url': f"{self.base_url}/cards/{work_data['person_id']}/files/{work_data['file_id']}_ruby_{work_data['file_id']}.html",
                    'plain_url': f"{self.base_url}/cards/{work_data['person_id']}/files/{work_data['file_id']}.txt",
                    'card_url': f"{self.base_url}/cards/{work_data['person_id']}/card{work_data['file_id']}.html"
                }
                works.append(work_info)
            
            self.logger.info(f"âœ… æ¤œç´¢å®Œäº†: {author_name} - {len(works)}ä½œå“ç™ºè¦‹")
            return works
        else:
            self.logger.warning(f"âŒ å›ºå®šãƒ‡ãƒ¼ã‚¿ã«æœªç™»éŒ²: {author_name}")
            return []
    
    def get_text_content(self, work_info: Dict) -> Optional[str]:
        """
        ä½œå“ã®æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
        
        Args:
            work_info: search_author_works()ã®çµæœã®1ã¤
            
        Returns:
            æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆé’ç©ºæ–‡åº«è¨˜æ³•é™¤å»æ¸ˆã¿ï¼‰
        """
        book_id = work_info.get('book_id')
        title = work_info.get('title')
        plain_url = work_info.get('plain_url')
        
        if not plain_url:
            self.logger.warning(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆURLæœªå–å¾—: {title}")
            return None
        
        self.logger.info(f"ğŸ“¥ ãƒ†ã‚­ã‚¹ãƒˆå–å¾—é–‹å§‹: {title}")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        cache_file = os.path.join(self.cache_dir, f"{book_id}_{title}.txt")
        if os.path.exists(cache_file):
            self.logger.info(f"ğŸ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿: {title}")
            with open(cache_file, 'r', encoding='utf-8') as f:
                return f.read()
        
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
            self.logger.info(f"ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {plain_url}")
            response = self.session.get(plain_url, timeout=30)
            response.raise_for_status()
            
            # Shift_JISã§ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆé’ç©ºæ–‡åº«ã®æ¨™æº–ï¼‰
            try:
                text_content = response.content.decode('shift_jis')
            except UnicodeDecodeError:
                # UTF-8ã§ã‚‚è©¦è¡Œ
                try:
                    text_content = response.content.decode('utf-8')
                except UnicodeDecodeError:
                    text_content = response.content.decode('shift_jis', errors='ignore')
            
            # é’ç©ºæ–‡åº«è¨˜æ³•é™¤å»
            cleaned_text = self._clean_aozora_text(text_content)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            with open(cache_file, 'w', encoding='utf-8') as f:
                f.write(cleaned_text)
            
            self.logger.info(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆå–å¾—å®Œäº†: {title} ({len(cleaned_text)}æ–‡å­—)")
            return cleaned_text
            
        except requests.RequestException as e:
            self.logger.error(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼ {title}: {e}")
            return None
        except Exception as e:
            self.logger.error(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼ {title}: {e}")
            return None
    
    def _clean_aozora_text(self, raw_text: str) -> str:
        """
        é’ç©ºæ–‡åº«è¨˜æ³•é™¤å»
        
        Args:
            raw_text: ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            è¨˜æ³•é™¤å»æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
        """
        # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ãƒ•ãƒƒã‚¿ãƒ¼é™¤å»
        lines = raw_text.split('\n')
        content_lines = []
        in_content = False
        
        for line in lines:
            # æœ¬æ–‡é–‹å§‹æ¤œå‡º
            if not in_content and ('ä¸€' in line or 'ã¯ã˜ã‚' in line or len(line.strip()) > 0 and not line.startswith('---')):
                # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã‚¹ã‚­ãƒƒãƒ—å¾Œã®æœ€åˆã®å®Ÿè³ªè¡Œ
                if not (line.startswith('åº•æœ¬ï¼š') or line.startswith('å…¥åŠ›ï¼š') or 
                       line.startswith('æ ¡æ­£ï¼š') or line.startswith('â€»') or
                       '---' in line or line.strip() == ''):
                    in_content = True
            
            # ãƒ•ãƒƒã‚¿ãƒ¼æ¤œå‡º
            if in_content and ('åº•æœ¬ï¼š' in line or 'å…¥åŠ›ï¼š' in line):
                break
            
            if in_content:
                content_lines.append(line)
        
        content = '\n'.join(content_lines)
        
        # é’ç©ºæ–‡åº«è¨˜æ³•é™¤å»
        patterns = [
            r'ï¼»ï¼ƒ.*?ï¼½',   # è¨˜æ³•ã‚¿ã‚°
            r'ï½œ([^ã€Š]*?)ã€Š.*?ã€‹',  # ãƒ«ãƒ“ï¼ˆèª­ã¿æ–¹ã¯æ®‹ã•ãšã€å…ƒã®æ–‡å­—ã®ã¿ï¼‰
            r'ã€Š.*?ã€‹',     # æ®‹ã£ãŸãƒ«ãƒ“
            r'ã€”.*?ã€•',     # ç·¨è€…æ³¨
            r'â€»\d+.*?\n',  # æ³¨é‡ˆ
        ]
        
        cleaned = content
        for pattern in patterns:
            if pattern == r'ï½œ([^ã€Š]*?)ã€Š.*?ã€‹':
                # ãƒ«ãƒ“ã®å ´åˆã¯å…ƒã®æ–‡å­—ã‚’æ®‹ã™
                cleaned = re.sub(pattern, r'\1', cleaned)
            else:
                cleaned = re.sub(pattern, '', cleaned, flags=re.DOTALL)
        
        # ä½™åˆ†ãªç©ºç™½ãƒ»æ”¹è¡Œæ•´ç†
        cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned)
        cleaned = re.sub(r'ã€€+', 'ã€€', cleaned)
        cleaned = cleaned.strip()
        
        return cleaned
    
    def get_multiple_works_text(self, author_name: str, max_works: int = 3) -> List[Dict]:
        """
        æŒ‡å®šä½œè€…ã®è¤‡æ•°ä½œå“ãƒ†ã‚­ã‚¹ãƒˆä¸€æ‹¬å–å¾—
        
        Args:
            author_name: ä½œè€…å
            max_works: æœ€å¤§å–å¾—ä½œå“æ•°
            
        Returns:
            ä½œå“ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
        """
        self.logger.info(f"ğŸ“š è¤‡æ•°ä½œå“å–å¾—é–‹å§‹: {author_name} (æœ€å¤§{max_works}ä½œå“)")
        
        # ä½œå“æ¤œç´¢
        works = self.search_author_works(author_name)
        if not works:
            return []
        
        results = []
        for i, work in enumerate(works[:max_works]):
            self.logger.info(f"ğŸ“– å–å¾—ä¸­ {i+1}/{min(len(works), max_works)}: {work['title']}")
            
            text_content = self.get_text_content(work)
            if text_content:
                results.append({
                    'author': author_name,
                    'title': work['title'],
                    'book_id': work['book_id'],
                    'text_content': text_content,
                    'char_count': len(text_content),
                    'aozora_url': work.get('card_url', ''),
                    'plain_text_url': work.get('plain_url', '')
                })
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆç¤¼å„€ï¼‰
            time.sleep(2)
        
        self.logger.info(f"âœ… è¤‡æ•°ä½œå“å–å¾—å®Œäº†: {author_name} - {len(results)}ä½œå“")
        return results

def test_aozora_fetcher_v2():
    """é’ç©ºæ–‡åº«å–å¾—ãƒ†ã‚¹ãƒˆï¼ˆV2ï¼‰"""
    print("ğŸ§ª é’ç©ºæ–‡åº«å–å¾—ãƒ†ã‚¹ãƒˆï¼ˆV2ï¼‰é–‹å§‹")
    
    fetcher = AozoraFetcherV2()
    
    # å¤ç›®æ¼±çŸ³ä½œå“æ¤œç´¢
    print("\n1. ä½œå“æ¤œç´¢ãƒ†ã‚¹ãƒˆ")
    works = fetcher.search_author_works("å¤ç›®æ¼±çŸ³")
    print(f"æ¤œç´¢çµæœ: {len(works)}ä½œå“")
    for work in works[:3]:
        print(f"  - {work['title']} (ID: {work['book_id']})")
        print(f"    URL: {work['plain_url']}")
    
    # ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ãƒ†ã‚¹ãƒˆ
    if works:
        print("\n2. ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ãƒ†ã‚¹ãƒˆ")
        first_work = works[0]  # åŠã£ã¡ã‚ƒã‚“
        text = fetcher.get_text_content(first_work)
        if text:
            print(f"ãƒ†ã‚­ã‚¹ãƒˆå–å¾—æˆåŠŸ: {first_work['title']}")
            print(f"æ–‡å­—æ•°: {len(text)}")
            print(f"å†’é ­: {text[:200]}...")
            print(f"æœ«å°¾: ...{text[-200:]}")
        else:
            print("ãƒ†ã‚­ã‚¹ãƒˆå–å¾—å¤±æ•—")
    
    # è¤‡æ•°ä½œå“å–å¾—ãƒ†ã‚¹ãƒˆ
    print("\n3. è¤‡æ•°ä½œå“å–å¾—ãƒ†ã‚¹ãƒˆ")
    results = fetcher.get_multiple_works_text("å¤ç›®æ¼±çŸ³", max_works=2)
    print(f"å–å¾—å®Œäº†: {len(results)}ä½œå“")
    for result in results:
        print(f"  - {result['title']}: {result['char_count']}æ–‡å­—")
    
    # èŠ¥å·é¾ä¹‹ä»‹ã‚‚ãƒ†ã‚¹ãƒˆ
    print("\n4. èŠ¥å·é¾ä¹‹ä»‹ãƒ†ã‚¹ãƒˆ")
    akutagawa_results = fetcher.get_multiple_works_text("èŠ¥å·é¾ä¹‹ä»‹", max_works=1)
    print(f"èŠ¥å·å–å¾—å®Œäº†: {len(akutagawa_results)}ä½œå“")
    for result in akutagawa_results:
        print(f"  - {result['title']}: {result['char_count']}æ–‡å­—")
        print(f"    å†’é ­: {result['text_content'][:100]}...")
    
    print("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    test_aozora_fetcher_v2() 